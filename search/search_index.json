{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the TechTaco Knowledge Base","title":"Home"},{"location":"#welcome-to-the-techtaco-knowledge-base","text":"","title":"Welcome to the TechTaco Knowledge Base"},{"location":"disclaimer/","text":"TechTaco.ORG Terms And Conditions Of Use All content provided on this site (blog/wiki/knowledge base) is for informational purposes only. The owner of this makes no representations as to the accuracy or completeness of any information on this site or found by following any link on this site. The owner of will not be liable for any errors or omissions in this information nor for the availability of this information. The owner will not be liable for any losses, injuries, or damages from the display or use of this information.","title":"Disclaimer"},{"location":"disclaimer/#techtacoorg-terms-and-conditions-of-use","text":"All content provided on this site (blog/wiki/knowledge base) is for informational purposes only. The owner of this makes no representations as to the accuracy or completeness of any information on this site or found by following any link on this site. The owner of will not be liable for any errors or omissions in this information nor for the availability of this information. The owner will not be liable for any losses, injuries, or damages from the display or use of this information.","title":"TechTaco.ORG Terms And Conditions Of Use"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/","text":"Building Apache Tomcat Connetor module (mod-jk) RPM with FPM on Centos 6.x The Apache Tomat Connector module (mod_jk) is not currently in included Centos or EPEL repositories. To assist in the deployment and management of the Apache mod_jk module a RPM package will be created from the built source using FPM . If you havn't stumbled upon or used FPM I highly recommend taking a look. Download the latest tomcat-connector source Tomcat-Connector Source Index . As of the writing of this article 1.2.40 Verify Build Prerequisites The following packages will need to be installed in order to correctly build the Apache Tomcat Connetor module (mod-jk). Apache sudo yum install httpd Apache Extension Tool (apxs) sudo yum install apache-devel GCC sudo yum instll gcc Build the Apache Tomcat Connetor module (mod-jk) Extract the downloaded source tar -zxvf tomcat-connectors-1.2.40-src.tar.gz Navigate into the /native subdirectory of the extracted source cd tomcat-connectors-1.2.40-src/native Run the configure script with apxs set ./configure --with-apxs = /usr/sbin/apxs Complie/Make the module make At this point the module is build and could be copied directly to a servers /usr/[lib/lib64]/httpd/mouldes directory to be used or tested. Create FPM Build Root FPM will be used to generate an RPM package from a build root source containing the mimiced directory structure and files to be installed on the system. This means if we want to install the mod_jk.so module file via the rpm into /usr/lib64/httpd/modules we need to create correct diretory structure inside the build root directory for FPM to reference. This can be created anywhere on your file system, but for ease of management and reference I recommand creating the build root in the /native subdirectory of the extracted source Note! The commands reference below are being run from inside the tomcat-connectors-1.2.40-src/native directory Create the Apache configureation directory structure inside the FPM build root sudo mkdir -p fpm_build_root/etc/httpd/conf.d/ Create the Apache Lib modules directory structure inside the FPM build root sudo mkdir -p fpm_build_root/usr/lib64/httpd/modules/ Populate the FPM Build Root With the FPM build root directory structure(s) create we can create the needed Apache configuration files and copy the newly built module into place Note! The commands reference below are being run from inside the tomcat-connectors-1.2.40-src/native directory Create a mod_jk.conf Apache configuration file to load the module and apply come basic settings. The exapmle below is one that has been used in a production environment for basic use sudo bash -c cat EOF fpm_build_root/etc/httpd/conf.d/mod_jk.conf # Load mod_jk module # Specify the filename of the mod_jk lib LoadModule jk_module modules/mod_jk.so # Where to find workers.properties JkWorkersFile conf.d/workers.properties # Where to put jk logs JkLogFile logs/mod_jk.log # Set the jk log level [debug/error/info] JkLogLevel info # Select the log format JkLogStampFormat [%a %b %d %H:%M:%S %Y] # JkRequestLogFormat JkRequestLogFormat %w %V %T # Add shared memory. # This directive is present with 1.2.10 and # later versions of mod_jk, and is needed for # for load balancing to work properly JkShmFile logs/jk.shm EOF Create a Apache Tomcat Connetor module (mod-jk) workers.properties file referenced in the mod_jk.conf file. This file will be empty touch fpm_build_root/etc/httpd/conf.d/workers.properties Copy the newly built Apache Tomcat Connetor module (mod-jk) into the FPM build root sudo cp apache-2.0/mod_jk.so fpm_build_root/usr/lib64/httpd/modules/ Create the Apache Tomcat Connetor module (mod-jk) RPM Package Warning! You must be at the top level of the FPM build root for it to correctly read the directory structures needed to generate the RPM package Run the FPM to generate the RPM package. Note the -p and -v flags contain the version of the built module fpm --iteration = 1 --rpm-use-file-permissions --verbose -s dir -t rpm -p /root/mod_jk-1.2.40.rpm -n mod_jk -v 1 .2.40 ./ Veify The RPM Package Version/Files Verify the package version rpm -qp /root/mod_jk-1.2.40.rpm Verify the package files (contents) rpm -qp --filesbypkg /root/mod_jk-1.2.40.rpm Disclaimer","title":"Building Apache Tomcat Connetor module (mod-jk) RPM with FPM on Centos 6.x"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#building-apache-tomcat-connetor-module-mod-jk-rpm-with-fpm-on-centos-6x","text":"The Apache Tomat Connector module (mod_jk) is not currently in included Centos or EPEL repositories. To assist in the deployment and management of the Apache mod_jk module a RPM package will be created from the built source using FPM . If you havn't stumbled upon or used FPM I highly recommend taking a look.","title":"Building Apache Tomcat Connetor module (mod-jk) RPM with FPM on Centos 6.x"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#download-the-latest-tomcat-connector-source","text":"Tomcat-Connector Source Index . As of the writing of this article 1.2.40","title":"Download the latest tomcat-connector source"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#verify-build-prerequisites","text":"The following packages will need to be installed in order to correctly build the Apache Tomcat Connetor module (mod-jk). Apache sudo yum install httpd Apache Extension Tool (apxs) sudo yum install apache-devel GCC sudo yum instll gcc","title":"Verify Build Prerequisites"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#build-the-apache-tomcat-connetor-module-mod-jk","text":"Extract the downloaded source tar -zxvf tomcat-connectors-1.2.40-src.tar.gz Navigate into the /native subdirectory of the extracted source cd tomcat-connectors-1.2.40-src/native Run the configure script with apxs set ./configure --with-apxs = /usr/sbin/apxs Complie/Make the module make At this point the module is build and could be copied directly to a servers /usr/[lib/lib64]/httpd/mouldes directory to be used or tested.","title":"Build the Apache Tomcat Connetor module (mod-jk)"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#create-fpm-build-root","text":"FPM will be used to generate an RPM package from a build root source containing the mimiced directory structure and files to be installed on the system. This means if we want to install the mod_jk.so module file via the rpm into /usr/lib64/httpd/modules we need to create correct diretory structure inside the build root directory for FPM to reference. This can be created anywhere on your file system, but for ease of management and reference I recommand creating the build root in the /native subdirectory of the extracted source Note! The commands reference below are being run from inside the tomcat-connectors-1.2.40-src/native directory Create the Apache configureation directory structure inside the FPM build root sudo mkdir -p fpm_build_root/etc/httpd/conf.d/ Create the Apache Lib modules directory structure inside the FPM build root sudo mkdir -p fpm_build_root/usr/lib64/httpd/modules/","title":"Create FPM Build Root"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#populate-the-fpm-build-root","text":"With the FPM build root directory structure(s) create we can create the needed Apache configuration files and copy the newly built module into place Note! The commands reference below are being run from inside the tomcat-connectors-1.2.40-src/native directory Create a mod_jk.conf Apache configuration file to load the module and apply come basic settings. The exapmle below is one that has been used in a production environment for basic use sudo bash -c cat EOF fpm_build_root/etc/httpd/conf.d/mod_jk.conf # Load mod_jk module # Specify the filename of the mod_jk lib LoadModule jk_module modules/mod_jk.so # Where to find workers.properties JkWorkersFile conf.d/workers.properties # Where to put jk logs JkLogFile logs/mod_jk.log # Set the jk log level [debug/error/info] JkLogLevel info # Select the log format JkLogStampFormat [%a %b %d %H:%M:%S %Y] # JkRequestLogFormat JkRequestLogFormat %w %V %T # Add shared memory. # This directive is present with 1.2.10 and # later versions of mod_jk, and is needed for # for load balancing to work properly JkShmFile logs/jk.shm EOF Create a Apache Tomcat Connetor module (mod-jk) workers.properties file referenced in the mod_jk.conf file. This file will be empty touch fpm_build_root/etc/httpd/conf.d/workers.properties Copy the newly built Apache Tomcat Connetor module (mod-jk) into the FPM build root sudo cp apache-2.0/mod_jk.so fpm_build_root/usr/lib64/httpd/modules/","title":"Populate the FPM Build Root"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#create-the-apache-tomcat-connetor-module-mod-jk-rpm-package","text":"Warning! You must be at the top level of the FPM build root for it to correctly read the directory structures needed to generate the RPM package Run the FPM to generate the RPM package. Note the -p and -v flags contain the version of the built module fpm --iteration = 1 --rpm-use-file-permissions --verbose -s dir -t rpm -p /root/mod_jk-1.2.40.rpm -n mod_jk -v 1 .2.40 ./","title":"Create the Apache Tomcat Connetor module (mod-jk) RPM Package"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#veify-the-rpm-package-versionfiles","text":"Verify the package version rpm -qp /root/mod_jk-1.2.40.rpm Verify the package files (contents) rpm -qp --filesbypkg /root/mod_jk-1.2.40.rpm","title":"Veify The RPM Package Version/Files"},{"location":"linux/administrative/building_apache_tomcat_connetor_mod-jk_rpm_with_fpm_on_centos_6.x/#disclaimer","text":"","title":"Disclaimer"},{"location":"linux/administrative/compose_mail_from_command_line/","text":"Compose Mail From Command Line Composing mail from command line uses the following format: mail -s For example write mail to mail -s Hello boss@example.com Then type in your message, followed by an \u2018control-D\u2019 at the beginning of a line. To stop simply type dot (.): Output: Hi, This is a test . Cc:","title":"Compose Mail From Command Line"},{"location":"linux/administrative/compose_mail_from_command_line/#compose-mail-from-command-line","text":"Composing mail from command line uses the following format: mail -s For example write mail to mail -s Hello boss@example.com Then type in your message, followed by an \u2018control-D\u2019 at the beginning of a line. To stop simply type dot (.): Output: Hi, This is a test . Cc:","title":"Compose Mail From Command Line"},{"location":"linux/administrative/create_rpm_build_environment/","text":"Create RPM Build Environment Overview Create an RPM Build Environment for a unprivilged user account. Warning! Building RPMs should NEVER be done as the root user. It shoudl ALWAYS be done with an unprivileged user account. Building RPMs as root could cause potential damage to your system. Prerequisites Install the rpm-build package sudo yum install rpm-build Most SRPMs targetted to be rebuilt on CentOS also need certain rpmbuild build macros and helper scripts, which are contained in package: redhat-rpm-config. To get results as desired, you should also install it in the same fashion as noted above, substituting the new package name. sudo yum install redhat-rpm-config Building packages will require various compilers, install a few commons ones to get you started sudo yum install gcc gcc-c++ make Create directories for RPM building Create RPM build directory structure in your home directory mkdir -p ~/rpmbuild/ { BUILD,RPMS,SOURCES,SPECS,SRPMS } Create .rpm macros file hunder your home directory echo %_topdir %(echo $HOME)/rpmbuild ~/.rpmmacros References http://wiki.centos.org/HowTos/SetupRpmBuildEnvironment","title":"Create RPM Build Environment"},{"location":"linux/administrative/create_rpm_build_environment/#create-rpm-build-environment","text":"","title":"Create RPM Build Environment"},{"location":"linux/administrative/create_rpm_build_environment/#overview","text":"Create an RPM Build Environment for a unprivilged user account. Warning! Building RPMs should NEVER be done as the root user. It shoudl ALWAYS be done with an unprivileged user account. Building RPMs as root could cause potential damage to your system.","title":"Overview"},{"location":"linux/administrative/create_rpm_build_environment/#prerequisites","text":"Install the rpm-build package sudo yum install rpm-build Most SRPMs targetted to be rebuilt on CentOS also need certain rpmbuild build macros and helper scripts, which are contained in package: redhat-rpm-config. To get results as desired, you should also install it in the same fashion as noted above, substituting the new package name. sudo yum install redhat-rpm-config Building packages will require various compilers, install a few commons ones to get you started sudo yum install gcc gcc-c++ make","title":"Prerequisites"},{"location":"linux/administrative/create_rpm_build_environment/#create-directories-for-rpm-building","text":"Create RPM build directory structure in your home directory mkdir -p ~/rpmbuild/ { BUILD,RPMS,SOURCES,SPECS,SRPMS } Create .rpm macros file hunder your home directory echo %_topdir %(echo $HOME)/rpmbuild ~/.rpmmacros","title":"Create directories for RPM building"},{"location":"linux/administrative/create_rpm_build_environment/#references","text":"http://wiki.centos.org/HowTos/SetupRpmBuildEnvironment","title":"References"},{"location":"linux/administrative/determine_current_running_linux_kernel/","text":"Determine Current Running Linux Kernel Commands Print kernel release sudo uname -r Print kernel version sudo uname -v Usage Usage: uname [ OPTION ] ... Print certain system information. With no OPTION, same as -s. -a, --all print all information, in the following order, except omit -p and -i if unknown: -s, --kernel-name print the kernel name -n, --nodename print the network node hostname -r, --kernel-release print the kernel release -v, --kernel-version print the kernel version -m, --machine print the machine hardware name -p, --processor print the processor type or unknown -i, --hardware-platform print the hardware platform or unknown -o, --operating-system print the operating system --help display this help and exit --version output version information and exit","title":"Determine Current Running Linux Kernel"},{"location":"linux/administrative/determine_current_running_linux_kernel/#determine-current-running-linux-kernel","text":"","title":"Determine Current Running Linux Kernel"},{"location":"linux/administrative/determine_current_running_linux_kernel/#commands","text":"Print kernel release sudo uname -r Print kernel version sudo uname -v","title":"Commands"},{"location":"linux/administrative/determine_current_running_linux_kernel/#usage","text":"Usage: uname [ OPTION ] ... Print certain system information. With no OPTION, same as -s. -a, --all print all information, in the following order, except omit -p and -i if unknown: -s, --kernel-name print the kernel name -n, --nodename print the network node hostname -r, --kernel-release print the kernel release -v, --kernel-version print the kernel version -m, --machine print the machine hardware name -p, --processor print the processor type or unknown -i, --hardware-platform print the hardware platform or unknown -o, --operating-system print the operating system --help display this help and exit --version output version information and exit","title":"Usage"},{"location":"linux/administrative/determine_the_location_of_linux_kernel_driver/","text":"Determine The Location of Linux Kernel Driver Note! /lib/modules/kernel-version/ directory stores all compiled drivers under Linux operating system. Display current modules ls -l /lib/modules/ $( uname -r ) Navigate to current modules directory cd /lib/modules/ $( uname -r )","title":"Determine The Location of Linux Kernel Driver"},{"location":"linux/administrative/determine_the_location_of_linux_kernel_driver/#determine-the-location-of-linux-kernel-driver","text":"Note! /lib/modules/kernel-version/ directory stores all compiled drivers under Linux operating system. Display current modules ls -l /lib/modules/ $( uname -r ) Navigate to current modules directory cd /lib/modules/ $( uname -r )","title":"Determine The Location of Linux Kernel Driver"},{"location":"linux/administrative/dynamic_motd/","text":"Dynamic MOTD Overview Enabling an automatic message on each SSH connection, with a dynamic MOTD. Configure By default the SSH connection cats the file /etc/motd. You can disable this by editing the sshd_config and restarting the sshd service Modify the ** #PrintMotd yes ** line Hint! This is not required: You can also leave the /etc/motd file blank or have a static and dynamic combo PrintMotd no Configure the PAM connection module. Add the following at the end of the file ** /etc/pam.d/login ** session optional pam_motd.so Configure the ** /etc/profile ** file to include executing the dynamic MOTD script /usr/local/bin/dynmotd Hint! The dynamic MOTD script below is using facter (Puppet Labs) to gather system information Create a dynamic MOTD script ** /usr/local/bin/dynmotd *** #!/bin/bash echo -e \\n facts = ` facter fqdn is_virtual lsbdistdescription architecture kernelversion uptime physicalprocessorcount processor0 memorytotal ipaddress ` echo Hostname: `echo ${ facts } | awk -F fqdn = |ipaddress { print $2 } ` echo Virtual Machine: `echo ${ facts } | awk -F is_virtual = |kernelversion { print $2 } ` echo OS: `echo ${ facts } | awk -F lsbdistdescription = |memorytotal { print $2 } ` echo Architecture: `echo ${ facts } | awk -F architecture = |fqdn { print $2 } ` echo Kernel: `echo ${ facts } | awk -F kernelversion = |lsbdistdescription { print $2 } ` echo Up Time: `echo ${ facts } | awk -F uptime = | $ { print $2 } ` echo Processors Count: `echo ${ facts } | awk -F physicalprocessorcount = |processor0 { print $2 } ` echo Processor Info: `echo ${ facts } | awk -F processor0 = |uptime { print $2 } ` echo Memory: `echo ${ facts } | awk -F memorytotal = |physicalprocessorcount { print $2 } ` echo IP: `echo ${ facts } | awk -F ipaddress = |is_virtual { print $2 } ` Modify the permissions to make the dynamic MOTD script executable sudo chmod +x /usr/local/bin/dynmotd Testing ~ \ue0b0 ssh techtaco.org Last login: Mon Aug 4 08 :40:15 2014 from core.arin.net Hostname: R2.droids.local Virtual Machine: false OS: CentOS release 6 .5 ( Final ) Architecture: i386 Kernel: 2 .6.32 Up Time: 41 days Processors Count: 1 Processor Info: Intel ( R ) Atom ( TM ) CPU D525 @ 1 .80GHz Memory: 3 .82 GB IP: 192 .168.1.254 [ smbambling@R2 ~ ] $","title":"Dynamic MOTD"},{"location":"linux/administrative/dynamic_motd/#dynamic-motd","text":"","title":"Dynamic MOTD"},{"location":"linux/administrative/dynamic_motd/#overview","text":"Enabling an automatic message on each SSH connection, with a dynamic MOTD.","title":"Overview"},{"location":"linux/administrative/dynamic_motd/#configure","text":"By default the SSH connection cats the file /etc/motd. You can disable this by editing the sshd_config and restarting the sshd service Modify the ** #PrintMotd yes ** line Hint! This is not required: You can also leave the /etc/motd file blank or have a static and dynamic combo PrintMotd no Configure the PAM connection module. Add the following at the end of the file ** /etc/pam.d/login ** session optional pam_motd.so Configure the ** /etc/profile ** file to include executing the dynamic MOTD script /usr/local/bin/dynmotd Hint! The dynamic MOTD script below is using facter (Puppet Labs) to gather system information Create a dynamic MOTD script ** /usr/local/bin/dynmotd *** #!/bin/bash echo -e \\n facts = ` facter fqdn is_virtual lsbdistdescription architecture kernelversion uptime physicalprocessorcount processor0 memorytotal ipaddress ` echo Hostname: `echo ${ facts } | awk -F fqdn = |ipaddress { print $2 } ` echo Virtual Machine: `echo ${ facts } | awk -F is_virtual = |kernelversion { print $2 } ` echo OS: `echo ${ facts } | awk -F lsbdistdescription = |memorytotal { print $2 } ` echo Architecture: `echo ${ facts } | awk -F architecture = |fqdn { print $2 } ` echo Kernel: `echo ${ facts } | awk -F kernelversion = |lsbdistdescription { print $2 } ` echo Up Time: `echo ${ facts } | awk -F uptime = | $ { print $2 } ` echo Processors Count: `echo ${ facts } | awk -F physicalprocessorcount = |processor0 { print $2 } ` echo Processor Info: `echo ${ facts } | awk -F processor0 = |uptime { print $2 } ` echo Memory: `echo ${ facts } | awk -F memorytotal = |physicalprocessorcount { print $2 } ` echo IP: `echo ${ facts } | awk -F ipaddress = |is_virtual { print $2 } ` Modify the permissions to make the dynamic MOTD script executable sudo chmod +x /usr/local/bin/dynmotd","title":"Configure"},{"location":"linux/administrative/dynamic_motd/#testing","text":"~ \ue0b0 ssh techtaco.org Last login: Mon Aug 4 08 :40:15 2014 from core.arin.net Hostname: R2.droids.local Virtual Machine: false OS: CentOS release 6 .5 ( Final ) Architecture: i386 Kernel: 2 .6.32 Up Time: 41 days Processors Count: 1 Processor Info: Intel ( R ) Atom ( TM ) CPU D525 @ 1 .80GHz Memory: 3 .82 GB IP: 192 .168.1.254 [ smbambling@R2 ~ ] $","title":"Testing"},{"location":"linux/administrative/nav/","text":"Administrative Articles Dynamic MOTD Sun Java JDK RPM Build The JPackage Way Modified Building Apache Tomcat Connetor (mod-jk) RPM with FPM on Centos 6.x Create RPM Build Environment Compose Mail From Command Line Use DD To Create An ISO Determine The Location Of Linux Kernel Driver Determine Your Current Running Linux Kernel Remove Unused Kernels","title":"Administrative"},{"location":"linux/administrative/nav/#administrative-articles","text":"Dynamic MOTD Sun Java JDK RPM Build The JPackage Way Modified Building Apache Tomcat Connetor (mod-jk) RPM with FPM on Centos 6.x Create RPM Build Environment Compose Mail From Command Line Use DD To Create An ISO Determine The Location Of Linux Kernel Driver Determine Your Current Running Linux Kernel Remove Unused Kernels","title":"Administrative Articles"},{"location":"linux/administrative/remove_unused_kernels/","text":"Remove Unused Kernels Overview When running yum update you can some times revive the error below Error Summary ------------- Disk Requirements: At least 10MB more space needed on the /boot filesystem. To get around this error you can remove unused kernels from the system to create space. You'll need to make sure that you do not remove the current running kernel from the system. You can use the uname -r command to view the current running kernel that you should not remove. uname -r 2 .6.32-220.4.2.el6.x86_64 List Current Installed Kernels You can view the current installed kernels on the systems by running the following command. rpm -qa | grep kernel | grep -e ^kernel You can view all no running kernels on the system with the following command. rpm -qa | grep kernel | grep -e ^kernel | grep -v `uname -r`\\|headers\\|firmware Clean-Up Space and Yum Update To make this process easier you can run the following command to remove the unused kernels and kick off a yum update. I've purposely not thrown the -y switch via yum remove so that you can review any dependencies that might be removed along with the kernel. This command will only proceed from command to command if the previous command is successful. REMOVE = $( rpm -qa | grep kernel | grep -e ^kernel | grep -v `uname -r`\\|headers\\|firmware ) sudo yum remove ${ REMOVE } sudo yum update -y You can also run this in a loop with a little escaping for i in HOST1 HOST2 ; REMOVE = ` ssh $i rpm -qa kernel \\* | grep -v \\` uname -r \\`\\| headers \\| firmware ` ; ssh $i yum -y remove $REMOVE ; ssh $i yum -y update ; done","title":"Remove Unused Kernels"},{"location":"linux/administrative/remove_unused_kernels/#remove-unused-kernels","text":"","title":"Remove Unused Kernels"},{"location":"linux/administrative/remove_unused_kernels/#overview","text":"When running yum update you can some times revive the error below Error Summary ------------- Disk Requirements: At least 10MB more space needed on the /boot filesystem. To get around this error you can remove unused kernels from the system to create space. You'll need to make sure that you do not remove the current running kernel from the system. You can use the uname -r command to view the current running kernel that you should not remove. uname -r 2 .6.32-220.4.2.el6.x86_64","title":"Overview"},{"location":"linux/administrative/remove_unused_kernels/#list-current-installed-kernels","text":"You can view the current installed kernels on the systems by running the following command. rpm -qa | grep kernel | grep -e ^kernel You can view all no running kernels on the system with the following command. rpm -qa | grep kernel | grep -e ^kernel | grep -v `uname -r`\\|headers\\|firmware","title":"List Current Installed Kernels"},{"location":"linux/administrative/remove_unused_kernels/#clean-up-space-and-yum-update","text":"To make this process easier you can run the following command to remove the unused kernels and kick off a yum update. I've purposely not thrown the -y switch via yum remove so that you can review any dependencies that might be removed along with the kernel. This command will only proceed from command to command if the previous command is successful. REMOVE = $( rpm -qa | grep kernel | grep -e ^kernel | grep -v `uname -r`\\|headers\\|firmware ) sudo yum remove ${ REMOVE } sudo yum update -y You can also run this in a loop with a little escaping for i in HOST1 HOST2 ; REMOVE = ` ssh $i rpm -qa kernel \\* | grep -v \\` uname -r \\`\\| headers \\| firmware ` ; ssh $i yum -y remove $REMOVE ; ssh $i yum -y update ; done","title":"Clean-Up Space and Yum Update"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/","text":"Sun Java JDK RPM Build The JPackage Way Modified Building Sun Java 1.6 u31 Download the JDK 1.6 u31 from Oracle Go to http://www.oracle.com/technetwork/java/javase/downloads/index.html Click on the Download button for the Java SE 6 Update 31 JDK Select the Accept License Agreement radio button Download the jdk-6u31-linux-x64.bin for 64bit or jdk-6u31-linux-i586.bin for 32bit file Download the Timezone Updater from Oracle Go to http://www.oracle.com/technetwork/java/javase/downloads/index.html Click on the Download button for JDK DST Timezone Update Tool - 1.3.45 Select the Accept License Agreement radio button Download the tzupdater-1_3_45-2011n.zip file Create RPM Build Environment (If needed) More to come on this\u2026 Move/Copy Updated/JDK into SOURCES If you building on a remote machine you\u2019ll need to transfer the files to that build server via SCP or some other preferred method mv tzupdater-1_3_45-2011n.zip ~/rpms/SOURCES/ mv jdk-6u31-linux-x64.bin ~/rpms/SOURCES/ Download java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm This is being performed on your build server. We will download directly to the SRPM build directory to keep things clean wget wget http://mirror.city-fan.org/ftp/contrib/java/java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm ~/rpms/SRPMS/ Build Sun Java RPMs rpmbuild --rebuild ~/rpms/SRPMS/java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm After completion you should have the following RPMs in ~/rpms/RPMS/x86_64 java-1.6.0-sun-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-demo-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-devel-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-jdbc-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-plugin-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-src-1.6.0.31-1.0.cf.x86_64.rpm Installing Java JDK (Development Environment) yum install java-1.6.0-sun-devel Check Java Version [ host1 ] ~ ] $ java -version java version 1.6.0_31 Java ( TM ) SE Runtime Environment ( build 1 .6.0_31-b04 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 20 .6-b01, mixed mode )","title":"Sun Java JDK RPM Build The JPackage Way Modified"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#sun-java-jdk-rpm-build-the-jpackage-way-modified","text":"","title":"Sun Java JDK RPM Build The JPackage Way Modified"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#building-sun-java-16-u31","text":"","title":"Building Sun Java 1.6 u31"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#download-the-jdk-16-u31-from-oracle","text":"Go to http://www.oracle.com/technetwork/java/javase/downloads/index.html Click on the Download button for the Java SE 6 Update 31 JDK Select the Accept License Agreement radio button Download the jdk-6u31-linux-x64.bin for 64bit or jdk-6u31-linux-i586.bin for 32bit file","title":"Download the JDK 1.6 u31 from Oracle"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#download-the-timezone-updater-from-oracle","text":"Go to http://www.oracle.com/technetwork/java/javase/downloads/index.html Click on the Download button for JDK DST Timezone Update Tool - 1.3.45 Select the Accept License Agreement radio button Download the tzupdater-1_3_45-2011n.zip file","title":"Download the Timezone Updater from Oracle"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#create-rpm-build-environment-if-needed","text":"More to come on this\u2026","title":"Create RPM Build Environment (If needed)"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#movecopy-updatedjdk-into-sources","text":"If you building on a remote machine you\u2019ll need to transfer the files to that build server via SCP or some other preferred method mv tzupdater-1_3_45-2011n.zip ~/rpms/SOURCES/ mv jdk-6u31-linux-x64.bin ~/rpms/SOURCES/","title":"Move/Copy Updated/JDK into SOURCES"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#download-java-160-sun-16031-10cfnosrcrpm","text":"This is being performed on your build server. We will download directly to the SRPM build directory to keep things clean wget wget http://mirror.city-fan.org/ftp/contrib/java/java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm ~/rpms/SRPMS/","title":"Download java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#build-sun-java-rpms","text":"rpmbuild --rebuild ~/rpms/SRPMS/java-1.6.0-sun-1.6.0.31-1.0.cf.nosrc.rpm After completion you should have the following RPMs in ~/rpms/RPMS/x86_64 java-1.6.0-sun-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-demo-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-devel-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-jdbc-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-plugin-1.6.0.31-1.0.cf.x86_64.rpm java-1.6.0-sun-src-1.6.0.31-1.0.cf.x86_64.rpm","title":"Build Sun Java RPMs"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#installing-java-jdk-development-environment","text":"yum install java-1.6.0-sun-devel","title":"Installing Java JDK (Development Environment)"},{"location":"linux/administrative/sun_java_jdk_rpm_build_the_jpackage_way_modified/#check-java-version","text":"[ host1 ] ~ ] $ java -version java version 1.6.0_31 Java ( TM ) SE Runtime Environment ( build 1 .6.0_31-b04 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 20 .6-b01, mixed mode )","title":"Check Java Version"},{"location":"linux/administrative/use_dd_to_create_an_iso/","text":"Use DD To Create An ISO Explanation dd means \u201cdisk dump.\u201d if means \u201cinput file.\u201d of means \u201coutput file.\u201d Command Example: dd if = /dev/cdrom of = /cdrom_image.iso","title":"Use DD To Create An ISO"},{"location":"linux/administrative/use_dd_to_create_an_iso/#use-dd-to-create-an-iso","text":"Explanation dd means \u201cdisk dump.\u201d if means \u201cinput file.\u201d of means \u201coutput file.\u201d Command Example: dd if = /dev/cdrom of = /cdrom_image.iso","title":"Use DD To Create An ISO"},{"location":"linux/apache/create_hidden_divs/","text":"Create Hidden Divs HTML Call your DIV as you would normal links a href= # onclick= show(1) link 1 /a br a href= # onclick= show(2) link 2 /a br a href= # onclick= show(3) link 3 /a br a href= # onclick= show(4) link 4 /a br The DIV's must have the id \u201ccontent_\u201d followed by a number. This number related to the link above. The Default DIV has no \u201clay\u201d class. This will be displayed when the page loads. div id= content_1 Div one is the default Content. br It will be covered by the other 2 links. br /div Hidden/ No Default div id= content_2 class= lay Div 2 Content is Here br Place What ever you would like!!! /div div id= content_3 class= lay Div 3 Content is Here br Place What ever you would like!!! /div div id= content_4 class= lay Div 4 Content is Here br Place What ever you would like!!! /div CSS The \u201clay\u201d class will tell the browser to hide the DIV unless clicked on. style type= text/css a,a:visited{ color: #0000FF; text-decoration: none; } a:hover{ color: #0000FF; text-decoration: underline; } .lay { display: none; position: relative; } /style Java You must set the number of hidden fields. script n = 4; // number of hidden layers function show(a){ for(i=1;i =n;i++){ document.getElementById( content_ +i).style.display = none ;} document.getElementById( content_ +a).style.display = inline ; } /script","title":"Create Hidden Divs"},{"location":"linux/apache/create_hidden_divs/#create-hidden-divs","text":"","title":"Create Hidden Divs"},{"location":"linux/apache/create_hidden_divs/#html","text":"Call your DIV as you would normal links a href= # onclick= show(1) link 1 /a br a href= # onclick= show(2) link 2 /a br a href= # onclick= show(3) link 3 /a br a href= # onclick= show(4) link 4 /a br The DIV's must have the id \u201ccontent_\u201d followed by a number. This number related to the link above. The Default DIV has no \u201clay\u201d class. This will be displayed when the page loads. div id= content_1 Div one is the default Content. br It will be covered by the other 2 links. br /div Hidden/ No Default div id= content_2 class= lay Div 2 Content is Here br Place What ever you would like!!! /div div id= content_3 class= lay Div 3 Content is Here br Place What ever you would like!!! /div div id= content_4 class= lay Div 4 Content is Here br Place What ever you would like!!! /div","title":"HTML"},{"location":"linux/apache/create_hidden_divs/#css","text":"The \u201clay\u201d class will tell the browser to hide the DIV unless clicked on. style type= text/css a,a:visited{ color: #0000FF; text-decoration: none; } a:hover{ color: #0000FF; text-decoration: underline; } .lay { display: none; position: relative; } /style","title":"CSS"},{"location":"linux/apache/create_hidden_divs/#java","text":"You must set the number of hidden fields. script n = 4; // number of hidden layers function show(a){ for(i=1;i =n;i++){ document.getElementById( content_ +i).style.display = none ;} document.getElementById( content_ +a).style.display = inline ; } /script","title":"Java"},{"location":"linux/apache/create_ldap_authorizatoin_for_a_website/","text":"Create LDAP Authorization For A Website Create Virtual Host for you site Navigate to Virtual host .conf file cd /etc/httpd/conf.d/x.conf Edit the virutal host .conf file vi /etc/httpd/conf.d/x.conf Location / --This is the location on the website that you want to password protect AuthName MKI REPO --This is the name that shows up on the pop up asking for password AuthType Basic -- This is the authentication type AuthBasicProvider ldap -- This is the authentication provider (ldap) AuthzLDAPAuthoritative Off -- Not sure AuthLDAPBindDN uid=svnldapuser,ou=Users,dc=mkisystems,dc=com -- This is the user that binds and searches ldap db AuthLDAPBindPassword `d5E Q0RnA(c;eM=q72 -- This is the users password for ldap auth AuthLDAPURL ldap://127.0.0.1/ou=Users,dc=mkisystems,dc=com -- This is the ldap db to connect to AuthLDAPGroupAttribute memberUid -- this is the attribute that is searched for AuthLDAPGroupAttributeIsDN off -- Not sure Require ldap-group cn=SVN,ou=Groups,dc=mkisystems,dc=com -- This is the group in which members but be a part of to Auth /Location","title":"Create LDAP Authorization For A Website"},{"location":"linux/apache/create_ldap_authorizatoin_for_a_website/#create-ldap-authorization-for-a-website","text":"Create Virtual Host for you site Navigate to Virtual host .conf file cd /etc/httpd/conf.d/x.conf Edit the virutal host .conf file vi /etc/httpd/conf.d/x.conf Location / --This is the location on the website that you want to password protect AuthName MKI REPO --This is the name that shows up on the pop up asking for password AuthType Basic -- This is the authentication type AuthBasicProvider ldap -- This is the authentication provider (ldap) AuthzLDAPAuthoritative Off -- Not sure AuthLDAPBindDN uid=svnldapuser,ou=Users,dc=mkisystems,dc=com -- This is the user that binds and searches ldap db AuthLDAPBindPassword `d5E Q0RnA(c;eM=q72 -- This is the users password for ldap auth AuthLDAPURL ldap://127.0.0.1/ou=Users,dc=mkisystems,dc=com -- This is the ldap db to connect to AuthLDAPGroupAttribute memberUid -- this is the attribute that is searched for AuthLDAPGroupAttributeIsDN off -- Not sure Require ldap-group cn=SVN,ou=Groups,dc=mkisystems,dc=com -- This is the group in which members but be a part of to Auth /Location","title":"Create LDAP Authorization For A Website"},{"location":"linux/apache/nav/","text":"Apache Articles Create LDAP Authorization For A Website Create Hidden DIVS","title":"Apache"},{"location":"linux/apache/nav/#apache-articles","text":"Create LDAP Authorization For A Website Create Hidden DIVS","title":"Apache Articles"},{"location":"linux/clustering/nav/","text":"Clustering Articles PCS CRMSH Quick Reference","title":"Clustering"},{"location":"linux/clustering/nav/#clustering-articles","text":"PCS CRMSH Quick Reference","title":"Clustering Articles"},{"location":"linux/clustering/pcs_crmsh_quick_reference/","text":"PCS CRMSH Quick Reference References ClusterLabs Doc General Operations Display the configuration crmsh # crm configure show xml pcs # pcs cluster cib crmsh can show a simplified (non-xml) syntax as well crmsh # crm configure show Display the current status crmsh # crm status pcs # pcs status also # crm_mon -1 Node standby Put node in standby crmsh # crm node standby pcmk-1 pcs # pcs cluster standby pcmk-1 Remove node from standby crmsh # crm node online pcmk-1 pcs # pcs cluster unstandby pcmk-1 crm has the ability to set the status on reboot or forever. pcs can apply the change to all the nodes. Set cluster property crmsh # crm configure property stonith-enabled=false pcs # pcs property set stonith-enabled=false Resource manipulation List Resource Agent (RA) classes crmsh # crm ra classes pcs # pcs resource standards List available RAs crmsh # crm ra list ocf crmsh # crm ra list lsb crmsh # crm ra list service crmsh # crm ra list stonith pcs # pcs resource agents ocf pcs # pcs resource agents lsb pcs # pcs resource agents service pcs # pcs resource agents stonith pcs # pcs resource agents You can also filter by provider crmsh # crm ra list ocf pacemaker pcs # pcs resource agents ocf:pacemaker List RA info crmsh # crm ra meta IPaddr2 pcs # pcs resource describe IPaddr2 Use any RA name (like IPaddr2) from the list displayed with the previous command You can also use the full class:provider:RA format if multiple RAs with the same name are available : crmsh # crm ra meta ocf:heartbeat:IPaddr2 pcs # pcs resource describe ocf:heartbeat:IPaddr2 Create a resource crmsh # crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \\ params ip=192.168.122.120 cidr_netmask=32 \\ op monitor interval=30s pcs # pcs resource create ClusterIP IPaddr2 ip=192.168.0.120 cidr_netmask=32 The standard and provider ( ocf:heartbeat ) are determined automatically since IPaddr2 is unique. The monitor operation is automatically created based on the agent's metadata. Display a resource crmsh # crm configure show pcs # pcs resource show crmsh also displays fencing resources. The result can be filtered by supplying a resource name (IE ClusterIP ): crmsh # crm configure show ClusterIP pcs # pcs resource show ClusterIP crmsh also displays fencing resources. Display fencing resources crmsh # crm resource show pcs # pcs stonith show pcs treats STONITH devices separately. Display Stonith RA info crmsh # crm ra meta stonith:fence_ipmilan pcs # pcs stonith describe fence_ipmilan Start a resource crmsh # crm resource start ClusterIP pcs # pcs resource enable ClusterIP Stop a resource crmsh # crm resource stop ClusterIP pcs # pcs resource disable ClusterIP Remove a resource crmsh # crm configure delete ClusterIP pcs # pcs resource delete ClusterIP Modify a resource crmsh # crm resource param ClusterIP set clusterip_hash=sourceip pcs # pcs resource update ClusterIP clusterip_hash=sourceip Delete parameters for a given resource crmsh # crm resource param ClusterIP delete nic pcs # pcs resource update ClusterIP ip=192.168.0.98 nic= List the current resource defaults crmsh # crm configure show type:rsc_defaults pcs # pcs resource rsc defaults Set resource defaults crmsh # crm configure rsc_defaults resource-stickiness=100 pcs # pcs resource rsc defaults resource-stickiness=100 List the current operation defaults crmsh # crm configure show type:op_defaults pcs # pcs resource op defaults Set operation defaults crmsh # crm configure op_defaults timeout=240s pcs # pcs resource op defaults timeout=240s Set Colocation crmsh # crm configure colocation website-with-ip INFINITY: WebSite ClusterIP pcs # pcs constraint colocation add ClusterIP with WebSite INFINITY With roles crmsh # crm configure colocation another-ip-with-website inf: AnotherIP WebSite:Master pcs # pcs constraint colocation add Started AnotherIP with Master WebSite INFINITY Set ordering crmsh # crm configure order apache-after-ip mandatory: ClusterIP WebSite pcs # pcs constraint order ClusterIP then WebSite With roles: crmsh # crm configure order ip-after-website Mandatory: WebSite:Master AnotherIP pcs # pcs constraint order promote WebSite then start AnotherIP Set preferred location crmsh # crm configure location prefer-pcmk-1 WebSite 50: pcmk-1 pcs # pcs constraint location WebSite prefers pcmk-1=50 With roles: crmsh # crm configure location prefer-pcmk-1 WebSite rule role=Master 50: \\#uname eq pcmk-1 pcs # pcs constraint location WebSite rule role=master 50 \\#uname eq pcmk-1 Move resources crmsh # crm resource move WebSite pcmk-1 pcs # pcs resource move WebSite pcmk-1 crmsh # crm resource unmove WebSite pcs # pcs resource clear WebSite A resource can also be moved away from a given node: crmsh # pcs # pcs resource ban Website pcmk-2 Remember that moving a resource sets a stickyness to -INF to a given node until unmoved Also, pcs deals with constraints differently. These can be manipulated by the command above as well as the following and others crmsh # pcs # pcs constraint list --full pcs # pcs constraint remove cli-ban-Website-on-pcmk-1 Create a clone crmsh # crm configure clone WebIP ClusterIP meta globally-unique=true clone-max=2 clone-node-max=2 pcs # pcs resource clone ClusterIP globally-unique=true clone-max=2 clone-node-max=2 Create a master/slave clone crmsh # crm configure ms WebDataClone WebData \\ meta master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true pcs # resource master WebDataClone WebData \\ master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 \\ notify=true Other operations Batch changes crmsh # crm crmsh # cib new drbd_cfg crmsh # configure primitive WebData ocf:linbit:drbd params drbd_resource=wwwdata \\ op monitor interval=60s crmsh # configure ms WebDataClone WebData meta master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true crmsh # cib commit drbd_cfg crmsh # quit . pcs # pcs cluster cib drbd_cfg pcs # pcs -f drbd_cfg resource create WebData ocf:linbit:drbd drbd_resource=wwwdata \\ op monitor interval=60s pcs # pcs -f drbd_cfg resource master WebDataClone WebData master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true pcs # pcs cluster push cib drbd_cfg","title":"PCS CRMSH Quick Reference"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#pcs-crmsh-quick-reference","text":"","title":"PCS CRMSH Quick Reference"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#references","text":"ClusterLabs Doc","title":"References"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#general-operations","text":"","title":"General Operations"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#display-the-configuration","text":"crmsh # crm configure show xml pcs # pcs cluster cib crmsh can show a simplified (non-xml) syntax as well crmsh # crm configure show","title":"Display the configuration"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#display-the-current-status","text":"crmsh # crm status pcs # pcs status also # crm_mon -1","title":"Display the current status"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#node-standby","text":"Put node in standby crmsh # crm node standby pcmk-1 pcs # pcs cluster standby pcmk-1 Remove node from standby crmsh # crm node online pcmk-1 pcs # pcs cluster unstandby pcmk-1 crm has the ability to set the status on reboot or forever. pcs can apply the change to all the nodes.","title":"Node standby"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-cluster-property","text":"crmsh # crm configure property stonith-enabled=false pcs # pcs property set stonith-enabled=false","title":"Set cluster property"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#resource-manipulation","text":"","title":"Resource manipulation"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#list-resource-agent-ra-classes","text":"crmsh # crm ra classes pcs # pcs resource standards","title":"List Resource Agent (RA) classes"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#list-available-ras","text":"crmsh # crm ra list ocf crmsh # crm ra list lsb crmsh # crm ra list service crmsh # crm ra list stonith pcs # pcs resource agents ocf pcs # pcs resource agents lsb pcs # pcs resource agents service pcs # pcs resource agents stonith pcs # pcs resource agents You can also filter by provider crmsh # crm ra list ocf pacemaker pcs # pcs resource agents ocf:pacemaker","title":"List available RAs"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#list-ra-info","text":"crmsh # crm ra meta IPaddr2 pcs # pcs resource describe IPaddr2 Use any RA name (like IPaddr2) from the list displayed with the previous command You can also use the full class:provider:RA format if multiple RAs with the same name are available : crmsh # crm ra meta ocf:heartbeat:IPaddr2 pcs # pcs resource describe ocf:heartbeat:IPaddr2","title":"List RA info"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#create-a-resource","text":"crmsh # crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \\ params ip=192.168.122.120 cidr_netmask=32 \\ op monitor interval=30s pcs # pcs resource create ClusterIP IPaddr2 ip=192.168.0.120 cidr_netmask=32 The standard and provider ( ocf:heartbeat ) are determined automatically since IPaddr2 is unique. The monitor operation is automatically created based on the agent's metadata.","title":"Create a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#display-a-resource","text":"crmsh # crm configure show pcs # pcs resource show crmsh also displays fencing resources. The result can be filtered by supplying a resource name (IE ClusterIP ): crmsh # crm configure show ClusterIP pcs # pcs resource show ClusterIP crmsh also displays fencing resources.","title":"Display a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#display-fencing-resources","text":"crmsh # crm resource show pcs # pcs stonith show pcs treats STONITH devices separately.","title":"Display fencing resources"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#display-stonith-ra-info","text":"crmsh # crm ra meta stonith:fence_ipmilan pcs # pcs stonith describe fence_ipmilan","title":"Display Stonith RA info"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#start-a-resource","text":"crmsh # crm resource start ClusterIP pcs # pcs resource enable ClusterIP","title":"Start a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#stop-a-resource","text":"crmsh # crm resource stop ClusterIP pcs # pcs resource disable ClusterIP","title":"Stop a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#remove-a-resource","text":"crmsh # crm configure delete ClusterIP pcs # pcs resource delete ClusterIP","title":"Remove a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#modify-a-resource","text":"crmsh # crm resource param ClusterIP set clusterip_hash=sourceip pcs # pcs resource update ClusterIP clusterip_hash=sourceip","title":"Modify a resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#delete-parameters-for-a-given-resource","text":"crmsh # crm resource param ClusterIP delete nic pcs # pcs resource update ClusterIP ip=192.168.0.98 nic=","title":"Delete parameters for a given resource"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#list-the-current-resource-defaults","text":"crmsh # crm configure show type:rsc_defaults pcs # pcs resource rsc defaults","title":"List the current resource defaults"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-resource-defaults","text":"crmsh # crm configure rsc_defaults resource-stickiness=100 pcs # pcs resource rsc defaults resource-stickiness=100","title":"Set resource defaults"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#list-the-current-operation-defaults","text":"crmsh # crm configure show type:op_defaults pcs # pcs resource op defaults","title":"List the current operation defaults"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-operation-defaults","text":"crmsh # crm configure op_defaults timeout=240s pcs # pcs resource op defaults timeout=240s","title":"Set operation defaults"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-colocation","text":"crmsh # crm configure colocation website-with-ip INFINITY: WebSite ClusterIP pcs # pcs constraint colocation add ClusterIP with WebSite INFINITY With roles crmsh # crm configure colocation another-ip-with-website inf: AnotherIP WebSite:Master pcs # pcs constraint colocation add Started AnotherIP with Master WebSite INFINITY","title":"Set Colocation"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-ordering","text":"crmsh # crm configure order apache-after-ip mandatory: ClusterIP WebSite pcs # pcs constraint order ClusterIP then WebSite With roles: crmsh # crm configure order ip-after-website Mandatory: WebSite:Master AnotherIP pcs # pcs constraint order promote WebSite then start AnotherIP","title":"Set ordering"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#set-preferred-location","text":"crmsh # crm configure location prefer-pcmk-1 WebSite 50: pcmk-1 pcs # pcs constraint location WebSite prefers pcmk-1=50 With roles: crmsh # crm configure location prefer-pcmk-1 WebSite rule role=Master 50: \\#uname eq pcmk-1 pcs # pcs constraint location WebSite rule role=master 50 \\#uname eq pcmk-1","title":"Set preferred location"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#move-resources","text":"crmsh # crm resource move WebSite pcmk-1 pcs # pcs resource move WebSite pcmk-1 crmsh # crm resource unmove WebSite pcs # pcs resource clear WebSite A resource can also be moved away from a given node: crmsh # pcs # pcs resource ban Website pcmk-2 Remember that moving a resource sets a stickyness to -INF to a given node until unmoved Also, pcs deals with constraints differently. These can be manipulated by the command above as well as the following and others crmsh # pcs # pcs constraint list --full pcs # pcs constraint remove cli-ban-Website-on-pcmk-1","title":"Move resources"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#create-a-clone","text":"crmsh # crm configure clone WebIP ClusterIP meta globally-unique=true clone-max=2 clone-node-max=2 pcs # pcs resource clone ClusterIP globally-unique=true clone-max=2 clone-node-max=2","title":"Create a clone"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#create-a-masterslave-clone","text":"crmsh # crm configure ms WebDataClone WebData \\ meta master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true pcs # resource master WebDataClone WebData \\ master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 \\ notify=true","title":"Create a master/slave clone"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#other-operations","text":"","title":"Other operations"},{"location":"linux/clustering/pcs_crmsh_quick_reference/#batch-changes","text":"crmsh # crm crmsh # cib new drbd_cfg crmsh # configure primitive WebData ocf:linbit:drbd params drbd_resource=wwwdata \\ op monitor interval=60s crmsh # configure ms WebDataClone WebData meta master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true crmsh # cib commit drbd_cfg crmsh # quit . pcs # pcs cluster cib drbd_cfg pcs # pcs -f drbd_cfg resource create WebData ocf:linbit:drbd drbd_resource=wwwdata \\ op monitor interval=60s pcs # pcs -f drbd_cfg resource master WebDataClone WebData master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 notify=true pcs # pcs cluster push cib drbd_cfg","title":"Batch changes"},{"location":"linux/git/nav/","text":"GIT Articles Simple Git Workflow With Two Branches","title":"GIT"},{"location":"linux/git/nav/#git-articles","text":"Simple Git Workflow With Two Branches","title":"GIT Articles"},{"location":"linux/git/simple_git_workflow_with_two_branches/","text":"Simple Git Workflow With Two Branches Branches There are two main branches, Master and Deploy. Master contains the usuable scripts and develop should contain working scripts. One qustion that might arrise is, why there is a Master and Develop branch? The reasoning for this is multiple people maybe working on deploy scripts at one time. Having the develop branch allows users to commit updates and pull down each others changes before commiting all changes to the Master branch. This also ensures that the Master branch is stable Workflow The normal workflow for this repository should be the following Working on deploy scripts Create a new feature branch related to the updates you are adding (f_feature_name) Checkout the new feature branch to your work space: git checkout -b f_feature_name Make any needed changes and commit them to your feature branch Merge your changes back into the development branch: git checkout develop Check your current branch to verify your moved to the devlop branch: git branch Merge items back into development branch: git merge `f_feature_name` Continue working on additional features. Once everything has been done for the Release from all participants perform the final merge and tag to Master. Final Master tag and push As the final step, after all testing and script work is done, merge the development changes into the master branch and tag it for the Sprint: git checkout master git merge -m Adding Feature X develop git log --pretty=oneline git tag -a feature_tag commit_hash git push --tags","title":"Simple Git Workflow With Two Branches"},{"location":"linux/git/simple_git_workflow_with_two_branches/#simple-git-workflow-with-two-branches","text":"","title":"Simple Git Workflow With Two Branches"},{"location":"linux/git/simple_git_workflow_with_two_branches/#branches","text":"There are two main branches, Master and Deploy. Master contains the usuable scripts and develop should contain working scripts. One qustion that might arrise is, why there is a Master and Develop branch? The reasoning for this is multiple people maybe working on deploy scripts at one time. Having the develop branch allows users to commit updates and pull down each others changes before commiting all changes to the Master branch. This also ensures that the Master branch is stable","title":"Branches"},{"location":"linux/git/simple_git_workflow_with_two_branches/#workflow","text":"The normal workflow for this repository should be the following","title":"Workflow"},{"location":"linux/git/simple_git_workflow_with_two_branches/#working-on-deploy-scripts","text":"Create a new feature branch related to the updates you are adding (f_feature_name) Checkout the new feature branch to your work space: git checkout -b f_feature_name Make any needed changes and commit them to your feature branch Merge your changes back into the development branch: git checkout develop Check your current branch to verify your moved to the devlop branch: git branch Merge items back into development branch: git merge `f_feature_name` Continue working on additional features. Once everything has been done for the Release from all participants perform the final merge and tag to Master.","title":"Working on deploy scripts"},{"location":"linux/git/simple_git_workflow_with_two_branches/#final-master-tag-and-push","text":"As the final step, after all testing and script work is done, merge the development changes into the master branch and tag it for the Sprint: git checkout master git merge -m Adding Feature X develop git log --pretty=oneline git tag -a feature_tag commit_hash git push --tags","title":"Final Master tag and push"},{"location":"linux/monitoring/graphite/","text":"Graphite Overview Graphite does two things: Store numeric time-series data Render graphs of data on demand However Graphite does NOT collect any data for you. You will need to install a seperate tool taht knows how to send data to graphite. Graphite Architecture graphite webapp : A Django webapp that renders graphs on-demand using Cairo carbon : Twisted daemon that listens for time-series data whisper : Simple database library for storing time-series data (similar in design to RRD) Graphite WebApp This is where you can desing graphs that plot your data. Allows you to save graph properties and layout in seperate dashboards for machine or application. Carbon Carbon is a collection of one or more daemons that make up the storage backend for Graphite. Carbon daemons are responsible for handling data that is sent over by other processes that collect and transmit statistics. There are several Carbon daemons each that handle the recieving of data in a different way. carbon-cache carbon-relay carbon-aggregator Carbon-Cache : Listens for data on a port and writes that data to disk as it arrives, in an efficient way. This daemon stores data in RAM and then flushes it to disk after a predetermined period of time. Note! Carbon-Cache only handles the data receiving and flushing procedures. It does not handle the actual storage mechanisms Multiple instanaces of the Carbon-Cache daemon can be run at once as your number of inbound statistics grows. These can be balanced by Carbon-Relay or Carbon-Aggregator Carbon-Relay : Used for replication or sharding. Either sends requrest to ALL backend daemons for redundancy or shards data across differnet Carbon-Cache instances to spread the load across multiple storage locations. Carbon-Aggregator : Buffer data and then dump it to a Carbon-Cache instance for processing. Whisper Whisper is a database library used to store information that is processed via Carbon. Whisper allows for higher resolution (seconds per point) of recent data to degrade into lower resolutions for long-term retention of historical data. Protocols There are three different protocols that can be used to send data to Graphite. Plain Text is the most straightforward protocol supported by Carbon. The data sent must be in the following format: . On Unix, the nc program can be used to create a socket and send data to Carbon (by default, \u2018plaintext\u2019 runs on port 2003): PORT = 2003 SERVER = graphite.your.org echo local.random.diceroll 4 `date +%s` | nc -q0 ${ SERVER } ${ PORT } Pickle is more effcient then plain text and supports sending batches of metrics to Carbon. The general idea is that the pickled data forms a list of multi-level tuples AMQP Messaging lets you handle large load os data more gracefully. References https://graphite.readthedocs.org/en/latest/overview.html https://www.digitalocean.com/community/tutorials/an-introduction-to-tracking-statistics-with-graphite-statsd-and-collectd https://wiki.icinga.org/display/howtos/graphite","title":"Graphite"},{"location":"linux/monitoring/graphite/#graphite","text":"","title":"Graphite"},{"location":"linux/monitoring/graphite/#overview","text":"Graphite does two things: Store numeric time-series data Render graphs of data on demand However Graphite does NOT collect any data for you. You will need to install a seperate tool taht knows how to send data to graphite.","title":"Overview"},{"location":"linux/monitoring/graphite/#graphite-architecture","text":"graphite webapp : A Django webapp that renders graphs on-demand using Cairo carbon : Twisted daemon that listens for time-series data whisper : Simple database library for storing time-series data (similar in design to RRD)","title":"Graphite Architecture"},{"location":"linux/monitoring/graphite/#graphite-webapp","text":"This is where you can desing graphs that plot your data. Allows you to save graph properties and layout in seperate dashboards for machine or application.","title":"Graphite WebApp"},{"location":"linux/monitoring/graphite/#carbon","text":"Carbon is a collection of one or more daemons that make up the storage backend for Graphite. Carbon daemons are responsible for handling data that is sent over by other processes that collect and transmit statistics. There are several Carbon daemons each that handle the recieving of data in a different way. carbon-cache carbon-relay carbon-aggregator Carbon-Cache : Listens for data on a port and writes that data to disk as it arrives, in an efficient way. This daemon stores data in RAM and then flushes it to disk after a predetermined period of time. Note! Carbon-Cache only handles the data receiving and flushing procedures. It does not handle the actual storage mechanisms Multiple instanaces of the Carbon-Cache daemon can be run at once as your number of inbound statistics grows. These can be balanced by Carbon-Relay or Carbon-Aggregator Carbon-Relay : Used for replication or sharding. Either sends requrest to ALL backend daemons for redundancy or shards data across differnet Carbon-Cache instances to spread the load across multiple storage locations. Carbon-Aggregator : Buffer data and then dump it to a Carbon-Cache instance for processing.","title":"Carbon"},{"location":"linux/monitoring/graphite/#whisper","text":"Whisper is a database library used to store information that is processed via Carbon. Whisper allows for higher resolution (seconds per point) of recent data to degrade into lower resolutions for long-term retention of historical data.","title":"Whisper"},{"location":"linux/monitoring/graphite/#protocols","text":"There are three different protocols that can be used to send data to Graphite. Plain Text is the most straightforward protocol supported by Carbon. The data sent must be in the following format: . On Unix, the nc program can be used to create a socket and send data to Carbon (by default, \u2018plaintext\u2019 runs on port 2003): PORT = 2003 SERVER = graphite.your.org echo local.random.diceroll 4 `date +%s` | nc -q0 ${ SERVER } ${ PORT } Pickle is more effcient then plain text and supports sending batches of metrics to Carbon. The general idea is that the pickled data forms a list of multi-level tuples AMQP Messaging lets you handle large load os data more gracefully.","title":"Protocols"},{"location":"linux/monitoring/graphite/#references","text":"https://graphite.readthedocs.org/en/latest/overview.html https://www.digitalocean.com/community/tutorials/an-introduction-to-tracking-statistics-with-graphite-statsd-and-collectd https://wiki.icinga.org/display/howtos/graphite","title":"References"},{"location":"linux/monitoring/icinga2_pagerduty_configuration/","text":"Icinga2 PagerDuty Configuration The Icinga Integration Guide is a good basis to start off, with a few gotchas for Icinga2. The format for the notification and command objects needed for Icinga2 The Icinga2 env vars need prefixed by \"ICINGA_\" as the pagerduty_icinga.pl script is looking for them while (( my $k , my $v ) = each %ENV ) { next unless $k = ~ /^ICINGA_ ( .* ) $/ ; $event { $1 } = $v ; } I've submitted/updated the current Icinga2 Puppet Module with the ability to create all the Icinga2 Object needed to configure PagerDuty integration # Configure PagerDuty Alerting Service # # Template Examples: # http://monitoring-portal.org/wbb/index.php?page=Thread postID=204321 # https://lists.icinga.org/pipermail/icinga-users/2014-May/008201.html class icinga2 :: arin :: pagerduty ( $pagerduty_service_apikey = undef , ) { include stdlib # Install Perl dependencies $pagerduty_dependencies_packages = [ perl-libwww-perl , perl-Crypt-SSLeay ] ensure_packages ( $pagerduty_dependencies_packages ) # Install PagerDuty Alerting Script file { pagerduty_icinga.pl : ensure = file , path = /usr/local/bin/pagerduty_icinga.pl , owner = root , group = root , mode = 0755 , content = template ( icinga2/pagerduty_icinga.pl.erb ), } # Create PagerDuty Icinga User icinga2 :: object :: user { pagerduty : display_name = PagerDuty Notification User , pager = $pagerduty_service_apikey , target_dir = /etc/icinga2/objects/users , states = [ OK , Critical ] , } ## Configure Cron for Icinga User cron { icinga pagerduty : ensure = present , command = /usr/local/bin/pagerduty_icinga.pl flush , user = icinga , minute = * , hour = * , monthday = * , month = * , weekday = * , } ## Configure Icinga2 PagerDuty Notification Command for Service icinga2 :: object :: notificationcommand { notify-service-by-pagerduty : command = [ /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=service ] , template_to_import = plugin-notification-command , env = { ICINGA_CONTACTPAGER = $user.pager$ , ICINGA_NOTIFICATIONTYPE = $notification.type$ , ICINGA_SERVICEDESC = $service.name$ , ICINGA_HOSTNAME = $host.name$ , ICINGA_HOSTALIAS = $host.display_name$ , ICINGA_SERVICESTATE = $service.state$ , ICINGA_SERVICEOUTPUT = $service.output$ , }, } ## Configure Icinga2 PagerDuty Notification Command for Hosts icinga2 :: object :: notificationcommand { notify-host-by-pagerduty : command = [ /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=host ] , template_to_import = plugin-notification-command , env = { ICINGA_CONTACTPAGER = $user.pager$ , ICINGA_NOTIFICATIONTYPE = $notification.type$ , ICINGA_HOSTNAME = $host.name$ , ICINGA_HOSTALIAS = $host.display_name$ , ICINGA_HOSTSTATE = $host.state$ , ICINGA_HOSTOUTPUT = $host.output$ , }, } ## Configure Apply Notification to Hosts icinga2 :: object :: apply_notification_to_host { pagerduty-host : assign_where = host.vars.enable_pagerduty == true , command = notify-host-by-pagerduty , users = [ pagerduty ] , states = [ Up , Down ] , types = [ Problem , Acknowledgement , Recovery , Custom ] , period = 24x7 , } ## Configure Apply Notification to Services icinga2 :: object :: apply_notification_to_service { pagerduty-service : assign_where = service.vars.enable_pagerduty == true , command = notify-service-by-pagerduty , users = [ pagerduty ] , states = [ OK , Warning , Critical , Unknown ] , types = [ Problem , Acknowledgement , Recovery , Custom ] , period = 24x7 , } } For those who want to configure this manually, below are examples of the objects needed. Icinga2 Apply Objects apply Notification pagerduty-host to Host { assign where host.vars.enable_pagerduty == true command = notify-host-by-pagerduty users = [ pagerduty ] period = 24x7 types = [ Problem, Acknowledgement, Recovery, Custom ] states = [ Up, Down ] } apply Notification pagerduty-service to Service { assign where service.vars.enable_pagerduty == true command = notify-service-by-pagerduty users = [ pagerduty ] period = 24x7 types = [ Problem, Acknowledgement, Recovery, Custom ] states = [ OK, Warning, Critical, Unknown ] } Icinga2 Notification Command Objects object NotificationCommand notify-host-by-pagerduty { import plugin-notification-command command = [ PluginDir + /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=host ] env = { ICINGA_HOSTSTATE = $host.state$ ICINGA_NOTIFICATIONTYPE = $notification.type$ ICINGA_CONTACTPAGER = $user.pager$ ICINGA_HOSTNAME = $host.name$ ICINGA_HOSTALIAS = $host.display_name$ ICINGA_HOSTOUTPUT = $host.output$ } } object NotificationCommand notify-service-by-pagerduty { import plugin-notification-command command = [ PluginDir + /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=service ] env = { ICINGA_SERVICESTATE = $service.state$ ICINGA_NOTIFICATIONTYPE = $notification.type$ ICINGA_CONTACTPAGER = $user.pager$ ICINGA_SERVICEDESC = $service.name$ ICINGA_HOSTNAME = $host.name$ ICINGA_HOSTALIAS = $host.display_name$ ICINGA_SERVICEOUTPUT = $service.output$ } } Icinga2 User Object object User pagerduty { display_name = PagerDuty Notification User pager = YOURAPIKEYGOESHERE states = [ OK , Critical , ] }","title":"Icinga2 PagerDuty Configuration"},{"location":"linux/monitoring/icinga2_pagerduty_configuration/#icinga2-pagerduty-configuration","text":"The Icinga Integration Guide is a good basis to start off, with a few gotchas for Icinga2. The format for the notification and command objects needed for Icinga2 The Icinga2 env vars need prefixed by \"ICINGA_\" as the pagerduty_icinga.pl script is looking for them while (( my $k , my $v ) = each %ENV ) { next unless $k = ~ /^ICINGA_ ( .* ) $/ ; $event { $1 } = $v ; } I've submitted/updated the current Icinga2 Puppet Module with the ability to create all the Icinga2 Object needed to configure PagerDuty integration # Configure PagerDuty Alerting Service # # Template Examples: # http://monitoring-portal.org/wbb/index.php?page=Thread postID=204321 # https://lists.icinga.org/pipermail/icinga-users/2014-May/008201.html class icinga2 :: arin :: pagerduty ( $pagerduty_service_apikey = undef , ) { include stdlib # Install Perl dependencies $pagerduty_dependencies_packages = [ perl-libwww-perl , perl-Crypt-SSLeay ] ensure_packages ( $pagerduty_dependencies_packages ) # Install PagerDuty Alerting Script file { pagerduty_icinga.pl : ensure = file , path = /usr/local/bin/pagerduty_icinga.pl , owner = root , group = root , mode = 0755 , content = template ( icinga2/pagerduty_icinga.pl.erb ), } # Create PagerDuty Icinga User icinga2 :: object :: user { pagerduty : display_name = PagerDuty Notification User , pager = $pagerduty_service_apikey , target_dir = /etc/icinga2/objects/users , states = [ OK , Critical ] , } ## Configure Cron for Icinga User cron { icinga pagerduty : ensure = present , command = /usr/local/bin/pagerduty_icinga.pl flush , user = icinga , minute = * , hour = * , monthday = * , month = * , weekday = * , } ## Configure Icinga2 PagerDuty Notification Command for Service icinga2 :: object :: notificationcommand { notify-service-by-pagerduty : command = [ /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=service ] , template_to_import = plugin-notification-command , env = { ICINGA_CONTACTPAGER = $user.pager$ , ICINGA_NOTIFICATIONTYPE = $notification.type$ , ICINGA_SERVICEDESC = $service.name$ , ICINGA_HOSTNAME = $host.name$ , ICINGA_HOSTALIAS = $host.display_name$ , ICINGA_SERVICESTATE = $service.state$ , ICINGA_SERVICEOUTPUT = $service.output$ , }, } ## Configure Icinga2 PagerDuty Notification Command for Hosts icinga2 :: object :: notificationcommand { notify-host-by-pagerduty : command = [ /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=host ] , template_to_import = plugin-notification-command , env = { ICINGA_CONTACTPAGER = $user.pager$ , ICINGA_NOTIFICATIONTYPE = $notification.type$ , ICINGA_HOSTNAME = $host.name$ , ICINGA_HOSTALIAS = $host.display_name$ , ICINGA_HOSTSTATE = $host.state$ , ICINGA_HOSTOUTPUT = $host.output$ , }, } ## Configure Apply Notification to Hosts icinga2 :: object :: apply_notification_to_host { pagerduty-host : assign_where = host.vars.enable_pagerduty == true , command = notify-host-by-pagerduty , users = [ pagerduty ] , states = [ Up , Down ] , types = [ Problem , Acknowledgement , Recovery , Custom ] , period = 24x7 , } ## Configure Apply Notification to Services icinga2 :: object :: apply_notification_to_service { pagerduty-service : assign_where = service.vars.enable_pagerduty == true , command = notify-service-by-pagerduty , users = [ pagerduty ] , states = [ OK , Warning , Critical , Unknown ] , types = [ Problem , Acknowledgement , Recovery , Custom ] , period = 24x7 , } } For those who want to configure this manually, below are examples of the objects needed. Icinga2 Apply Objects apply Notification pagerduty-host to Host { assign where host.vars.enable_pagerduty == true command = notify-host-by-pagerduty users = [ pagerduty ] period = 24x7 types = [ Problem, Acknowledgement, Recovery, Custom ] states = [ Up, Down ] } apply Notification pagerduty-service to Service { assign where service.vars.enable_pagerduty == true command = notify-service-by-pagerduty users = [ pagerduty ] period = 24x7 types = [ Problem, Acknowledgement, Recovery, Custom ] states = [ OK, Warning, Critical, Unknown ] } Icinga2 Notification Command Objects object NotificationCommand notify-host-by-pagerduty { import plugin-notification-command command = [ PluginDir + /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=host ] env = { ICINGA_HOSTSTATE = $host.state$ ICINGA_NOTIFICATIONTYPE = $notification.type$ ICINGA_CONTACTPAGER = $user.pager$ ICINGA_HOSTNAME = $host.name$ ICINGA_HOSTALIAS = $host.display_name$ ICINGA_HOSTOUTPUT = $host.output$ } } object NotificationCommand notify-service-by-pagerduty { import plugin-notification-command command = [ PluginDir + /usr/local/bin/pagerduty_icinga.pl enqueue --verbose -f pd_nagios_object=service ] env = { ICINGA_SERVICESTATE = $service.state$ ICINGA_NOTIFICATIONTYPE = $notification.type$ ICINGA_CONTACTPAGER = $user.pager$ ICINGA_SERVICEDESC = $service.name$ ICINGA_HOSTNAME = $host.name$ ICINGA_HOSTALIAS = $host.display_name$ ICINGA_SERVICEOUTPUT = $service.output$ } } Icinga2 User Object object User pagerduty { display_name = PagerDuty Notification User pager = YOURAPIKEYGOESHERE states = [ OK , Critical , ] }","title":"Icinga2 PagerDuty Configuration"},{"location":"linux/monitoring/nav/","text":"Monitoring Articles Graphite Icinga2 PagerDuty Confuration","title":"Monitoring"},{"location":"linux/monitoring/nav/#monitoring-articles","text":"Graphite Icinga2 PagerDuty Confuration","title":"Monitoring Articles"},{"location":"linux/networking/nav/","text":"Networking Articles Spoofing MAC Address in OS X","title":"Networking Articles"},{"location":"linux/networking/nav/#networking-articles","text":"Spoofing MAC Address in OS X","title":"Networking Articles"},{"location":"linux/networking/spoofing_mac_address_in_os_x/","text":"Spoofing MAC Address in OS X Overview A MAC address is a unique identifier assigned to your network card, and some networks implement MAC address filtering as a method of security. Spoofing a MAC address can be desired for multiple reasons. In the article below is an explanation of how to spoof your Mac\u2019s wireless MAC address. This procedure can be slightly modified to spoof the MAC on other interfaces Retrieving your current MAC address Determining your current MAC address will allow you to set it back without rebooting. Via a terminal execute the following command ifconfig en0 | grep ether Your output should be similar to ether b8:e8:56:37:6d:c0 Write this down somewhere so you don\u2019t forget it. If you do, it\u2019s not the end of the world, you\u2019ll just have to reboot to reset it from a change. Spoofing a MAC address To spoof your MAC address, you simply set that value returned from ifconfig to another hex value in the format of aa cc:dd:ee:ff. You can generate a random one if need be. Via a terminal execute the following command, be sure to replace the MAC address to the one you are trying to spoof The sudo command will require that you enter your root password to make the change. sudo ifconfig en0 ether 5c:f9:38:b9:17:4f Note: Some Macs use en1, so if you run into any issues you can try that. Verifying the Spoofed MAC address was set Issue the same command as you did above to get you current MAC address to determine if the spoof address was set ifconfig en0 | grep ether Your output should be ether 5c:f9:38:b9:17:4f . This means your MAC was correctly set to the spoofed address Restore your original MAC Address In order to restore you MAC address to its original address you will need the address that you obtained from the Retrieving your current MAC address section above. Via a terminal execute the following command, be sure to replace the MAC address to the one you are trying to restore The sudo command will require that you enter your root password to make the change. sudo ifconfig en0 ether b8:e8:56:37:6d:c0 Note: Some Macs use en1, so if you run into any issues you can try that. References http://osxdaily.com/2008/01/17/how-to-spoof-your-mac-address-in-mac-os-x/","title":"Spoofing MAC Address in OS X"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#spoofing-mac-address-in-os-x","text":"","title":"Spoofing MAC Address in OS X"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#overview","text":"A MAC address is a unique identifier assigned to your network card, and some networks implement MAC address filtering as a method of security. Spoofing a MAC address can be desired for multiple reasons. In the article below is an explanation of how to spoof your Mac\u2019s wireless MAC address. This procedure can be slightly modified to spoof the MAC on other interfaces","title":"Overview"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#retrieving-your-current-mac-address","text":"Determining your current MAC address will allow you to set it back without rebooting. Via a terminal execute the following command ifconfig en0 | grep ether Your output should be similar to ether b8:e8:56:37:6d:c0 Write this down somewhere so you don\u2019t forget it. If you do, it\u2019s not the end of the world, you\u2019ll just have to reboot to reset it from a change.","title":"Retrieving your current MAC address"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#spoofing-a-mac-address","text":"To spoof your MAC address, you simply set that value returned from ifconfig to another hex value in the format of aa cc:dd:ee:ff. You can generate a random one if need be. Via a terminal execute the following command, be sure to replace the MAC address to the one you are trying to spoof The sudo command will require that you enter your root password to make the change. sudo ifconfig en0 ether 5c:f9:38:b9:17:4f Note: Some Macs use en1, so if you run into any issues you can try that.","title":"Spoofing a MAC address"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#verifying-the-spoofed-mac-address-was-set","text":"Issue the same command as you did above to get you current MAC address to determine if the spoof address was set ifconfig en0 | grep ether Your output should be ether 5c:f9:38:b9:17:4f . This means your MAC was correctly set to the spoofed address","title":"Verifying the Spoofed MAC address was set"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#restore-your-original-mac-address","text":"In order to restore you MAC address to its original address you will need the address that you obtained from the Retrieving your current MAC address section above. Via a terminal execute the following command, be sure to replace the MAC address to the one you are trying to restore The sudo command will require that you enter your root password to make the change. sudo ifconfig en0 ether b8:e8:56:37:6d:c0 Note: Some Macs use en1, so if you run into any issues you can try that.","title":"Restore your original MAC Address"},{"location":"linux/networking/spoofing_mac_address_in_os_x/#references","text":"http://osxdaily.com/2008/01/17/how-to-spoof-your-mac-address-in-mac-os-x/","title":"References"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/","text":"multi_level_SAN_wildcard_certificates Multi-Level Subdomain (SANs) Wildcard SSL Certificate Management with EasyRSA Overview In some circumstacnes it's desirable to match multiple levels of wildcards in an SSL certificate. One example is limiting the amount of 'internal' certificates to manage for multiple evnironment tiers such as dev,qa,stage,test,etc. Whose domain names are in the format n.foo.com, where n is a numeric identifier of the tier), where having a certificate which matches something like web1.dev.foo.com and also web1.qa.foo.com would be needed. The method to do this is with the subjectAltName extension (which is an alias within the certificate for the subject of the certificate, abbreviated SAN) for each subdomain which we want to wildcard. One caveat is that if SANs are in use they must also contain the commonName (CN) as an alternate name, since the browser will ignore the CN in that case (in this example, a SAN for *.foo.com and foo.com would be added). Create/Update OpenSSL Configuration File SANs are added to the certificate when the certificate signing request is created. The safest way to do this is to create a copy of EasyRSAs openssl.cnf file. This allows you to build certificates that do NOT contain SANs by default. EasyRSA uses the same OpenSSL configuration file (cnf) for both request generation and signing In order to determine the current openssl configuration file used by EasyRSA switch to the user that manages your certficiates and view the $KEY_CONFIG variable echo $KEY_CONFIG /home/ca/openssl-1.0.0.cnf Now create a copy of this openssl configuration file. I suggest giving it a name that describes the top level domain you'll be creating a wildcard for. For example openssl-*.foo.com.cnf cp /home/ca/openssl-1.0.0.cnf openssl-*.foo.com.cnf Update OpenSSL Settings for Certificate Request Generation (CSR) Uncomment or Add the line containing req_extensions = v3_req to allow the extentions to be added to the certificate request CSR. req_extensions = v3_req # The extensions to add to a certificate request Next in the [v3_req] section, add the SANs sectional reference ( subjectAltName = @alt_names ). You can also add them inline if desired [ v3_req ] # Extensions to add to a certificate request basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names Then add the setion [alt_names] in the file Note that in the DNS.n lines, the n enumerates the entry number starting at 1. [ v3_req ] # Extensions to add to a certificate request basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [ alt_names ] DNS.1 = foo.com DNS.2 = *.foo.com DNS.3 = *.dev.foo.com DNS.4 = *.qa.foo.com Update OpenSSL Settings for Certificate Creation/Signing Uncomment or Add the line containing copy_extensions = copy . This line ensures that the v3 extension sections for subjectAltNames are copied from the CSR into the newly minted certificate. The reason that this is commented out by default is that it introduces a security risk. For example if a certificate request contains a basicConstraints extension with CA:TRUE and the copy_extensions value is set to copyall and the user does not spot this when the certificate is displayed then this will hand the requester a valid CA certificate. [ CA_default ] dir = $ENV::KEY_DIR # Where ... ... ... RANDFILE = $dir/.rand # private random number file x509_extensions = usr_cert # The extentions to add to the cert copy_extensions = copy # https://www.openssl.org/docs/apps/ca.html#WARNINGS Build and Sign Wildcard Certificate/Key Before building and/or signing a certificate request we need to point EasyRSA to use the custom OpenSSL configuration file we created. The eaiest way to do this is to export a new KEY_CONFIG parameter set by the vars file export KEY_CONFIG=/home/ca/openssl-*.foo.com.cnf Verify the parameter is correctly set echo $KEY_CONFIG /home/ca/openssl-1.0.0.cnf When creating a wildcard certificate for the first time you can use the build-key-server script bundled with EasyRSA $ ./build-key-server *.foo.com-server Generating a 2048 bit RSA private key ...................+++ .........................................................................+++ writing new private key to *.foo.com-server.key ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter . , the field will be left blank. ----- Country Name (2 letter code) [US]: State or Province Name (full name) [NY]: Locality Name (eg, city) [Metropolis]: Organization Name (eg, company) [LexCorp]: Organizational Unit Name (eg, section) [Weapons Manufacturing]: Common Name (eg, your name or your server s hostname) [*.foo.com-server]:*.foo.com Name []:*.foo.com-server Email Address [root@ca.example.dev]: Please enter the following extra attributes to be sent with your certificate request A challenge password []: An optional company name []: Using configuration from /home/ca/openssl-*.foo.com.cnf Check that the request matches the signature Signature ok The Subject s Distinguished Name is as follows countryName :PRINTABLE: US stateOrProvinceName :PRINTABLE: NY localityName :PRINTABLE: Metropolis organizationName :PRINTABLE: LexCorp organizationalUnitName:PRINTABLE: Weapons Manufacturing commonName :T61STRING: *.foo.com name :T61STRING: *.foo.com-server emailAddress :IA5STRING: root@ca.example.dev Certificate is to be certified until Jun 21 13:15:42 2025 GMT (3650 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated \"Update\"/Build New Wildcard Certificate If you need to add or remove SANs from the wildcard certificate a new certificate Signing Request and Certifcate must be built. This new Singing Request and Certificate will still use the original key. Update the OpenSSL configuration file (cnf) to add or remove the needed SANs Note that in the DNS.n lines, the n enumerates the entry number starting at 1. vim openssl-\\*.foo.com.cnf Before a new Singing Request and Certificate can be built/generated we must revoke the old certificate. This can be accomplished using the revoke-full script bundled with EasyRSA ./revoke-full \\*.foo.com-server Using configuration from /home/ca/openssl-*.foo.com.cnf Revoking Certificate 14. Data Base Updated Using configuration from /home/ca/openssl-*.foo.com.cnf *.foo.com-server.crt: C = US, ST = NY, L = Metropolis, O = LexCorp, OU = Weapons Manufacturing, CN = *.foo.com, name = *.foo.com-server, emailAddress = root@ca.example.dev error 23 at 0 depth lookup:certificate revoked After the old certificate is revoke a new Signing Request must be generated using the current Key. This can be accomplished by calling the openssl command directly to build a new signing request openssl req -new -key keys/\\*.foo.com-server.key -out keys/\\*.foo.com-server.csr -config openssl-\\*.foo.com.cnf You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter . , the field will be left blank. ----- Country Name (2 letter code) [US]: State or Province Name (full name) [NY]: Locality Name (eg, city) [Metropolis]: Organization Name (eg, company) [LexCorp]: Organizational Unit Name (eg, section) [Weapons Manufacturing]: Common Name (eg, your name or your server s hostname) [CHANGEME]:*.foo.com Name []:*.foo.com-server Email Address [root@ca.example.dev]: Please enter the following extra attributes to be sent with your certificate request A challenge password []: An optional company name []: Next build/generate and sign the certificate from the signing request. This can be accomplished using the sign-req script bundled with EasyRSA ./sign-req *.foo.com-server Using configuration from /home/ca/openssl-*.foo.com.cnf Check that the request matches the signature Signature ok The Subject s Distinguished Name is as follows countryName :PRINTABLE: US stateOrProvinceName :PRINTABLE: NY localityName :PRINTABLE: Metropolis organizationName :PRINTABLE: LexCorp organizationalUnitName:PRINTABLE: Weapons Manufacturing commonName :T61STRING: *.foo.com name :T61STRING: *.foo.com-server emailAddress :IA5STRING: root@ca.example.dev Certificate is to be certified until Jun 21 13:31:50 2025 GMT (3650 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated","title":"multi_level_SAN_wildcard_certificates"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#multi_level_san_wildcard_certificates","text":"","title":"multi_level_SAN_wildcard_certificates"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#multi-level-subdomain-sans-wildcard-ssl-certificate-management-with-easyrsa","text":"","title":"Multi-Level Subdomain (SANs) Wildcard SSL Certificate Management with EasyRSA"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#overview","text":"In some circumstacnes it's desirable to match multiple levels of wildcards in an SSL certificate. One example is limiting the amount of 'internal' certificates to manage for multiple evnironment tiers such as dev,qa,stage,test,etc. Whose domain names are in the format n.foo.com, where n is a numeric identifier of the tier), where having a certificate which matches something like web1.dev.foo.com and also web1.qa.foo.com would be needed. The method to do this is with the subjectAltName extension (which is an alias within the certificate for the subject of the certificate, abbreviated SAN) for each subdomain which we want to wildcard. One caveat is that if SANs are in use they must also contain the commonName (CN) as an alternate name, since the browser will ignore the CN in that case (in this example, a SAN for *.foo.com and foo.com would be added).","title":"Overview"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#createupdate-openssl-configuration-file","text":"SANs are added to the certificate when the certificate signing request is created. The safest way to do this is to create a copy of EasyRSAs openssl.cnf file. This allows you to build certificates that do NOT contain SANs by default. EasyRSA uses the same OpenSSL configuration file (cnf) for both request generation and signing In order to determine the current openssl configuration file used by EasyRSA switch to the user that manages your certficiates and view the $KEY_CONFIG variable echo $KEY_CONFIG /home/ca/openssl-1.0.0.cnf Now create a copy of this openssl configuration file. I suggest giving it a name that describes the top level domain you'll be creating a wildcard for. For example openssl-*.foo.com.cnf cp /home/ca/openssl-1.0.0.cnf openssl-*.foo.com.cnf","title":"Create/Update OpenSSL Configuration File"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#update-openssl-settings-for-certificate-request-generation-csr","text":"Uncomment or Add the line containing req_extensions = v3_req to allow the extentions to be added to the certificate request CSR. req_extensions = v3_req # The extensions to add to a certificate request Next in the [v3_req] section, add the SANs sectional reference ( subjectAltName = @alt_names ). You can also add them inline if desired [ v3_req ] # Extensions to add to a certificate request basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names Then add the setion [alt_names] in the file Note that in the DNS.n lines, the n enumerates the entry number starting at 1. [ v3_req ] # Extensions to add to a certificate request basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [ alt_names ] DNS.1 = foo.com DNS.2 = *.foo.com DNS.3 = *.dev.foo.com DNS.4 = *.qa.foo.com","title":"Update OpenSSL Settings for Certificate Request Generation (CSR)"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#update-openssl-settings-for-certificate-creationsigning","text":"Uncomment or Add the line containing copy_extensions = copy . This line ensures that the v3 extension sections for subjectAltNames are copied from the CSR into the newly minted certificate. The reason that this is commented out by default is that it introduces a security risk. For example if a certificate request contains a basicConstraints extension with CA:TRUE and the copy_extensions value is set to copyall and the user does not spot this when the certificate is displayed then this will hand the requester a valid CA certificate. [ CA_default ] dir = $ENV::KEY_DIR # Where ... ... ... RANDFILE = $dir/.rand # private random number file x509_extensions = usr_cert # The extentions to add to the cert copy_extensions = copy # https://www.openssl.org/docs/apps/ca.html#WARNINGS","title":"Update OpenSSL Settings for Certificate Creation/Signing"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#build-and-sign-wildcard-certificatekey","text":"Before building and/or signing a certificate request we need to point EasyRSA to use the custom OpenSSL configuration file we created. The eaiest way to do this is to export a new KEY_CONFIG parameter set by the vars file export KEY_CONFIG=/home/ca/openssl-*.foo.com.cnf Verify the parameter is correctly set echo $KEY_CONFIG /home/ca/openssl-1.0.0.cnf When creating a wildcard certificate for the first time you can use the build-key-server script bundled with EasyRSA $ ./build-key-server *.foo.com-server Generating a 2048 bit RSA private key ...................+++ .........................................................................+++ writing new private key to *.foo.com-server.key ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter . , the field will be left blank. ----- Country Name (2 letter code) [US]: State or Province Name (full name) [NY]: Locality Name (eg, city) [Metropolis]: Organization Name (eg, company) [LexCorp]: Organizational Unit Name (eg, section) [Weapons Manufacturing]: Common Name (eg, your name or your server s hostname) [*.foo.com-server]:*.foo.com Name []:*.foo.com-server Email Address [root@ca.example.dev]: Please enter the following extra attributes to be sent with your certificate request A challenge password []: An optional company name []: Using configuration from /home/ca/openssl-*.foo.com.cnf Check that the request matches the signature Signature ok The Subject s Distinguished Name is as follows countryName :PRINTABLE: US stateOrProvinceName :PRINTABLE: NY localityName :PRINTABLE: Metropolis organizationName :PRINTABLE: LexCorp organizationalUnitName:PRINTABLE: Weapons Manufacturing commonName :T61STRING: *.foo.com name :T61STRING: *.foo.com-server emailAddress :IA5STRING: root@ca.example.dev Certificate is to be certified until Jun 21 13:15:42 2025 GMT (3650 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated","title":"Build and Sign Wildcard Certificate/Key"},{"location":"linux/pki_certificates/multi_level_san_wildcard_certificates/#updatebuild-new-wildcard-certificate","text":"If you need to add or remove SANs from the wildcard certificate a new certificate Signing Request and Certifcate must be built. This new Singing Request and Certificate will still use the original key. Update the OpenSSL configuration file (cnf) to add or remove the needed SANs Note that in the DNS.n lines, the n enumerates the entry number starting at 1. vim openssl-\\*.foo.com.cnf Before a new Singing Request and Certificate can be built/generated we must revoke the old certificate. This can be accomplished using the revoke-full script bundled with EasyRSA ./revoke-full \\*.foo.com-server Using configuration from /home/ca/openssl-*.foo.com.cnf Revoking Certificate 14. Data Base Updated Using configuration from /home/ca/openssl-*.foo.com.cnf *.foo.com-server.crt: C = US, ST = NY, L = Metropolis, O = LexCorp, OU = Weapons Manufacturing, CN = *.foo.com, name = *.foo.com-server, emailAddress = root@ca.example.dev error 23 at 0 depth lookup:certificate revoked After the old certificate is revoke a new Signing Request must be generated using the current Key. This can be accomplished by calling the openssl command directly to build a new signing request openssl req -new -key keys/\\*.foo.com-server.key -out keys/\\*.foo.com-server.csr -config openssl-\\*.foo.com.cnf You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter . , the field will be left blank. ----- Country Name (2 letter code) [US]: State or Province Name (full name) [NY]: Locality Name (eg, city) [Metropolis]: Organization Name (eg, company) [LexCorp]: Organizational Unit Name (eg, section) [Weapons Manufacturing]: Common Name (eg, your name or your server s hostname) [CHANGEME]:*.foo.com Name []:*.foo.com-server Email Address [root@ca.example.dev]: Please enter the following extra attributes to be sent with your certificate request A challenge password []: An optional company name []: Next build/generate and sign the certificate from the signing request. This can be accomplished using the sign-req script bundled with EasyRSA ./sign-req *.foo.com-server Using configuration from /home/ca/openssl-*.foo.com.cnf Check that the request matches the signature Signature ok The Subject s Distinguished Name is as follows countryName :PRINTABLE: US stateOrProvinceName :PRINTABLE: NY localityName :PRINTABLE: Metropolis organizationName :PRINTABLE: LexCorp organizationalUnitName:PRINTABLE: Weapons Manufacturing commonName :T61STRING: *.foo.com name :T61STRING: *.foo.com-server emailAddress :IA5STRING: root@ca.example.dev Certificate is to be certified until Jun 21 13:31:50 2025 GMT (3650 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated","title":"\"Update\"/Build New Wildcard Certificate"},{"location":"linux/pki_certificates/nav/","text":"pki_certificates Articles multi_level_SAN_wildcard_certificates","title":"PKI Certificates"},{"location":"linux/pki_certificates/nav/#pki_certificates-articles","text":"multi_level_SAN_wildcard_certificates","title":"pki_certificates Articles"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/","text":"Overview Building a highly available multi-node PostgreSQL cluster, using freely available software including Pacemaker , Corsync , Cman and PostgresSQL on CentOS Infrastructure Vagrant Setup In order to assist in following along with this tutorial, you can use the following Vagrantfile to spool up a cluster environment using CentOS 6.6 For those unfaimilar with please download and install the latest version from vagrantup . Once installed create a root directory that will house files for this project. mkdir pgdb_cluster Navigate into the newly created root directory cd pgdb_cluster Download the Vagrantfile noted above into your root directory wget http://kb.techtaco.org/linux/postgresql/attachments/Vagrantfile Now to create the three virtual machines needed for this development environment we simply run {varant up}. This will read the Vargrantfile located in the project root directory. This downloads the neede .box (virtual machine) image and creates the needed clones with specified modifications. vagrant up Once the virtual machines are provisioned and started you can access them via the { vagrant ssh } command. Replace pgdb1 with the other machines to access them as well, note you must be in the root directory of the project. vagrant ssh pgdb1 HA Cluster Installation The required are available and included in the base/updates repositories for Centos 6.x. From my readings and research it is also possible to use heartbeat 3.x with Pacemaker to achieve similar results. I've decided to go with Corosync as its backed by Red Hat and Suse and it looks to have more active development. Not to mention that the Pacemaker projects recommends you should use Corosync. Cluster Installation Warning! As of RedHat/CentOS 6.4 crmsh is no longer included in the default repositories. If you want to use CRM vs PCS You can include the OpenSuse repositories HERE . More information on the crmsh can be found HERE In this tutorial we will add the openSUSE repository to our nodes. Though I recommend building or copying these packages into a local repository for more controlled management. Configure the openSUSE repository. This need to be done on ALL nodes in the cluster. sudo wget -4 http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/RedHat_RHEL-6/network:ha-clustering:Stable.repo -O /etc/yum.repos.d/network_ha-clustering_Stable.repo Limit the packages to be installed from the openSUSE repository. We only want the crm shell package and required dependencies. This need to be done on ALL nodes in the cluster. sudo runuser -l root -c echo includepkgs=crmsh pssh python-pssh /etc/yum.repos.d/network_ha-clustering_Stable.repo Now that we have the required repositories configured we need to install the needed packages. This need to be done on ALL nodes in the cluster. You will see multiple dependencies being pulled in sudo yum install pacemaker pcs corosync fence-agents crmsh cman ccs Cluster Configuration The first step is to configure the underlying Cman/Corosync cluster ring communication between the nodes and setup Pacemaker to use Corosync as its communication mechanism. For secure communication Corosync requires an pre-shared authkey. This shared key must be added to all nodes in the cluster. To generate the authkey Corosync has a utility corosync-keygen. Invoke this command as the root users to generate the authkey. The key will be generated at /etc/corosync/authkey. You only need to perform this action on one of the nodes in the cluster as we'll copy it to the other nodes sudo /usr/sbin/corosync-keygen Hint! Grab a cup of coffee this process takes a while to complete as it pulls from the more secure /dev/random. You don\u2019t have to press anything on the keyboard it will still generate the authkey** Once the key has been generated copy it to the other nodes in the cluster sudo scp /etc/corosync/authkey root@pgdb2:/etc/corosync/ sudo scp /etc/corosync/authkey root@pgdb3:/etc/corosync/ In multiple examples and documents on the web they reference using the packmaker corosync plugin by adding a /etc/corosync/service.d/pcmk configure file on each node. This is becoming deprecated and will show in the logs if you enable or use the corosync pacemaker plugin. There is a small but important distinction that I stumbled upon, the pacemaker plugin has never been supported on RHEL systems. The real issue is that at some point it will no longer be supplied with the packages on RHEL systems. Prior to 6.4 ( Though this is looking to change in 6.5 and above ), pacemaker only had a tech preview status for the plugin and using the CMAN plugin instead. Reference this wiki article Disable quorum in order to allow Cman/Corosync to complete startup in a standalone state. This need to be done on ALL nodes in the cluster. sudo sed -i.sed s/.*CMAN_QUORUM_TIMEOUT=.*/CMAN_QUORUM_TIMEOUT=0/g /etc/sysconfig/cman Define the cluster, where pg_cluster is the cluster name. This will generate the cluster.conf configuration file. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --createcluster pg_cluster Create the cluster redundant ring(s). The name used for each node in the cluster should correspond to the nodes network hostname uname -n . This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb3.example.com Configure the fence_pcmk agent (supplied with Pacemaker) to redirect any fencing requests from CMAN components (such as dlm_controld) to Pacemaker. Pacemaker\u2019s fencing subsystem lets other parts of the stack know that a node has been successfully fenced, thus avoiding the need for it to be fenced again when other subsystems notice the node is gone. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb3.example.com sudo ccs -f /etc/cluster/cluster.conf --addfencedev pcmk agent=fence_pcmk sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb1.example.com pcmk-redirect port=pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb2.example.com pcmk-redirect port=pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb3.example.com pcmk-redirect port=pgdb3.example.com Enable secure communciation between nodes in the Corosync cluster using the corosync authkey generated above. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --setcman keyfile= /etc/corosync/authkey transport= udpu Before we start the cman service and copy the configuration to the other nodes in the cluster lets verify that the generated configuration values are valid. This should be run on the same node as the pervious commands. For simplicity we will run this on pgdb1 in the cluster sudo ccs_config_validate -f /etc/cluster/cluster.conf Start the Cman/Corosync cluster services This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo /etc/init.d/cman start Starting cluster: Checking if cluster has been disabled at boot... [ OK ] Checking Network Manager... [ OK ] Global setup... [ OK ] Loading kernel modules... [ OK ] Mounting configfs... [ OK ] Starting cman... [ OK ] Waiting for quorum... [ OK ] Starting fenced... [ OK ] Starting dlm_controld... [ OK ] Tuning DLM kernel config... [ OK ] Starting gfs_controld... [ OK ] Unfencing self... [ OK ] Joining fence domain... [ OK ] Note that starting Cman also starts the Corosync service. This can be easily verified via the Corosync init script sudo /etc/init.d/corosync status corosync (pid 18376) is running... Start the Pacemaker cluster service This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo /etc/init.d/pacemaker start Starting Pacemaker Cluster Manager [ OK ] Before continuing verify that all services have correctly started and are running. sudo /etc/init.d/cman status cluster is running. sudo /etc/init.d/corosync status corosync (pid 615) is running... sudo /etc/init.d/pacemaker status pacemakerd (pid 868) is running... After the initial node has been successfully configured and services have started copy the cluster.conf to the other nodes in the cluster sudo scp /etc/cluster/cluster.conf pgdb2.example.com:/etc/cluster/cluster.conf sudo scp /etc/cluster/cluster.conf pgdb3.example.com:/etc/cluster/cluster.conf Start the Cman/Corosync services on additional nodes in the cluster. sudo /etc/init.d/cman start Starting cluster: Checking if cluster has been disabled at boot... [ OK ] Checking Network Manager... [ OK ] Global setup... [ OK ] Loading kernel modules... [ OK ] Mounting configfs... [ OK ] Starting cman... [ OK ] Waiting for quorum... [ OK ] Starting fenced... [ OK ] Starting dlm_controld... [ OK ] Tuning DLM kernel config... [ OK ] Starting gfs_controld... [ OK ] Unfencing self... [ OK ] Joining fence domain... [ OK ] Before continuing and starting Pacemaker on additional nodes in the cluster verify that ALL nodes in the cluster are communicating via the Cman/Corosync cluster ring. View cluster ring Cman/Corosync status, this should be run on ALL nodes to verify that are correctly communicating sudo cman_tool nodes -a Node Sts Inc Joined Name 1 M 4 2014-04-09 08:30:22 pgdb1.example.com Addresses: 10.4.10.60 2 M 8 2014-04-09 08:44:01 pgdb2.example.com Addresses: 10.4.10.61 3 M 12 2014-04-09 08:44:08 pgdb3.example.com Addresses: 10.4.10.62 Start the Pacemaker service on additional nodes in the cluster sudo /etc/init.d/pacemaker start Starting Pacemaker Cluster Manager [ OK ] Before continuing verify that all services have correctly started and are running on the additional nodes in the cluster. sudo /etc/init.d/cman status cluster is running. sudo /etc/init.d/corosync status corosync (pid 615) is running... sudo /etc/init.d/pacemaker status pacemakerd (pid 868) is running... Verify that ALL nodes have joined the Pacemaker cluster. View Pacemaker HA cluster status sudo pcs status Cluster name: pg_cluster Last updated: Thu Apr 10 07:39:08 2014 Last change: Thu Apr 10 06:49:19 2014 via cibadmin on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.10-14.el6_5.2-368c726 3 Nodes configured 0 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: Pacemaker Cluster Configuration At this point we have configured the basic cluster communication ring. All nodes are now communicating and reporting their heartbeat status to each of the nodes via Corosync. Verify the Pacemaker cluster configuration. Here you'll notice the cluster is complaining that STONITH (Shoot The Other Node In The Head) is not configured. sudo pcs cluster verify -V error: unpack_resources: Resource start-up disabled since no STONITH resources have been defined error: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option error: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity Errors found during check: config not valid For now we are going to disable this and come back to it later in the tutorial. This only needs to be run on one node of the cluster as they are syncing configurations between the nodes. sudo pcs property set stonith-enabled = false Verify the Pacemaker stonith property was correctly configured. sudo pcs config Cluster Name: pg_cluster Corosync Nodes: Pacemaker Nodes: pgdb1.example.com pgdb2.example.com pgdb3.example.com Resources: Stonith Devices: Fencing Levels: Location Constraints: Ordering Constraints: Colocation Constraints: Cluster Properties: cluster-infrastructure: cman dc-version: 1 .1.10-14.el6_5.2-368c726 stonith-enabled: false ```` Now verifying the Pacemaker cluster configuration again returns no errors. ```` sudo pcs cluster verify -V ```` Pacemaker IP Resources ---------------------- With a basic cluster configuration setup resources can be created for the cluster to manage. The first resource to add is a cluster IP or VIP so that applications will be able to continuously communicate with the cluster regardless of where the cluster services are running. Resources only need to be created on **one** node in the cluster Pacemaker/Corosync will replicate the cluster information base ( CIB ) to all nodes in the cluster Create a IP resources VIPs using the **ocf:heartbeat:IPaddr2** resource agent script . Create Replication VIP : This will be used for additional PostgreSQL replicas to recieve updates from the Master ```` bash sudo pcs resource create pgdbrepvip ocf:heartbeat:IPaddr2 ip = 10 .10.10.104 cidr_netmask = 24 iflabel = pgdbrepvip op monitor interval = 1s meta target-role = Started ```` Create Client Access VIP : This will be used for client applications to connect to the acitve Master database ```` bash sudo pcs resource create pgdbclivip ocf:heartbeat:IPaddr2 ip = 10 .10.10.105 cidr_netmask = 24 iflabel = pgdbclivip op monitor interval = 1s meta target-role = Started ```` Verify the Pacemaker cluster resource has been correctly added to the cluster information base ( CIB ) . ```` bash sudo pcs config ```` ```` Cluster Name: pg_cluster Corosync Nodes: Pacemaker Nodes: pgdb1.example.com pgdb2.example.com pgdb3.example.com Resources: Resource: pgdbrepvip ( class = ocf provider = heartbeat type = IPaddr2 ) Attributes: ip = 10 .10.10.104 cidr_netmask = 24 iflabel = pgdbrepvip Meta Attrs: target-role = Started Operations: monitor interval = 1s ( pgdbrepvip-monitor-interval-1s ) Resource: pgdbclivip ( class = ocf provider = heartbeat type = IPaddr2 ) Attributes: ip = 10 .10.10.105 cidr_netmask = 24 iflabel = pgdbclivip Meta Attrs: target-role = Started Operations: monitor interval = 1s ( pgdbclivip-monitor-interval-1s ) Stonith Devices: Fencing Levels: Location Constraints: Ordering Constraints: Colocation Constraints: Cluster Properties: cluster-infrastructure: cman dc-version: 1 .1.10-14.el6_5.2-368c726 stonith-enabled: false ```` View the running status of the cluster. Here we can see that both the IP resources VIPs are running. ```` sudo pcs status ```` ```` Cluster name: pg_cluster Last updated: Thu Apr 10 08 :04:14 2014 Last change: Thu Apr 10 07 :53:03 2014 via cibadmin on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.10-14.el6_5.2-368c726 3 Nodes configured 2 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb2.example.com ```` The pgdbclivip IP resource VIP was started on pgdb2, for simplicity we will move it to pgdb1. ```` sudo pcs resource move pgdbclivip pgdb1.example.com ```` Viewing the running status of the cluster again we can see both resources are now running on pgdb1 ```` sudo pcs status ```` ```` Cluster name: pg_cluster Last updated: Thu Apr 10 08 :11:48 2014 Last change: Thu Apr 10 08 :06:56 2014 via crm_resource on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.10-14.el6_5.2-368c726 3 Nodes configured 2 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com ```` PostgreSQL Database Configuration --------------------------------- Before adding a Pacemaker pgsql resource to manage the PostgreSQL services, its recommended to setup the PostgreSQL cluster ( The PostgreSQL internal cluster ) with some basic streaming replication. The version of PostgreSQL that is in the provided repositories on CentOS 6 .5 is 8 .4.20 which does not provide the needed streaming replication. To work around this we will add PGDG ( PostgreSQL Global Development Group ) repository. As of this writing we are using PostgreSQL version 9 .3.5 Configure the needed repository. This need to be done on **ALL** nodes in the cluster. ``` bash sudo wget http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm -O /tmp/pgdg-centos93-9.3-1.noarch.rpm sudo rpm -Uvh /tmp/pgdg-centos93-9.3-1.noarch.rpm With the correct repository configured install the recommended packages. sudo yum install postgresql93-server postgresql93-contrib postgresql93-devel Initialize the PostgreSQL database via initdb. We only need to perform this on the Master node as we'll be transferring the database to the remaining nodes. I'll be referring to these nodes as PostgreSQL replicas (pgdb2, pgdb3). We will use pgdb1 as the Master from here on out. sudo /etc/init.d/postgresql-9.3 initdb Once the initialization is successful you'll see the PostgreSQL data directory populated. On CentOS this is located in /var/lib/pgsql/{version (9.3)}/data sudo ls /var/lib/pgsql/9.3/data/ When the database was initialized via initdb it configured permissions in the pg_hba.conf. This uses the ident scheme to determine if a user is allowed to connect to the database. ident : An authentication schema that relies on the currently logged in user. If you\u2019ve su -s to postgres and then try to login as another user, ident will fail (as it\u2019s not the currently logged in user). This can be a sore spot if you're not aware how it was configured and will produce an error if trying to create a database with a user that is not currently logged into the system. createdb: could not connect to database postgres: FATAL: Ident authentication failed for user \"myUser\" To avoid this headache modify the pg_hba.conf file to move from the ident scheme to the md5 scheme. This needs to be modified on the Master pgdb1 sudo sed -i s/\\ ident/\\ md5/g /var/lib/pgsql/9.3/data/pg_hba.conf Result: # IPv4 local connections: host all all 127 .0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5 Modify the pg_hba.conf to allow the repclias to connect to the Master . In this tutorial we are adding a basic connection line. It is recommended that you tune this based on your infrastructure for proper security. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/pg_hba.conf # Allowing Replicas to connect in for streaming replication host replication replicator all trust EOF Configure the bind address the PostgreSQL will listen on. This needs to be set to * so the PostgreSQL service will listen on any address. PostgreSQL will scan for new addresses and automatically bind to them as they appear on the node. This is required to allow PostgreSQL to start listening on the VIP address in the event of node failover. Modify the postgresql.conf with your favorite text editor and modify the listen_addresses parameter or add an additional parameter to the end of the configuration file. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c echo \\ listen_addresses = * \\ /var/lib/pgsql/9.3/data/postgresql.conf The PostgreSQL has the concept of archiving for its WAL logs. Its recommended to create a separate archive directory, this will be used to store and recover archived WAL logs. In this tutorial we will create this in the current PostgreSQL {version} directory, you can create this anywhere. This need to be done on ALL nodes in the cluster. sudo runuser -l postgres -c mkdir /var/lib/pgsql/9.3/archive Configure the ability for PostgreSQL to sync archive logs between the nodes for recovery and backup. To configure syncing of the WAL archives between each of the Postgresql nodes, we'll setup a custom script that will be called by the archive_command in the postgresql.conf. This script utilizes rsync to keep the archive directory in sync on each of the nodes with the Postgresql Master. In order to facilitate syncing between the nodes we'll be using ssh keys to allow the nodes to send updates automagically. We will be creating the ssh key with no passphrase named pgarchivesync for the postgresql user. This needs to be run on ALL nodes sudo runuser -l postgres -c ssh-keygen -t rsa -f /var/lib/pgsql/.ssh/pgarchivesync -N In order to simplfy the setup of the authorized_key files for the postgres user we need to set a password. This needs to be done on ALL nodes echo password | sudo passwd postgres --stdin With the keys generated on all of the nodes, the public key for the postgres user on each nodes needs added to the authorized_key file on each of the other nodes that we want to maintain syncing between. me = ` hostname ` cluster_nodes = pgdb1 pgdb2 pgdb3 nodes =( ${ cluster_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` for node in ${ nodes } ; do sudo ssh-copy-id -i /var/lib/pgsql/.ssh/pgarchivesync.pub postgres@ ${ node } ; done Accept the ssh host keys from each of the other nodes in the cluster. me = ` hostname ` cluster_nodes = pgdb1 pgdb2 pgdb3 nodes =( ${ cluster_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` for node in ${ nodes } ; do sudo runuser -l postgres -c ssh -o StrictHostKeyChecking=no -i /var/lib/pgsql/.ssh/pgarchivesync postgres@ ${ node } exit ; done To add a bit more security, which hasn't really been the practice so far in this tutorial we will lock down that can be run with using the pgarchivesync ssh key. This needs to be run on ALL nodes sudo sed -i s/^/command= \\/usr\\/bin\\/rsync --server -avzp --delete \\/var\\/lib\\/pgsql\\/9.3\\/archive ,no-pty,no-agent-forwarding,no-port-forwarding / /var/lib/pgsql/.ssh/authorized_keys With the script ssh key requirements taken care of, we need to populate the script on ALL of the nodes in the cluster. This will allow the archive syncing to happen from any node that is promoted to Master. sudo runuser -l root -c cat EOF /usr/local/sbin/pgarchivesync.sh #!/bin/bash archivedir= /var/lib/pgsql/9.3/archive synckey= /var/lib/pgsql/.ssh/pgarchivesync # Exit code to Postgres FAILURE=0 # Copy the file locally to the archive directory /bin/gzip \\$1 \\$archivedir/\\$2.gz rc=\\$? if [ \\$rc != 0 ]; then FAILURE=1 exit 1 fi me=\\`hostname\\` cluster_nodes= pgdb1 pgdb2 pgdb3 #cluster_nodes=\\`sudo cman_tool nodes -F name | sed s/.example.com//g \\` nodes=(\\${cluster_nodes[@]//\\${me}}) nodes=\\`echo \\${nodes[@]}\\` verifynodes=\\`echo \\${nodes[@]}\\` # Sync the archive dir with the currently correct replicas for node in \\${nodes}; do /usr/bin/nc -z -w2 \\${node} 22 /dev/null 2 1 rc=\\$? if [ \\$rc != 0 ]; then /usr/bin/logger PGSQL Archive Sync Failure: \\${node} is not accessible for archive syncing, skipping this node if [[ \\${verifynodes[*]} =~ \\${node} ]]; then FAILURE=1 fi else /usr/bin/rsync -avzp --delete -e ssh -i \\$synckey \\$archivedir/ postgres@\\$node:\\$archivedir rc=\\$? if [ \\$rc != 0 ]; then /usr/bin/logger PGSQL Archive Sync Failure: \\${node} RSYNC failure if [[ \\${verifynodes[*]} =~ \\${node} ]]; then FAILURE=1 fi fi fi done exit \\$FAILURE EOF Make the command executable so that the postgres user can run the archive sync script. sudo chmod +x /usr/local/sbin/pgarchivesync.sh Configure PostgreSQL steaming replication in the postgresql.conf file. These settings are very basic and will need to be tuned based on your infrastructure. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/postgresql.conf wal_level = hot_standby archive_mode = on archive_command = /usr/local/sbin/pgarchivesync.sh %p %f max_wal_senders = 3 wal_keep_segments = 100 hot_standby = on EOF Start the PostgreSQL service on the Master and check for erros in /var/lib/pgsql/9.3/pg_log/*. sudo /etc/init.d/postgresql-9.3 start Starting postgresql-9.3 service: [ OK ] Once the PostgreSQL service is started, to assist with the replication process and some basic security, a separate replication user account should be created. sudo runuser -l postgres -c psql -c \\ CREATE USER replicator REPLICATION LOGIN ENCRYPTED PASSWORD replaceme ;\\ With a functioning Master database up and running, the replicas (slaves) need to be initialized and configured to synchronize from the Master To clone the PostgreSQL database cluster from the Master node to the replicas (pgdb2, pgdb3). We are using a modern version of PostgreSQL that include pg_basebackup, which makes the process 1000000000 time simpler. You can also use pg_start_backup, rsync and pg_stop_backup to perform a more manaul cloning. On the replica nodes (pgdb2, pgdb3) run the pg_basebackup command sudo runuser -l postgres -c pg_basebackup -D /var/lib/pgsql/9.3/data -l `date + %m-%d-%y `_initial_cloning -P -h pgdb1.example.com -p 5432 -U replicator -W -X stream To avoid any confusion with troubleshooting remove the log files that were transferred during the pg_basebackup process. This needs to be done on both replicas sudo runuser -l root -c rm /var/lib/pgsql/9.3/data/pg_log/* In order for the replicas to connect to the Master for streaming replication a recovery.conf file must exist in the PostgreSQL data directory. Create a recovery.conf file on both replicas (pgdb2, pgdb3) sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/recovery.conf standby_mode = on primary_conninfo = host=10.10.10.104 port=5432 user=replicator application_name=`hostname` restore_command = gunzip /var/lib/pgsql/9.2/archive/%f.gz \\ %p\\ EOF Start the PostgreSQL service on both of the replicas (pgdb2, pgdb3) sudo /etc/init.d/postgresql-9.3 start On the Master verify and view the active replica connections and their status. You'll notice the sync_state is async for both nodes, this is because we have not set the standby_node_names parameter on the master to let it know what nodes it should attempt to perform synchronous replication with. You'll also notice the state is streaming, this is continually sending changes to the replicas/slaves without waiting for WAL segments to fill and then be shipped. sudo runuser -l postgres -c psql -c \\ SELECT application_name, client_addr, client_hostname, sync_state, state, sync_priority, replay_location FROM pg_stat_replication;\\ application_name | client_addr | client_hostname | sync_state | state | sync_priority | replay_location ------------------------+-------------+-----------------+------------+-----------+---------------+----------------- pgdb2.example.com | 10.4.10.61 | | async | streaming | 0 | 0/40000C8 pgdb3.example.com | 10.4.10.62 | | async | streaming | 0 | 0/40000C8 Pacemaker PostgreSQL Resource This will be a master/slave resource as opposed to the primitive resource that we created above for the IP resources (VIPs). The resource-agents package that was installed above includes a pgsql resource agent that was designed to work with Postgresql 9.1+ and streaming replication Before we create the master/slave resource we need to stop the PostgreSQL service on ALL the nodes (pgdb1 pgdb2 pgdb3). This is because the Pacemaker PostgreSQL resource will be controlling the state of the PostgreSQL service. sudo /etc/init.d/postgresql-9.3 stop Additionally the run level for the PostgreSQL service needs set so that it does NOT start on boot. This is to insure that the Pacemaker PostgreSQL resource has rull control of the PostgreSQL service. sudo /sbin/chkconfig postgresql-9.3 off Lastly to avoid issues with non master nodes (pgdb2, pgdb3) becoming the master as dictated by the Pacemaker cluster, we will place the additonal nodes into standby mode. This also helps prevent nodes slipping to a different timeline. sudo pcs cluster standby pgdb2.example.com; sudo pcs cluster standby pgdb3.example.com Verify that pgdb2 and pgdb3 have been placed into standby mode sudo pcs status Cluster name: pg_clu Last updated: Tue Nov 11 11:31:33 2014 Last change: Tue Nov 11 11:31:32 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 2 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com To create a stateful/multi-state resource, a primitive resource must first be created and the from that primitive resource you can create a master/slave resource. This resource only needs to be created on one node in the cluster Pacemaker/Corosync will replicate the cluster information base (CIB) to all nodes in the cluster. For simplicity create this resource on the Master pgdb1. The parameters are all setting located in the pgsql resource script, provided via the resource-agents package. In preparation to move this to a multi-state resource multiple monitoring operations are included for each Pacemaker cluster state. sudo /usr/sbin/pcs resource create postgresql ocf:heartbeat:pgsql \\ pgctl= /usr/pgsql-9.3/bin/pg_ctl \\ pgdata= /var/lib/pgsql/9.3/data \\ psql= /usr/pgsql-9.3/bin/psql \\ config= /var/lib/pgsql/9.3/data/postgresql.conf \\ stop_escalate= 5 \\ rep_mode= sync \\ node_list= pgdb1.example.com pgdb2.example.com pgdb3.example.com \\ restore_command= gunzip /var/lib/pgsql/9.3/archive/%f.gz \\ %p\\ \\ master_ip= 10.10.10.104 \\ repuser= replicator \\ restart_on_promote= true \\ tmpdir= /var/lib/pgsql/9.3/tmpdir \\ xlog_check_count= 3 \\ crm_attr_timeout= 5 \\ check_wal_receiver= true \\ op start timeout= 60s interval= 0s on-fail= restart \\ op monitor timeout= 30 interval= 2s \\ op monitor timeout= 30 interval= 1s role= Master \\ op promote timeout= 60s interval= 0s on-fail= restart \\ op demote timeout= 60s interval= 0s on-fail= stop \\ op stop timeout= 60s interval= 0s on-fail= block \\ op notify timeout= 60s interval= 0s Create the multi-state resource from the primitive resource created above sudo /usr/sbin/pcs resource master mspostgresql postgresql \\ notify= true \\ target-role= Started Verify that creation and status of the PostgreSQL cluster resources Note! You will notice the resource fail after adding it from above, this is because the primitive resource for PostgreSQL was created with an active node and rep_mode parameter requires a Master/Slave resource. pgsql(postgresql)[28784]: ERROR: Replication(rep_mode=async or sync) requires Master/Slave configuration. sudo pcs status To clear the failures and allow the PostgreSQL service to start controlled via Pacemaker the fail count must be cleared and service re-probed The command below sets the fail counts to 0 for the resource postgresql on the node pgdb1.example.com sudo crm resource failcount postgresql set pgdb1.example.com 0; sudo crm_resource -P Checking the cluster status you can see that pgdb1 has now started the postgresql resource as a Slave. If you tail /var/log/messages you'll notice that the cluster is processing the pdgdb1 node and in the process of promoting it to Master. sudo pcs status Cluster name: pg_cluster Last updated: Fri Nov 14 07:38:56 2014 Last change: Fri Nov 14 07:38:55 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Slaves: [ pgdb1.example.com ] Stopped: [ pgdb2.example.com pgdb3.example.com ] After a brief period of time you should now see that pgdb1 has started as a Master. This resource now starts and manages the PostgreSQL service sudo pcs status Cluster name: pg_cluster Last updated: Tue Nov 11 13:23:16 2014 Last change: Tue Nov 11 13:21:05 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Stopped: [ pgdb2.example.com pgdb3.example.com ] The additional nodes now need to placed into online mode within the Pacemaker cluster. This will start the postgresql resource and in turn start the postgresql service on each of the replica nodes. Place pgdb2 into online mode within the cluster. Execute the following command on any node in the cluster. sudo pcs cluster unstandby pgdb2.example.com Place pgdb3 into online mode within the cluster. Execute the following command on any node in cluster. sudo pcs cluster unstandby pgdb3.example.com Checking the cluster status you now will see that both pgdb2 and pgdb3 have started the postgresql resource as Slaves. sudo pcs status Cluster name: pg_cluster Last updated: Wed Nov 12 06:39:22 2014 Last change: Wed Nov 12 06:30:37 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] With all the nodes started in the Pacemaker cluster and running the needed resources we need to create resource constraints. The first constraint that is needed is for the colocation of the VIPs (pgdbrepvip, pgdbclivip) and the PostgreSQL service, for the node that is running as Master. This is so that the pgdbrepvip and pgdbclivip are always started on the same node that is running the postgresql resource in Master mode. This is so if one or more of the resources fails on the Master node resource are migrated to the new node sudo pcs constraint colocation set pgdbrepvip role=Started set mspostgresql role=Master set pgdbclivip role=Started setoptions score=INFINITY Then we'll set the order that the cluster resources should be started or promoted in. For this we'll be starting the pgdbrepvip resource then the postgresql resource and lastly the pgdbclivip resource. This seems counter intuitive to start the postgresql service before starting the PG_CLI_VIP for postgresql to bind to. This is worked around by the listen_addresses=\"*\" parameter that we set above. This tells postgresql to listen on all interfaces regardless if the are coming up after the postgresql service starts. This gives us the benefit of not allowing application utilizing the VIP to access the database until it is up. Note: The version of pcs doesn't support configuring the order set as needed. This was brought up in IRC #linux-cluster and fiest__ is working on the fix to add this ability For now we will use the crm command to properly configure the ordering constraint, more detail below is given on the crm command sudo crm configure order order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory inf: pgdbrepvip:start mspostgresql:promote pgdbclivip:start The last constraint we'll set is resource stickiness. The concept of resource stickiness which controls how much a cluster resource prefers to stay running where it is. You may like to think of it as the \"cost\" of any downtime. By default, Pacemaker assumes there is zero cost associated with moving resources and will do so to achieve \"optimal\" resource placement. We can specify a different stickiness for every resource, but it is often sufficient to change the default as seen below. After setting the default resources stickiness when setting a node into standby mode and then placing it back into online mode, resources will remain on the node that they migrated to sudo pcs property set default-resource-stickiness=100 Something you might have noticed is that there is a location constraint listed in the configuration that will cause issues with the execution of the other constraints. I'm a little unsure why its created and will update this document when I have a more in-depth answer. View the current constraints sudo pcs constraint list --full Location Constraints: Resource: pgdbclivip Enabled on: pgdb1.example.com (score:INFINITY) (role: Started) (id:cli-prefer-pgdbclivip) Ordering Constraints: Resource Sets: set pgdbrepvip action=start (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-0) set mspostgresql action=promote (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-1) set pgdbclivip action=start (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-2) setoptions score=INFINITY (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory) Colocation Constraints: Resource Sets: set pgdbrepvip role=Started (id:pcs_rsc_set_pgdbrepvip-1) set mspostgresql role=Master (id:pcs_rsc_set_mspostgresql-1) set pgdbclivip role=Started (id:pcs_rsc_set_pgdbclivip-1) setoptions score=INFINITY (id:pcs_rsc_colocation_pgdbrepvip_set_mspostgresql_set_pgdbclivip) Remove the location constraint, take note of the constraint ID from the output above sudo pcs constraint remove cli-prefer-pgdbclivip Fencing and STONITH Note! This section is still a work in progress Additional Tools CRM Packages pulled down from the OpenSUSE repository at the beginning of this tutorial provide the crm command. This is an alternative to the pcs command used to control/configure the Pacemaker cluster. Unlike the pcs command the crm command provides an live interactive shell along with tab completion to assist in cluster control/configuration. sudo crm crm ( live ) # cluster status Services: corosync unknown pacemaker unknown Printing ring status. Local node ID 3 RING ID 0 id = 10 .10.10.103 status = ring 0 active with no faults crm ( live ) # status Last updated: Fri Nov 14 15 :48:39 2014 Last change: Fri Nov 14 13 :07:04 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] CRM_MON So far in this guide the pcs command for cluster configuration and to see a quick overview of the cluster status. There however is tool provided with the pacemaker-cli package that provides near real-time details and 'monitoring' of the cluster. The crm_mon command is very extensive and can even provide output in a nagios/icinga format. The options that I have found move useful are: -A, --show-node-attributes Display node attributes -r, --inactive Display inactive resources -f, --failcounts Display resource fail counts -i, --interval=value Update frequency in seconds This will list sudo crm_mon -Arf -i1 Last updated: Fri Nov 14 15 :09:16 2014 Last change: Fri Nov 14 13 :07:04 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 0000000010000090 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync + postgresql-xlog-loc : 00000000100000F8 * Node pgdb3.example.com: + master-postgresql : -INFINITY + postgresql-data-status : STREAMING | POTENTIAL + postgresql-receiver-status : normal + postgresql-status : HS:potential Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Node Failover Below is an extensive list of possible failover senarios that might be experienced within the cluster. I wont be covering each of these in detail but you should test each of these senarios in your production environment before going live to verify the cluster is correctly configured and promotion is happening as expected. For each of the exercises below open a seperate terminal window running the crm_mon command to view the failover in near real time. sudo crm_mon -Arf -i1 Fail the asynchronous node (pgdb3) Place pgdb3 into standby mode within the cluster. sudo pcs cluster standby pgdb3.example.com You should see pgdb3 placed into standby mode and its postgresql-data-status reported as disconnected Last updated: Mon Nov 17 06:12:16 2014 Last change: Mon Nov 17 06:12:12 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb3.example.com: standby Online: [ pgdb1.example.com pgdb2.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com ] Stopped: [ pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING|SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync * Node pgdb3.example.com: + master-postgresql : -INFINITY + postgresql-data-status : DISCONNECT + postgresql-status : STOP Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Now 'reset the cluster by placing pgdb3 back into online mode' sudo pcs cluster unstandby pgdb3.example.com Fail the synchronous node (pgdb2) Place pgdb2 into standby mode within the cluster sudo pcs cluster standby pgdb2.example.com You chould see pgdb2 placed into standby mode and its postgresql-data-status reported as disconnected. Notice that pgdb3 is now the synchronous node with its postgresql-data-status set to *|SYNC Last updated: Mon Nov 17 06 :21:23 2014 Last change: Mon Nov 17 06 :21:20 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Online: [ pgdb1.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb3.example.com ] Stopped: [ pgdb2.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : -INFINITY + postgresql-data-status : DISCONNECT + postgresql-status : STOP * Node pgdb3.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Now 'reset the cluster by placing pgdb3 back into online mode' sudo pcs cluster unstandby pgdb2.example.com Notice that pgdb2 is now the asynchronous node with its postgresql-data-status set to *|POTENTIAL Last updated: Mon Nov 17 06 :25:23 2014 Last change: Mon Nov 17 06 :25:22 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : -INFINITY + postgresql-data-status : STREAMING | POTENTIAL + postgresql-receiver-status : normal + postgresql-status : HS:potential * Node pgdb3.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Fail the 'Master' node (pgdb1) Place pgdb2 into standby mode within the cluster sudo pcs cluster standby pgdb1.example.com Potential Failover Scenarios Master node loses network connectivity Master nodes' postgresql service fails Master node loses system power Master nodes' pgdbrepvip pacemaker resource fails Master nodes' pgdbclivip pacemaker resource fails Sync Replica node loses network connectivity Sync Replica nodes' postgresql service fails Sync Replica node loses system power Async Replica node loses network connectivity Async Replica nodes' postgresql service fails Async Replica node loses system power Sync and Async Replica nodes lose network connectivity Sync and Async Replica nodes lose system power Sync and Async Replica nodes postgresql service fails Master and Sync Replica nodes lose network connectivity Master and Sync Replica nodes lose system power Master and Sync Replica nodes postgresql service fails Master and Async Replica nodes lose network connectivity Master and Async Replica nodes lose system power Master and Async Replica nodes postgresql service fails Master,Sync Replica and Async Replica nodes lose network connectivity Master,Sync Replica and Async Replica nodes lose system power Master,Sync Replica and Async Replica nodes postgresql service fails Production Considerations This tutorial details only a basic cluster setup and is not intended to be used in a production environment. Below are some points that you should consider within a production environment. System Configuration Make sure you have a developed and well tested procedure for testing updates for then cluster components ( pacemaker pcs corosync fence-agents crmsh cman ccs ) Verify that DNS is 100% complete and accurate for all cluster nodes and pacemaker IP resources Packages downloaded from the OpenSUSE repositories should be hosted and maintained in a local repository. The OpenSUSE repository doesn't seem to support rsync or reposync so I've created a script to sync down the needed packages locally. #!/bin/bash DATE = ` /bin/date +%Y-%m-%d ` OUTDIR = /path/to/logs/dir/ OUTFILE = $OUTDIR /ha-cluster-mirror- $DATE .txt [ -d $OUTDIR ] || mkdir -p $OUTDIR URL = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/x86_64/ REPOKEY = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/repodata/repomd.xml.key DESDIR = /repo/dir/path/ha-cluster KEYDIR = /repo/dir/path/ha-cluster/repodata CRMRPMS = ` wget -4qO - $URL | grep -oe crm.*.rpm | cut -d -f1 ` for i in $CRMRPMS ; do wget -4 -N -P $DESDIR $URL$i $OUTFILE 2 1 done PSRPMS = ` wget -4qO - $URL | grep -oe pssh.*.rpm | cut -d -f1 ` for i in $PSRPMS ; do wget -4 -N -P $DESDIR $URL$i $OUTFILE 2 1 done wget -4 -N -P $KEYDIR $REPOKEY $OUTFILE 2 1 /usr/bin/createrepo /repo/dir/path/ha-cluster $OUTFILE 2 1 Cluster Configuration Configuring network based fencing, as it provides the flexibility of being able to access the machine via an alternative connection ( LOM, iDrac, Console ) and view the state of a fenced machine. Our current solution is to use the fence_ifmib script located in /usr/sbin that is provided with the fence-agents package to administratively disable ports that fenced nodes are connected to. PostgreSQL Best Practices A unique and strong password should be set for the replicator PostgreSQL account A unique and strong password should be set for the postgres NIX account The pg_hba.conf file should limit the nodes allowed to replicate from the master with the method trust via specific IPs (/32) # pgdb2.example.com host replication replicator 10 .10.10.102/32 trust A more robust archive_command command should be used. Its recommended to use a script that does error checking. Below is a example script that moves the xlogs to an #!/bin/bash archivedir = /var/lib/pgsql/9.3/archive synckey = /var/lib/pgsql/.ssh/pgarchivesync # Exit code to Postgres FAILURE = 0 # Copy the file locally to the archive directory /bin/gzip $1 $archivedir / $2 .gz rc = $? if [ $rc ! = 0 ] ; then FAILURE = 1 exit 1 fi me = ` hostname -f ` sitea_nodes = pgdb1.sitea.com pgdb2.sitea.com pgdb3.sitea.com siteb_nodes = pgdb1.siteb.com pgdb2.siteb.com pgdb3.siteb.com pgdb4.siteb.com all_nodes = ${ sitea_nodes } ${ siteb_nodes } # Remove myself from the node list, no need to sync to myself nodes =( ${ all_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` #Get my domain mydomain = ` /bin/dnsdomainname ` # Set a list of nodes to verify the archive logs are synced to if [ ${ mydomain } == siteb.com ] ; then all_verifynodes = ${ chantilly_nodes } verifynodes =( ${ all_verifynodes [@]// ${ me }} ) verifynodes = ` echo ${ verifynodes [@] } ` elif [ ${ mydomain } == sitea.com ] ; then all_verifynodes = ${ ashburn_nodes } verifynodes =( ${ all_verifynodes [@]// ${ me }} ) verifynodes = ` echo ${ verifynodes [@] } ` fi # Sync the archive dir with the currently correct replicas for node in ${ nodes } ; do /usr/bin/nc -z -w2 ${ node } 22 /dev/null 2 1 rc = $? if [ $rc ! = 0 ] ; then /usr/bin/logger PGSQL Archive Sync Failure: ${ node } is not accessible for archive syncing, skipping this node if [[ ${ verifynodes [*] } = ~ ${ node } ]] ; then FAILURE = 1 fi else /usr/bin/rsync -avzp --delete -e ssh -i $synckey $archivedir / postgres@ $node : $archivedir rc = $? if [ $rc ! = 0 ] ; then /usr/bin/logger PGSQL Archive Sync Failure: ${ node } RSYNC failure if [[ ${ verifynodes [*] } = ~ ${ node } ]] ; then FAILURE = 1 fi fi fi done exit $FAILURE * In place of a single command a script should be used for the restore_command attribute/parameter used in both the cluster configuration and postgresql.conf EXAMPLE SCRIPT HERE Monitoring Monitor everything!","title":"Building a highly available multi node cluster with pacemaker & corosync"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#overview","text":"Building a highly available multi-node PostgreSQL cluster, using freely available software including Pacemaker , Corsync , Cman and PostgresSQL on CentOS","title":"Overview"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#infrastructure","text":"","title":"Infrastructure"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#vagrant-setup","text":"In order to assist in following along with this tutorial, you can use the following Vagrantfile to spool up a cluster environment using CentOS 6.6 For those unfaimilar with please download and install the latest version from vagrantup . Once installed create a root directory that will house files for this project. mkdir pgdb_cluster Navigate into the newly created root directory cd pgdb_cluster Download the Vagrantfile noted above into your root directory wget http://kb.techtaco.org/linux/postgresql/attachments/Vagrantfile Now to create the three virtual machines needed for this development environment we simply run {varant up}. This will read the Vargrantfile located in the project root directory. This downloads the neede .box (virtual machine) image and creates the needed clones with specified modifications. vagrant up Once the virtual machines are provisioned and started you can access them via the { vagrant ssh } command. Replace pgdb1 with the other machines to access them as well, note you must be in the root directory of the project. vagrant ssh pgdb1","title":"Vagrant Setup"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#ha-cluster-installation","text":"The required are available and included in the base/updates repositories for Centos 6.x. From my readings and research it is also possible to use heartbeat 3.x with Pacemaker to achieve similar results. I've decided to go with Corosync as its backed by Red Hat and Suse and it looks to have more active development. Not to mention that the Pacemaker projects recommends you should use Corosync.","title":"HA Cluster Installation"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#cluster-installation","text":"Warning! As of RedHat/CentOS 6.4 crmsh is no longer included in the default repositories. If you want to use CRM vs PCS You can include the OpenSuse repositories HERE . More information on the crmsh can be found HERE In this tutorial we will add the openSUSE repository to our nodes. Though I recommend building or copying these packages into a local repository for more controlled management. Configure the openSUSE repository. This need to be done on ALL nodes in the cluster. sudo wget -4 http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/RedHat_RHEL-6/network:ha-clustering:Stable.repo -O /etc/yum.repos.d/network_ha-clustering_Stable.repo Limit the packages to be installed from the openSUSE repository. We only want the crm shell package and required dependencies. This need to be done on ALL nodes in the cluster. sudo runuser -l root -c echo includepkgs=crmsh pssh python-pssh /etc/yum.repos.d/network_ha-clustering_Stable.repo Now that we have the required repositories configured we need to install the needed packages. This need to be done on ALL nodes in the cluster. You will see multiple dependencies being pulled in sudo yum install pacemaker pcs corosync fence-agents crmsh cman ccs","title":"Cluster Installation"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#cluster-configuration","text":"The first step is to configure the underlying Cman/Corosync cluster ring communication between the nodes and setup Pacemaker to use Corosync as its communication mechanism. For secure communication Corosync requires an pre-shared authkey. This shared key must be added to all nodes in the cluster. To generate the authkey Corosync has a utility corosync-keygen. Invoke this command as the root users to generate the authkey. The key will be generated at /etc/corosync/authkey. You only need to perform this action on one of the nodes in the cluster as we'll copy it to the other nodes sudo /usr/sbin/corosync-keygen Hint! Grab a cup of coffee this process takes a while to complete as it pulls from the more secure /dev/random. You don\u2019t have to press anything on the keyboard it will still generate the authkey** Once the key has been generated copy it to the other nodes in the cluster sudo scp /etc/corosync/authkey root@pgdb2:/etc/corosync/ sudo scp /etc/corosync/authkey root@pgdb3:/etc/corosync/ In multiple examples and documents on the web they reference using the packmaker corosync plugin by adding a /etc/corosync/service.d/pcmk configure file on each node. This is becoming deprecated and will show in the logs if you enable or use the corosync pacemaker plugin. There is a small but important distinction that I stumbled upon, the pacemaker plugin has never been supported on RHEL systems. The real issue is that at some point it will no longer be supplied with the packages on RHEL systems. Prior to 6.4 ( Though this is looking to change in 6.5 and above ), pacemaker only had a tech preview status for the plugin and using the CMAN plugin instead. Reference this wiki article Disable quorum in order to allow Cman/Corosync to complete startup in a standalone state. This need to be done on ALL nodes in the cluster. sudo sed -i.sed s/.*CMAN_QUORUM_TIMEOUT=.*/CMAN_QUORUM_TIMEOUT=0/g /etc/sysconfig/cman Define the cluster, where pg_cluster is the cluster name. This will generate the cluster.conf configuration file. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --createcluster pg_cluster Create the cluster redundant ring(s). The name used for each node in the cluster should correspond to the nodes network hostname uname -n . This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addnode pgdb3.example.com Configure the fence_pcmk agent (supplied with Pacemaker) to redirect any fencing requests from CMAN components (such as dlm_controld) to Pacemaker. Pacemaker\u2019s fencing subsystem lets other parts of the stack know that a node has been successfully fenced, thus avoiding the need for it to be fenced again when other subsystems notice the node is gone. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect pgdb3.example.com sudo ccs -f /etc/cluster/cluster.conf --addfencedev pcmk agent=fence_pcmk sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb1.example.com pcmk-redirect port=pgdb1.example.com sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb2.example.com pcmk-redirect port=pgdb2.example.com sudo ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk pgdb3.example.com pcmk-redirect port=pgdb3.example.com Enable secure communciation between nodes in the Corosync cluster using the corosync authkey generated above. This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo ccs -f /etc/cluster/cluster.conf --setcman keyfile= /etc/corosync/authkey transport= udpu Before we start the cman service and copy the configuration to the other nodes in the cluster lets verify that the generated configuration values are valid. This should be run on the same node as the pervious commands. For simplicity we will run this on pgdb1 in the cluster sudo ccs_config_validate -f /etc/cluster/cluster.conf Start the Cman/Corosync cluster services This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo /etc/init.d/cman start Starting cluster: Checking if cluster has been disabled at boot... [ OK ] Checking Network Manager... [ OK ] Global setup... [ OK ] Loading kernel modules... [ OK ] Mounting configfs... [ OK ] Starting cman... [ OK ] Waiting for quorum... [ OK ] Starting fenced... [ OK ] Starting dlm_controld... [ OK ] Tuning DLM kernel config... [ OK ] Starting gfs_controld... [ OK ] Unfencing self... [ OK ] Joining fence domain... [ OK ] Note that starting Cman also starts the Corosync service. This can be easily verified via the Corosync init script sudo /etc/init.d/corosync status corosync (pid 18376) is running... Start the Pacemaker cluster service This only needs to be run on one node, we'll copy it to the other nodes. For simplicity we will run this on pgdb1 in the cluster sudo /etc/init.d/pacemaker start Starting Pacemaker Cluster Manager [ OK ] Before continuing verify that all services have correctly started and are running. sudo /etc/init.d/cman status cluster is running. sudo /etc/init.d/corosync status corosync (pid 615) is running... sudo /etc/init.d/pacemaker status pacemakerd (pid 868) is running... After the initial node has been successfully configured and services have started copy the cluster.conf to the other nodes in the cluster sudo scp /etc/cluster/cluster.conf pgdb2.example.com:/etc/cluster/cluster.conf sudo scp /etc/cluster/cluster.conf pgdb3.example.com:/etc/cluster/cluster.conf Start the Cman/Corosync services on additional nodes in the cluster. sudo /etc/init.d/cman start Starting cluster: Checking if cluster has been disabled at boot... [ OK ] Checking Network Manager... [ OK ] Global setup... [ OK ] Loading kernel modules... [ OK ] Mounting configfs... [ OK ] Starting cman... [ OK ] Waiting for quorum... [ OK ] Starting fenced... [ OK ] Starting dlm_controld... [ OK ] Tuning DLM kernel config... [ OK ] Starting gfs_controld... [ OK ] Unfencing self... [ OK ] Joining fence domain... [ OK ] Before continuing and starting Pacemaker on additional nodes in the cluster verify that ALL nodes in the cluster are communicating via the Cman/Corosync cluster ring. View cluster ring Cman/Corosync status, this should be run on ALL nodes to verify that are correctly communicating sudo cman_tool nodes -a Node Sts Inc Joined Name 1 M 4 2014-04-09 08:30:22 pgdb1.example.com Addresses: 10.4.10.60 2 M 8 2014-04-09 08:44:01 pgdb2.example.com Addresses: 10.4.10.61 3 M 12 2014-04-09 08:44:08 pgdb3.example.com Addresses: 10.4.10.62 Start the Pacemaker service on additional nodes in the cluster sudo /etc/init.d/pacemaker start Starting Pacemaker Cluster Manager [ OK ] Before continuing verify that all services have correctly started and are running on the additional nodes in the cluster. sudo /etc/init.d/cman status cluster is running. sudo /etc/init.d/corosync status corosync (pid 615) is running... sudo /etc/init.d/pacemaker status pacemakerd (pid 868) is running... Verify that ALL nodes have joined the Pacemaker cluster. View Pacemaker HA cluster status sudo pcs status Cluster name: pg_cluster Last updated: Thu Apr 10 07:39:08 2014 Last change: Thu Apr 10 06:49:19 2014 via cibadmin on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.10-14.el6_5.2-368c726 3 Nodes configured 0 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources:","title":"Cluster Configuration"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#pacemaker-cluster-configuration","text":"At this point we have configured the basic cluster communication ring. All nodes are now communicating and reporting their heartbeat status to each of the nodes via Corosync. Verify the Pacemaker cluster configuration. Here you'll notice the cluster is complaining that STONITH (Shoot The Other Node In The Head) is not configured. sudo pcs cluster verify -V error: unpack_resources: Resource start-up disabled since no STONITH resources have been defined error: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option error: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity Errors found during check: config not valid For now we are going to disable this and come back to it later in the tutorial. This only needs to be run on one node of the cluster as they are syncing configurations between the nodes. sudo pcs property set stonith-enabled = false Verify the Pacemaker stonith property was correctly configured. sudo pcs config Cluster Name: pg_cluster Corosync Nodes: Pacemaker Nodes: pgdb1.example.com pgdb2.example.com pgdb3.example.com Resources: Stonith Devices: Fencing Levels: Location Constraints: Ordering Constraints: Colocation Constraints: Cluster Properties: cluster-infrastructure: cman dc-version: 1 .1.10-14.el6_5.2-368c726 stonith-enabled: false ```` Now verifying the Pacemaker cluster configuration again returns no errors. ```` sudo pcs cluster verify -V ```` Pacemaker IP Resources ---------------------- With a basic cluster configuration setup resources can be created for the cluster to manage. The first resource to add is a cluster IP or VIP so that applications will be able to continuously communicate with the cluster regardless of where the cluster services are running. Resources only need to be created on **one** node in the cluster Pacemaker/Corosync will replicate the cluster information base ( CIB ) to all nodes in the cluster Create a IP resources VIPs using the **ocf:heartbeat:IPaddr2** resource agent script . Create Replication VIP : This will be used for additional PostgreSQL replicas to recieve updates from the Master ```` bash sudo pcs resource create pgdbrepvip ocf:heartbeat:IPaddr2 ip = 10 .10.10.104 cidr_netmask = 24 iflabel = pgdbrepvip op monitor interval = 1s meta target-role = Started ```` Create Client Access VIP : This will be used for client applications to connect to the acitve Master database ```` bash sudo pcs resource create pgdbclivip ocf:heartbeat:IPaddr2 ip = 10 .10.10.105 cidr_netmask = 24 iflabel = pgdbclivip op monitor interval = 1s meta target-role = Started ```` Verify the Pacemaker cluster resource has been correctly added to the cluster information base ( CIB ) . ```` bash sudo pcs config ```` ```` Cluster Name: pg_cluster Corosync Nodes: Pacemaker Nodes: pgdb1.example.com pgdb2.example.com pgdb3.example.com Resources: Resource: pgdbrepvip ( class = ocf provider = heartbeat type = IPaddr2 ) Attributes: ip = 10 .10.10.104 cidr_netmask = 24 iflabel = pgdbrepvip Meta Attrs: target-role = Started Operations: monitor interval = 1s ( pgdbrepvip-monitor-interval-1s ) Resource: pgdbclivip ( class = ocf provider = heartbeat type = IPaddr2 ) Attributes: ip = 10 .10.10.105 cidr_netmask = 24 iflabel = pgdbclivip Meta Attrs: target-role = Started Operations: monitor interval = 1s ( pgdbclivip-monitor-interval-1s ) Stonith Devices: Fencing Levels: Location Constraints: Ordering Constraints: Colocation Constraints: Cluster Properties: cluster-infrastructure: cman dc-version: 1 .1.10-14.el6_5.2-368c726 stonith-enabled: false ```` View the running status of the cluster. Here we can see that both the IP resources VIPs are running. ```` sudo pcs status ```` ```` Cluster name: pg_cluster Last updated: Thu Apr 10 08 :04:14 2014 Last change: Thu Apr 10 07 :53:03 2014 via cibadmin on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.10-14.el6_5.2-368c726 3 Nodes configured 2 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb2.example.com ```` The pgdbclivip IP resource VIP was started on pgdb2, for simplicity we will move it to pgdb1. ```` sudo pcs resource move pgdbclivip pgdb1.example.com ```` Viewing the running status of the cluster again we can see both resources are now running on pgdb1 ```` sudo pcs status ```` ```` Cluster name: pg_cluster Last updated: Thu Apr 10 08 :11:48 2014 Last change: Thu Apr 10 08 :06:56 2014 via crm_resource on pgdb1.example.com Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.10-14.el6_5.2-368c726 3 Nodes configured 2 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com ```` PostgreSQL Database Configuration --------------------------------- Before adding a Pacemaker pgsql resource to manage the PostgreSQL services, its recommended to setup the PostgreSQL cluster ( The PostgreSQL internal cluster ) with some basic streaming replication. The version of PostgreSQL that is in the provided repositories on CentOS 6 .5 is 8 .4.20 which does not provide the needed streaming replication. To work around this we will add PGDG ( PostgreSQL Global Development Group ) repository. As of this writing we are using PostgreSQL version 9 .3.5 Configure the needed repository. This need to be done on **ALL** nodes in the cluster. ``` bash sudo wget http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm -O /tmp/pgdg-centos93-9.3-1.noarch.rpm sudo rpm -Uvh /tmp/pgdg-centos93-9.3-1.noarch.rpm With the correct repository configured install the recommended packages. sudo yum install postgresql93-server postgresql93-contrib postgresql93-devel Initialize the PostgreSQL database via initdb. We only need to perform this on the Master node as we'll be transferring the database to the remaining nodes. I'll be referring to these nodes as PostgreSQL replicas (pgdb2, pgdb3). We will use pgdb1 as the Master from here on out. sudo /etc/init.d/postgresql-9.3 initdb Once the initialization is successful you'll see the PostgreSQL data directory populated. On CentOS this is located in /var/lib/pgsql/{version (9.3)}/data sudo ls /var/lib/pgsql/9.3/data/ When the database was initialized via initdb it configured permissions in the pg_hba.conf. This uses the ident scheme to determine if a user is allowed to connect to the database. ident : An authentication schema that relies on the currently logged in user. If you\u2019ve su -s to postgres and then try to login as another user, ident will fail (as it\u2019s not the currently logged in user). This can be a sore spot if you're not aware how it was configured and will produce an error if trying to create a database with a user that is not currently logged into the system. createdb: could not connect to database postgres: FATAL: Ident authentication failed for user \"myUser\" To avoid this headache modify the pg_hba.conf file to move from the ident scheme to the md5 scheme. This needs to be modified on the Master pgdb1 sudo sed -i s/\\ ident/\\ md5/g /var/lib/pgsql/9.3/data/pg_hba.conf Result: # IPv4 local connections: host all all 127 .0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5 Modify the pg_hba.conf to allow the repclias to connect to the Master . In this tutorial we are adding a basic connection line. It is recommended that you tune this based on your infrastructure for proper security. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/pg_hba.conf # Allowing Replicas to connect in for streaming replication host replication replicator all trust EOF Configure the bind address the PostgreSQL will listen on. This needs to be set to * so the PostgreSQL service will listen on any address. PostgreSQL will scan for new addresses and automatically bind to them as they appear on the node. This is required to allow PostgreSQL to start listening on the VIP address in the event of node failover. Modify the postgresql.conf with your favorite text editor and modify the listen_addresses parameter or add an additional parameter to the end of the configuration file. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c echo \\ listen_addresses = * \\ /var/lib/pgsql/9.3/data/postgresql.conf The PostgreSQL has the concept of archiving for its WAL logs. Its recommended to create a separate archive directory, this will be used to store and recover archived WAL logs. In this tutorial we will create this in the current PostgreSQL {version} directory, you can create this anywhere. This need to be done on ALL nodes in the cluster. sudo runuser -l postgres -c mkdir /var/lib/pgsql/9.3/archive Configure the ability for PostgreSQL to sync archive logs between the nodes for recovery and backup. To configure syncing of the WAL archives between each of the Postgresql nodes, we'll setup a custom script that will be called by the archive_command in the postgresql.conf. This script utilizes rsync to keep the archive directory in sync on each of the nodes with the Postgresql Master. In order to facilitate syncing between the nodes we'll be using ssh keys to allow the nodes to send updates automagically. We will be creating the ssh key with no passphrase named pgarchivesync for the postgresql user. This needs to be run on ALL nodes sudo runuser -l postgres -c ssh-keygen -t rsa -f /var/lib/pgsql/.ssh/pgarchivesync -N In order to simplfy the setup of the authorized_key files for the postgres user we need to set a password. This needs to be done on ALL nodes echo password | sudo passwd postgres --stdin With the keys generated on all of the nodes, the public key for the postgres user on each nodes needs added to the authorized_key file on each of the other nodes that we want to maintain syncing between. me = ` hostname ` cluster_nodes = pgdb1 pgdb2 pgdb3 nodes =( ${ cluster_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` for node in ${ nodes } ; do sudo ssh-copy-id -i /var/lib/pgsql/.ssh/pgarchivesync.pub postgres@ ${ node } ; done Accept the ssh host keys from each of the other nodes in the cluster. me = ` hostname ` cluster_nodes = pgdb1 pgdb2 pgdb3 nodes =( ${ cluster_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` for node in ${ nodes } ; do sudo runuser -l postgres -c ssh -o StrictHostKeyChecking=no -i /var/lib/pgsql/.ssh/pgarchivesync postgres@ ${ node } exit ; done To add a bit more security, which hasn't really been the practice so far in this tutorial we will lock down that can be run with using the pgarchivesync ssh key. This needs to be run on ALL nodes sudo sed -i s/^/command= \\/usr\\/bin\\/rsync --server -avzp --delete \\/var\\/lib\\/pgsql\\/9.3\\/archive ,no-pty,no-agent-forwarding,no-port-forwarding / /var/lib/pgsql/.ssh/authorized_keys With the script ssh key requirements taken care of, we need to populate the script on ALL of the nodes in the cluster. This will allow the archive syncing to happen from any node that is promoted to Master. sudo runuser -l root -c cat EOF /usr/local/sbin/pgarchivesync.sh #!/bin/bash archivedir= /var/lib/pgsql/9.3/archive synckey= /var/lib/pgsql/.ssh/pgarchivesync # Exit code to Postgres FAILURE=0 # Copy the file locally to the archive directory /bin/gzip \\$1 \\$archivedir/\\$2.gz rc=\\$? if [ \\$rc != 0 ]; then FAILURE=1 exit 1 fi me=\\`hostname\\` cluster_nodes= pgdb1 pgdb2 pgdb3 #cluster_nodes=\\`sudo cman_tool nodes -F name | sed s/.example.com//g \\` nodes=(\\${cluster_nodes[@]//\\${me}}) nodes=\\`echo \\${nodes[@]}\\` verifynodes=\\`echo \\${nodes[@]}\\` # Sync the archive dir with the currently correct replicas for node in \\${nodes}; do /usr/bin/nc -z -w2 \\${node} 22 /dev/null 2 1 rc=\\$? if [ \\$rc != 0 ]; then /usr/bin/logger PGSQL Archive Sync Failure: \\${node} is not accessible for archive syncing, skipping this node if [[ \\${verifynodes[*]} =~ \\${node} ]]; then FAILURE=1 fi else /usr/bin/rsync -avzp --delete -e ssh -i \\$synckey \\$archivedir/ postgres@\\$node:\\$archivedir rc=\\$? if [ \\$rc != 0 ]; then /usr/bin/logger PGSQL Archive Sync Failure: \\${node} RSYNC failure if [[ \\${verifynodes[*]} =~ \\${node} ]]; then FAILURE=1 fi fi fi done exit \\$FAILURE EOF Make the command executable so that the postgres user can run the archive sync script. sudo chmod +x /usr/local/sbin/pgarchivesync.sh Configure PostgreSQL steaming replication in the postgresql.conf file. These settings are very basic and will need to be tuned based on your infrastructure. This needs to be modified on the Master pgdb1 sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/postgresql.conf wal_level = hot_standby archive_mode = on archive_command = /usr/local/sbin/pgarchivesync.sh %p %f max_wal_senders = 3 wal_keep_segments = 100 hot_standby = on EOF Start the PostgreSQL service on the Master and check for erros in /var/lib/pgsql/9.3/pg_log/*. sudo /etc/init.d/postgresql-9.3 start Starting postgresql-9.3 service: [ OK ] Once the PostgreSQL service is started, to assist with the replication process and some basic security, a separate replication user account should be created. sudo runuser -l postgres -c psql -c \\ CREATE USER replicator REPLICATION LOGIN ENCRYPTED PASSWORD replaceme ;\\ With a functioning Master database up and running, the replicas (slaves) need to be initialized and configured to synchronize from the Master To clone the PostgreSQL database cluster from the Master node to the replicas (pgdb2, pgdb3). We are using a modern version of PostgreSQL that include pg_basebackup, which makes the process 1000000000 time simpler. You can also use pg_start_backup, rsync and pg_stop_backup to perform a more manaul cloning. On the replica nodes (pgdb2, pgdb3) run the pg_basebackup command sudo runuser -l postgres -c pg_basebackup -D /var/lib/pgsql/9.3/data -l `date + %m-%d-%y `_initial_cloning -P -h pgdb1.example.com -p 5432 -U replicator -W -X stream To avoid any confusion with troubleshooting remove the log files that were transferred during the pg_basebackup process. This needs to be done on both replicas sudo runuser -l root -c rm /var/lib/pgsql/9.3/data/pg_log/* In order for the replicas to connect to the Master for streaming replication a recovery.conf file must exist in the PostgreSQL data directory. Create a recovery.conf file on both replicas (pgdb2, pgdb3) sudo runuser -l postgres -c cat EOF /var/lib/pgsql/9.3/data/recovery.conf standby_mode = on primary_conninfo = host=10.10.10.104 port=5432 user=replicator application_name=`hostname` restore_command = gunzip /var/lib/pgsql/9.2/archive/%f.gz \\ %p\\ EOF Start the PostgreSQL service on both of the replicas (pgdb2, pgdb3) sudo /etc/init.d/postgresql-9.3 start On the Master verify and view the active replica connections and their status. You'll notice the sync_state is async for both nodes, this is because we have not set the standby_node_names parameter on the master to let it know what nodes it should attempt to perform synchronous replication with. You'll also notice the state is streaming, this is continually sending changes to the replicas/slaves without waiting for WAL segments to fill and then be shipped. sudo runuser -l postgres -c psql -c \\ SELECT application_name, client_addr, client_hostname, sync_state, state, sync_priority, replay_location FROM pg_stat_replication;\\ application_name | client_addr | client_hostname | sync_state | state | sync_priority | replay_location ------------------------+-------------+-----------------+------------+-----------+---------------+----------------- pgdb2.example.com | 10.4.10.61 | | async | streaming | 0 | 0/40000C8 pgdb3.example.com | 10.4.10.62 | | async | streaming | 0 | 0/40000C8","title":"Pacemaker Cluster Configuration"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#pacemaker-postgresql-resource","text":"This will be a master/slave resource as opposed to the primitive resource that we created above for the IP resources (VIPs). The resource-agents package that was installed above includes a pgsql resource agent that was designed to work with Postgresql 9.1+ and streaming replication Before we create the master/slave resource we need to stop the PostgreSQL service on ALL the nodes (pgdb1 pgdb2 pgdb3). This is because the Pacemaker PostgreSQL resource will be controlling the state of the PostgreSQL service. sudo /etc/init.d/postgresql-9.3 stop Additionally the run level for the PostgreSQL service needs set so that it does NOT start on boot. This is to insure that the Pacemaker PostgreSQL resource has rull control of the PostgreSQL service. sudo /sbin/chkconfig postgresql-9.3 off Lastly to avoid issues with non master nodes (pgdb2, pgdb3) becoming the master as dictated by the Pacemaker cluster, we will place the additonal nodes into standby mode. This also helps prevent nodes slipping to a different timeline. sudo pcs cluster standby pgdb2.example.com; sudo pcs cluster standby pgdb3.example.com Verify that pgdb2 and pgdb3 have been placed into standby mode sudo pcs status Cluster name: pg_clu Last updated: Tue Nov 11 11:31:33 2014 Last change: Tue Nov 11 11:31:32 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 2 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com To create a stateful/multi-state resource, a primitive resource must first be created and the from that primitive resource you can create a master/slave resource. This resource only needs to be created on one node in the cluster Pacemaker/Corosync will replicate the cluster information base (CIB) to all nodes in the cluster. For simplicity create this resource on the Master pgdb1. The parameters are all setting located in the pgsql resource script, provided via the resource-agents package. In preparation to move this to a multi-state resource multiple monitoring operations are included for each Pacemaker cluster state. sudo /usr/sbin/pcs resource create postgresql ocf:heartbeat:pgsql \\ pgctl= /usr/pgsql-9.3/bin/pg_ctl \\ pgdata= /var/lib/pgsql/9.3/data \\ psql= /usr/pgsql-9.3/bin/psql \\ config= /var/lib/pgsql/9.3/data/postgresql.conf \\ stop_escalate= 5 \\ rep_mode= sync \\ node_list= pgdb1.example.com pgdb2.example.com pgdb3.example.com \\ restore_command= gunzip /var/lib/pgsql/9.3/archive/%f.gz \\ %p\\ \\ master_ip= 10.10.10.104 \\ repuser= replicator \\ restart_on_promote= true \\ tmpdir= /var/lib/pgsql/9.3/tmpdir \\ xlog_check_count= 3 \\ crm_attr_timeout= 5 \\ check_wal_receiver= true \\ op start timeout= 60s interval= 0s on-fail= restart \\ op monitor timeout= 30 interval= 2s \\ op monitor timeout= 30 interval= 1s role= Master \\ op promote timeout= 60s interval= 0s on-fail= restart \\ op demote timeout= 60s interval= 0s on-fail= stop \\ op stop timeout= 60s interval= 0s on-fail= block \\ op notify timeout= 60s interval= 0s Create the multi-state resource from the primitive resource created above sudo /usr/sbin/pcs resource master mspostgresql postgresql \\ notify= true \\ target-role= Started Verify that creation and status of the PostgreSQL cluster resources Note! You will notice the resource fail after adding it from above, this is because the primitive resource for PostgreSQL was created with an active node and rep_mode parameter requires a Master/Slave resource. pgsql(postgresql)[28784]: ERROR: Replication(rep_mode=async or sync) requires Master/Slave configuration. sudo pcs status To clear the failures and allow the PostgreSQL service to start controlled via Pacemaker the fail count must be cleared and service re-probed The command below sets the fail counts to 0 for the resource postgresql on the node pgdb1.example.com sudo crm resource failcount postgresql set pgdb1.example.com 0; sudo crm_resource -P Checking the cluster status you can see that pgdb1 has now started the postgresql resource as a Slave. If you tail /var/log/messages you'll notice that the cluster is processing the pdgdb1 node and in the process of promoting it to Master. sudo pcs status Cluster name: pg_cluster Last updated: Fri Nov 14 07:38:56 2014 Last change: Fri Nov 14 07:38:55 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Slaves: [ pgdb1.example.com ] Stopped: [ pgdb2.example.com pgdb3.example.com ] After a brief period of time you should now see that pgdb1 has started as a Master. This resource now starts and manages the PostgreSQL service sudo pcs status Cluster name: pg_cluster Last updated: Tue Nov 11 13:23:16 2014 Last change: Tue Nov 11 13:21:05 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Node pgdb3.example.com: standby Online: [ pgdb1.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Stopped: [ pgdb2.example.com pgdb3.example.com ] The additional nodes now need to placed into online mode within the Pacemaker cluster. This will start the postgresql resource and in turn start the postgresql service on each of the replica nodes. Place pgdb2 into online mode within the cluster. Execute the following command on any node in the cluster. sudo pcs cluster unstandby pgdb2.example.com Place pgdb3 into online mode within the cluster. Execute the following command on any node in cluster. sudo pcs cluster unstandby pgdb3.example.com Checking the cluster status you now will see that both pgdb2 and pgdb3 have started the postgresql resource as Slaves. sudo pcs status Cluster name: pg_cluster Last updated: Wed Nov 12 06:39:22 2014 Last change: Wed Nov 12 06:30:37 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] With all the nodes started in the Pacemaker cluster and running the needed resources we need to create resource constraints. The first constraint that is needed is for the colocation of the VIPs (pgdbrepvip, pgdbclivip) and the PostgreSQL service, for the node that is running as Master. This is so that the pgdbrepvip and pgdbclivip are always started on the same node that is running the postgresql resource in Master mode. This is so if one or more of the resources fails on the Master node resource are migrated to the new node sudo pcs constraint colocation set pgdbrepvip role=Started set mspostgresql role=Master set pgdbclivip role=Started setoptions score=INFINITY Then we'll set the order that the cluster resources should be started or promoted in. For this we'll be starting the pgdbrepvip resource then the postgresql resource and lastly the pgdbclivip resource. This seems counter intuitive to start the postgresql service before starting the PG_CLI_VIP for postgresql to bind to. This is worked around by the listen_addresses=\"*\" parameter that we set above. This tells postgresql to listen on all interfaces regardless if the are coming up after the postgresql service starts. This gives us the benefit of not allowing application utilizing the VIP to access the database until it is up. Note: The version of pcs doesn't support configuring the order set as needed. This was brought up in IRC #linux-cluster and fiest__ is working on the fix to add this ability For now we will use the crm command to properly configure the ordering constraint, more detail below is given on the crm command sudo crm configure order order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory inf: pgdbrepvip:start mspostgresql:promote pgdbclivip:start The last constraint we'll set is resource stickiness. The concept of resource stickiness which controls how much a cluster resource prefers to stay running where it is. You may like to think of it as the \"cost\" of any downtime. By default, Pacemaker assumes there is zero cost associated with moving resources and will do so to achieve \"optimal\" resource placement. We can specify a different stickiness for every resource, but it is often sufficient to change the default as seen below. After setting the default resources stickiness when setting a node into standby mode and then placing it back into online mode, resources will remain on the node that they migrated to sudo pcs property set default-resource-stickiness=100 Something you might have noticed is that there is a location constraint listed in the configuration that will cause issues with the execution of the other constraints. I'm a little unsure why its created and will update this document when I have a more in-depth answer. View the current constraints sudo pcs constraint list --full Location Constraints: Resource: pgdbclivip Enabled on: pgdb1.example.com (score:INFINITY) (role: Started) (id:cli-prefer-pgdbclivip) Ordering Constraints: Resource Sets: set pgdbrepvip action=start (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-0) set mspostgresql action=promote (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-1) set pgdbclivip action=start (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory-2) setoptions score=INFINITY (id:order-pgdbrepvip-mspostgresql-pgdbclivip-mandatory) Colocation Constraints: Resource Sets: set pgdbrepvip role=Started (id:pcs_rsc_set_pgdbrepvip-1) set mspostgresql role=Master (id:pcs_rsc_set_mspostgresql-1) set pgdbclivip role=Started (id:pcs_rsc_set_pgdbclivip-1) setoptions score=INFINITY (id:pcs_rsc_colocation_pgdbrepvip_set_mspostgresql_set_pgdbclivip) Remove the location constraint, take note of the constraint ID from the output above sudo pcs constraint remove cli-prefer-pgdbclivip","title":"Pacemaker PostgreSQL Resource"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#fencing-and-stonith","text":"Note! This section is still a work in progress","title":"Fencing and STONITH"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#additional-tools","text":"","title":"Additional Tools"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#crm","text":"Packages pulled down from the OpenSUSE repository at the beginning of this tutorial provide the crm command. This is an alternative to the pcs command used to control/configure the Pacemaker cluster. Unlike the pcs command the crm command provides an live interactive shell along with tab completion to assist in cluster control/configuration. sudo crm crm ( live ) # cluster status Services: corosync unknown pacemaker unknown Printing ring status. Local node ID 3 RING ID 0 id = 10 .10.10.103 status = ring 0 active with no faults crm ( live ) # status Last updated: Fri Nov 14 15 :48:39 2014 Last change: Fri Nov 14 13 :07:04 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ]","title":"CRM"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#crm_mon","text":"So far in this guide the pcs command for cluster configuration and to see a quick overview of the cluster status. There however is tool provided with the pacemaker-cli package that provides near real-time details and 'monitoring' of the cluster. The crm_mon command is very extensive and can even provide output in a nagios/icinga format. The options that I have found move useful are: -A, --show-node-attributes Display node attributes -r, --inactive Display inactive resources -f, --failcounts Display resource fail counts -i, --interval=value Update frequency in seconds This will list sudo crm_mon -Arf -i1 Last updated: Fri Nov 14 15 :09:16 2014 Last change: Fri Nov 14 13 :07:04 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 0000000010000090 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync + postgresql-xlog-loc : 00000000100000F8 * Node pgdb3.example.com: + master-postgresql : -INFINITY + postgresql-data-status : STREAMING | POTENTIAL + postgresql-receiver-status : normal + postgresql-status : HS:potential Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com:","title":"CRM_MON"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#node-failover","text":"Below is an extensive list of possible failover senarios that might be experienced within the cluster. I wont be covering each of these in detail but you should test each of these senarios in your production environment before going live to verify the cluster is correctly configured and promotion is happening as expected. For each of the exercises below open a seperate terminal window running the crm_mon command to view the failover in near real time. sudo crm_mon -Arf -i1","title":"Node Failover"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#fail-the-asynchronous-node-pgdb3","text":"Place pgdb3 into standby mode within the cluster. sudo pcs cluster standby pgdb3.example.com You should see pgdb3 placed into standby mode and its postgresql-data-status reported as disconnected Last updated: Mon Nov 17 06:12:16 2014 Last change: Mon Nov 17 06:12:12 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1.1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb3.example.com: standby Online: [ pgdb1.example.com pgdb2.example.com ] Full list of resources: pgdbrepvip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com pgdbclivip (ocf::heartbeat:IPaddr2): Started pgdb1.example.com Master/Slave Set: mspostgresql [postgresql] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com ] Stopped: [ pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING|SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync * Node pgdb3.example.com: + master-postgresql : -INFINITY + postgresql-data-status : DISCONNECT + postgresql-status : STOP Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Now 'reset the cluster by placing pgdb3 back into online mode' sudo pcs cluster unstandby pgdb3.example.com","title":"Fail the asynchronous node (pgdb3)"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#fail-the-synchronous-node-pgdb2","text":"Place pgdb2 into standby mode within the cluster sudo pcs cluster standby pgdb2.example.com You chould see pgdb2 placed into standby mode and its postgresql-data-status reported as disconnected. Notice that pgdb3 is now the synchronous node with its postgresql-data-status set to *|SYNC Last updated: Mon Nov 17 06 :21:23 2014 Last change: Mon Nov 17 06 :21:20 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Node pgdb2.example.com: standby Online: [ pgdb1.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb3.example.com ] Stopped: [ pgdb2.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : -INFINITY + postgresql-data-status : DISCONNECT + postgresql-status : STOP * Node pgdb3.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com: Now 'reset the cluster by placing pgdb3 back into online mode' sudo pcs cluster unstandby pgdb2.example.com Notice that pgdb2 is now the asynchronous node with its postgresql-data-status set to *|POTENTIAL Last updated: Mon Nov 17 06 :25:23 2014 Last change: Mon Nov 17 06 :25:22 2014 Stack: cman Current DC: pgdb1.example.com - partition with quorum Version: 1 .1.11-97629de 3 Nodes configured 5 Resources configured Online: [ pgdb1.example.com pgdb2.example.com pgdb3.example.com ] Full list of resources: pgdbrepvip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com pgdbclivip ( ocf::heartbeat:IPaddr2 ) : Started pgdb1.example.com Master/Slave Set: mspostgresql [ postgresql ] Masters: [ pgdb1.example.com ] Slaves: [ pgdb2.example.com pgdb3.example.com ] Node Attributes: * Node pgdb1.example.com: + master-postgresql : 1000 + postgresql-data-status : LATEST + postgresql-master-baseline : 00000000100000F8 + postgresql-receiver-status : ERROR + postgresql-status : PRI * Node pgdb2.example.com: + master-postgresql : -INFINITY + postgresql-data-status : STREAMING | POTENTIAL + postgresql-receiver-status : normal + postgresql-status : HS:potential * Node pgdb3.example.com: + master-postgresql : 100 + postgresql-data-status : STREAMING | SYNC + postgresql-receiver-status : normal + postgresql-status : HS:sync Migration summary: * Node pgdb1.example.com: * Node pgdb2.example.com: * Node pgdb3.example.com:","title":"Fail the synchronous node (pgdb2)"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#fail-the-master-node-pgdb1","text":"Place pgdb2 into standby mode within the cluster sudo pcs cluster standby pgdb1.example.com","title":"Fail the 'Master' node (pgdb1)"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#potential-failover-scenarios","text":"Master node loses network connectivity Master nodes' postgresql service fails Master node loses system power Master nodes' pgdbrepvip pacemaker resource fails Master nodes' pgdbclivip pacemaker resource fails Sync Replica node loses network connectivity Sync Replica nodes' postgresql service fails Sync Replica node loses system power Async Replica node loses network connectivity Async Replica nodes' postgresql service fails Async Replica node loses system power Sync and Async Replica nodes lose network connectivity Sync and Async Replica nodes lose system power Sync and Async Replica nodes postgresql service fails Master and Sync Replica nodes lose network connectivity Master and Sync Replica nodes lose system power Master and Sync Replica nodes postgresql service fails Master and Async Replica nodes lose network connectivity Master and Async Replica nodes lose system power Master and Async Replica nodes postgresql service fails Master,Sync Replica and Async Replica nodes lose network connectivity Master,Sync Replica and Async Replica nodes lose system power Master,Sync Replica and Async Replica nodes postgresql service fails","title":"Potential Failover Scenarios"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#production-considerations","text":"This tutorial details only a basic cluster setup and is not intended to be used in a production environment. Below are some points that you should consider within a production environment.","title":"Production Considerations"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#system-configuration","text":"Make sure you have a developed and well tested procedure for testing updates for then cluster components ( pacemaker pcs corosync fence-agents crmsh cman ccs ) Verify that DNS is 100% complete and accurate for all cluster nodes and pacemaker IP resources Packages downloaded from the OpenSUSE repositories should be hosted and maintained in a local repository. The OpenSUSE repository doesn't seem to support rsync or reposync so I've created a script to sync down the needed packages locally. #!/bin/bash DATE = ` /bin/date +%Y-%m-%d ` OUTDIR = /path/to/logs/dir/ OUTFILE = $OUTDIR /ha-cluster-mirror- $DATE .txt [ -d $OUTDIR ] || mkdir -p $OUTDIR URL = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/x86_64/ REPOKEY = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/repodata/repomd.xml.key DESDIR = /repo/dir/path/ha-cluster KEYDIR = /repo/dir/path/ha-cluster/repodata CRMRPMS = ` wget -4qO - $URL | grep -oe crm.*.rpm | cut -d -f1 ` for i in $CRMRPMS ; do wget -4 -N -P $DESDIR $URL$i $OUTFILE 2 1 done PSRPMS = ` wget -4qO - $URL | grep -oe pssh.*.rpm | cut -d -f1 ` for i in $PSRPMS ; do wget -4 -N -P $DESDIR $URL$i $OUTFILE 2 1 done wget -4 -N -P $KEYDIR $REPOKEY $OUTFILE 2 1 /usr/bin/createrepo /repo/dir/path/ha-cluster $OUTFILE 2 1","title":"System Configuration"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#cluster-configuration_1","text":"Configuring network based fencing, as it provides the flexibility of being able to access the machine via an alternative connection ( LOM, iDrac, Console ) and view the state of a fenced machine. Our current solution is to use the fence_ifmib script located in /usr/sbin that is provided with the fence-agents package to administratively disable ports that fenced nodes are connected to.","title":"Cluster Configuration"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#postgresql-best-practices","text":"A unique and strong password should be set for the replicator PostgreSQL account A unique and strong password should be set for the postgres NIX account The pg_hba.conf file should limit the nodes allowed to replicate from the master with the method trust via specific IPs (/32) # pgdb2.example.com host replication replicator 10 .10.10.102/32 trust A more robust archive_command command should be used. Its recommended to use a script that does error checking. Below is a example script that moves the xlogs to an #!/bin/bash archivedir = /var/lib/pgsql/9.3/archive synckey = /var/lib/pgsql/.ssh/pgarchivesync # Exit code to Postgres FAILURE = 0 # Copy the file locally to the archive directory /bin/gzip $1 $archivedir / $2 .gz rc = $? if [ $rc ! = 0 ] ; then FAILURE = 1 exit 1 fi me = ` hostname -f ` sitea_nodes = pgdb1.sitea.com pgdb2.sitea.com pgdb3.sitea.com siteb_nodes = pgdb1.siteb.com pgdb2.siteb.com pgdb3.siteb.com pgdb4.siteb.com all_nodes = ${ sitea_nodes } ${ siteb_nodes } # Remove myself from the node list, no need to sync to myself nodes =( ${ all_nodes [@]// ${ me }} ) nodes = ` echo ${ nodes [@] } ` #Get my domain mydomain = ` /bin/dnsdomainname ` # Set a list of nodes to verify the archive logs are synced to if [ ${ mydomain } == siteb.com ] ; then all_verifynodes = ${ chantilly_nodes } verifynodes =( ${ all_verifynodes [@]// ${ me }} ) verifynodes = ` echo ${ verifynodes [@] } ` elif [ ${ mydomain } == sitea.com ] ; then all_verifynodes = ${ ashburn_nodes } verifynodes =( ${ all_verifynodes [@]// ${ me }} ) verifynodes = ` echo ${ verifynodes [@] } ` fi # Sync the archive dir with the currently correct replicas for node in ${ nodes } ; do /usr/bin/nc -z -w2 ${ node } 22 /dev/null 2 1 rc = $? if [ $rc ! = 0 ] ; then /usr/bin/logger PGSQL Archive Sync Failure: ${ node } is not accessible for archive syncing, skipping this node if [[ ${ verifynodes [*] } = ~ ${ node } ]] ; then FAILURE = 1 fi else /usr/bin/rsync -avzp --delete -e ssh -i $synckey $archivedir / postgres@ $node : $archivedir rc = $? if [ $rc ! = 0 ] ; then /usr/bin/logger PGSQL Archive Sync Failure: ${ node } RSYNC failure if [[ ${ verifynodes [*] } = ~ ${ node } ]] ; then FAILURE = 1 fi fi fi done exit $FAILURE * In place of a single command a script should be used for the restore_command attribute/parameter used in both the cluster configuration and postgresql.conf EXAMPLE SCRIPT HERE","title":"PostgreSQL Best Practices"},{"location":"linux/postgresql/building_a_highly_available_multi-node_cluster_with_pacemaker_&_corosync/#monitoring","text":"Monitor everything!","title":"Monitoring"},{"location":"linux/postgresql/nav/","text":"PostgreSQL Articles Building A Highly Available Multi-Node Cluster With Pacemaker Corosync PostgreSQL File System Tuning PostgreSQL Disk Array, Layout and RAID Tuning","title":"PostgreSQL"},{"location":"linux/postgresql/nav/#postgresql-articles","text":"Building A Highly Available Multi-Node Cluster With Pacemaker Corosync PostgreSQL File System Tuning PostgreSQL Disk Array, Layout and RAID Tuning","title":"PostgreSQL Articles"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/","text":"PostgreSQL Disk Array, Layout and RAID Tuning Overview This article covers basic recommendation for setting up and configuring Disk Arrays, RAID and Disk Layout for your Postgresql database server. Definitions No-Read-Ahead: Selecting no-read-ahead policy indicates that the controller should not use read-ahead policy. Read-Ahead: When using read-ahead policy, the controller reads sequential sectors of the virtual disk when seeking data. Read-ahead policy may improve system performance if the data is actually written to sequential sectors of the virtual disk. Adaptive Read-Ahead: When using adaptive read-ahead policy, the controller initiates read-ahead only if the two most recent read requests accessed sequential sectors of the disk. If subsequent read requests access random sectors of the disk, the controller reverts to no-read-ahead policy. The controller continues to evaluate whether read requests are accessing sequential sectors of the disk, and can initiate read-ahead if necessary. Write-Back: When using write-back caching, the controller sends a write-request completion signal as soon as the data is in the controller cache but has not yet been written to disk. This is only recommended with a Battery Back Raid Controller. In that configuration, when the battery isn't there or isn't working, the controller will degrade to Write-Through automatically. This is still safe, but writes can be much slower. Write-Through: When using write-through caching, the controller sends a write-request completion signal only after the data is written to the disk. Force Write Back: When using force write-back caching, the write cache is enabled regardless of whether the controller has a battery. Disk Array If you have a large amount of disks and disk space required for the database a good optimal configuration would be. In the table below $PGDTA = /var/lib/pgsql/9.2 on a Centos 6 installation with Postgresql 9.2.x installed. Hint! Its recommended that if you don't have at least 2 disk dedicated to the pg_xlog that you allow it to reside on the same disk as the $PGDATA Location Disks RAID Purpose / (root) 2 1 Operating System $PGDATA 4+ 10 Database $PGDATA/pg_xlog 2 1 Write Ahead Log (WAL) Tablespace 1+ None Temporary files Looking at some of the access patterns of the main file systems. This is useful if you have less disk or want to create a larger single array/fewer arrays. Location Cache Flushes Access Patterns Operating System Rare Mix of Sequential and Random Database Regularly Mix of Sequential and Random Write Ahead Log (WAL) Constant Sequential Temporary Files Never More random as client count increases Note that the exact mix of sequential and random behavior for your database is completely application dependent. Any attempt to optimize disk layout that doesn't take into account the access pattern of your app, including concurrency is unlikely to predict performance correctly. Hardware RAID Parameters Additional performance tuning can be achieved by setting hardware raid card parameters such as Read-Ahead and Write-Back . Though from talks with users read-ahead/adaptive read-ahead set on the raid controller does not significantly contribute to performance improvement in comparison to the OS which will usually perform the read-ahead. Lastly Write-Back is only recommended with a Battery Back Raid Controller, as if there is no battery data loss may occur in the event of a power failure. Disk Layout There are a few guidelines that can help prune down the possible configuration. Avoid putting the WAL on the operating system drive, because they have completely different access patterns and both will suffer when combined. If you have evidence you don't do any large sorting, the temporary files can be kept at their default location, as part of the database disk. All things considered splitting the data up allows for easier measurement of data going to the database, WAL, and temporary disk.","title":"PostgreSQL Disk Array, Layout and RAID Tuning"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#postgresql-disk-array-layout-and-raid-tuning","text":"","title":"PostgreSQL Disk Array, Layout and RAID Tuning"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#overview","text":"This article covers basic recommendation for setting up and configuring Disk Arrays, RAID and Disk Layout for your Postgresql database server.","title":"Overview"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#definitions","text":"No-Read-Ahead: Selecting no-read-ahead policy indicates that the controller should not use read-ahead policy. Read-Ahead: When using read-ahead policy, the controller reads sequential sectors of the virtual disk when seeking data. Read-ahead policy may improve system performance if the data is actually written to sequential sectors of the virtual disk. Adaptive Read-Ahead: When using adaptive read-ahead policy, the controller initiates read-ahead only if the two most recent read requests accessed sequential sectors of the disk. If subsequent read requests access random sectors of the disk, the controller reverts to no-read-ahead policy. The controller continues to evaluate whether read requests are accessing sequential sectors of the disk, and can initiate read-ahead if necessary. Write-Back: When using write-back caching, the controller sends a write-request completion signal as soon as the data is in the controller cache but has not yet been written to disk. This is only recommended with a Battery Back Raid Controller. In that configuration, when the battery isn't there or isn't working, the controller will degrade to Write-Through automatically. This is still safe, but writes can be much slower. Write-Through: When using write-through caching, the controller sends a write-request completion signal only after the data is written to the disk. Force Write Back: When using force write-back caching, the write cache is enabled regardless of whether the controller has a battery.","title":"Definitions"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#disk-array","text":"If you have a large amount of disks and disk space required for the database a good optimal configuration would be. In the table below $PGDTA = /var/lib/pgsql/9.2 on a Centos 6 installation with Postgresql 9.2.x installed. Hint! Its recommended that if you don't have at least 2 disk dedicated to the pg_xlog that you allow it to reside on the same disk as the $PGDATA Location Disks RAID Purpose / (root) 2 1 Operating System $PGDATA 4+ 10 Database $PGDATA/pg_xlog 2 1 Write Ahead Log (WAL) Tablespace 1+ None Temporary files Looking at some of the access patterns of the main file systems. This is useful if you have less disk or want to create a larger single array/fewer arrays. Location Cache Flushes Access Patterns Operating System Rare Mix of Sequential and Random Database Regularly Mix of Sequential and Random Write Ahead Log (WAL) Constant Sequential Temporary Files Never More random as client count increases Note that the exact mix of sequential and random behavior for your database is completely application dependent. Any attempt to optimize disk layout that doesn't take into account the access pattern of your app, including concurrency is unlikely to predict performance correctly.","title":"Disk Array"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#hardware-raid-parameters","text":"Additional performance tuning can be achieved by setting hardware raid card parameters such as Read-Ahead and Write-Back . Though from talks with users read-ahead/adaptive read-ahead set on the raid controller does not significantly contribute to performance improvement in comparison to the OS which will usually perform the read-ahead. Lastly Write-Back is only recommended with a Battery Back Raid Controller, as if there is no battery data loss may occur in the event of a power failure.","title":"Hardware RAID Parameters"},{"location":"linux/postgresql/postgresql_disk_array,_layout_and_raid_tuning/#disk-layout","text":"There are a few guidelines that can help prune down the possible configuration. Avoid putting the WAL on the operating system drive, because they have completely different access patterns and both will suffer when combined. If you have evidence you don't do any large sorting, the temporary files can be kept at their default location, as part of the database disk. All things considered splitting the data up allows for easier measurement of data going to the database, WAL, and temporary disk.","title":"Disk Layout"},{"location":"linux/postgresql/postgresql_file_system_tuning/","text":"PostgreSQL File System Tuning Overview This article covers some basic recommended setting for files system tuning in Linux to get some additional performance when running a Postgresql Database. File System (EXT4) I most commonly use CentOS (Currently 6.x versoins) as my go to operating system, because of this I'm going to focus on EXT4. Though some other choices are XFS used for some very large databases and some considered less stable JFS,ReiserFS, BTRFS. Its important to note for Postgersql some bugs involving FSYNC handling were not fully corrected until kernel 2.6.31.8/2.6.32.1. Kernel 2.6.32 is the first version that includes an ext4 version that should be considered safe for production Postgresql databases. From the Postgresql perspective the main improvement over EXT3 is EXT4's better handling of write barriers and FSYNC operations. Read-Ahead Tuning This feature results in asking for blocks form the disk ahead of the application requesting them. You can check your current read-ahead using the blockdev command, either for a single drive or with a summary for all drives, RAID devices, and logical volumes. The default is 256 for regular drives, and may be larger for software RAID devices. The units here are normally 512 bytes, making the default value equal to 128KB of read-ahead. blockdev --getra /dev/sda blockdev --report The normal properly tuned range on modern hardware normally works out to be 4096 to 16384. You can make the change via the blockdev command. Unfortunatley, read-ahead needs to be set for each drive on your system. It's usually handled by putting a blockdev adjustement for each device in rc.local boot script. blockdev --setra 4096 /dev/sda If you run bonnie++ with a few read-ahead values, you should see the sequential read numbers increase as you tune upwards, eventually leveling off. On smaller disk arrays or single drives, the difference may not be large. Using this amount of read-ahead is necessary to reach full speed on a larger disk array. File Access Time Tuning Each time you access a file in Linux, a file attribute called the file's last access time (atime) is updated. You can disable this on the database volume by adding noatime to the volume mount options in /etc/fstab. Note that setting noatime disables both nodiratime and relatime levels of access time udpates. /dev/sda1 /pgsql ext4 noatime,errors = remount-ro 0 1 Write Barrier Tuning When PostgreSQL writes to disk, it executes one of the system fsync calls (fsync or fdatasync) to flush that data to disk. By default, the cache on hard disks and disk controllers are assumed to be volatile: when power fails, data written there will be lost. Since that can result in filesystem and/or database corruption when it happens, fsync in Linux issues a write barrier that forces all data onto physical disk. That will flush the write cache on both RAID and drive write caches. When the database hardware includes a battery-backed write that's setup correctly, this is not necessary. Writes to that cache are not volatile; they will be saved in the case of a power interruption. In that case, you can turn off barriers on that disk. This results in a significant boost in write performance. You can disable barriers on the database volume by adding nobarrier to the volume mount options in /etc/fstab. /dev/sda1 /pgsql ext4 noatime,nobarrier,errors = remount-ro 0 1 Read Caching and Swapping Tuning You can check the current value on your system by looking at /proc/sys/vm/swappiness and the easiest way to make a permanent change is to add a line to the /etc/sysctl.conf as shown below. A value of 0 perfers shrinking the filesystem cache rather then using then swap, which is recommended behavior for getting predictable database performance. cat /proc/sys/vm/swappiness vm.swappiness = 0 Linux's tendency to let processes allocate more RAM than the system has, in hopes not all of it will actually be used. This overcommit behavior should be disabled by making the change below to the sysctl configuration. vm.overcommit_memory = 2","title":"PostgreSQL File System Tuning"},{"location":"linux/postgresql/postgresql_file_system_tuning/#postgresql-file-system-tuning","text":"","title":"PostgreSQL File System Tuning"},{"location":"linux/postgresql/postgresql_file_system_tuning/#overview","text":"This article covers some basic recommended setting for files system tuning in Linux to get some additional performance when running a Postgresql Database.","title":"Overview"},{"location":"linux/postgresql/postgresql_file_system_tuning/#file-system-ext4","text":"I most commonly use CentOS (Currently 6.x versoins) as my go to operating system, because of this I'm going to focus on EXT4. Though some other choices are XFS used for some very large databases and some considered less stable JFS,ReiserFS, BTRFS. Its important to note for Postgersql some bugs involving FSYNC handling were not fully corrected until kernel 2.6.31.8/2.6.32.1. Kernel 2.6.32 is the first version that includes an ext4 version that should be considered safe for production Postgresql databases. From the Postgresql perspective the main improvement over EXT3 is EXT4's better handling of write barriers and FSYNC operations.","title":"File System (EXT4)"},{"location":"linux/postgresql/postgresql_file_system_tuning/#read-ahead-tuning","text":"This feature results in asking for blocks form the disk ahead of the application requesting them. You can check your current read-ahead using the blockdev command, either for a single drive or with a summary for all drives, RAID devices, and logical volumes. The default is 256 for regular drives, and may be larger for software RAID devices. The units here are normally 512 bytes, making the default value equal to 128KB of read-ahead. blockdev --getra /dev/sda blockdev --report The normal properly tuned range on modern hardware normally works out to be 4096 to 16384. You can make the change via the blockdev command. Unfortunatley, read-ahead needs to be set for each drive on your system. It's usually handled by putting a blockdev adjustement for each device in rc.local boot script. blockdev --setra 4096 /dev/sda If you run bonnie++ with a few read-ahead values, you should see the sequential read numbers increase as you tune upwards, eventually leveling off. On smaller disk arrays or single drives, the difference may not be large. Using this amount of read-ahead is necessary to reach full speed on a larger disk array.","title":"Read-Ahead Tuning"},{"location":"linux/postgresql/postgresql_file_system_tuning/#file-access-time-tuning","text":"Each time you access a file in Linux, a file attribute called the file's last access time (atime) is updated. You can disable this on the database volume by adding noatime to the volume mount options in /etc/fstab. Note that setting noatime disables both nodiratime and relatime levels of access time udpates. /dev/sda1 /pgsql ext4 noatime,errors = remount-ro 0 1","title":"File Access Time Tuning"},{"location":"linux/postgresql/postgresql_file_system_tuning/#write-barrier-tuning","text":"When PostgreSQL writes to disk, it executes one of the system fsync calls (fsync or fdatasync) to flush that data to disk. By default, the cache on hard disks and disk controllers are assumed to be volatile: when power fails, data written there will be lost. Since that can result in filesystem and/or database corruption when it happens, fsync in Linux issues a write barrier that forces all data onto physical disk. That will flush the write cache on both RAID and drive write caches. When the database hardware includes a battery-backed write that's setup correctly, this is not necessary. Writes to that cache are not volatile; they will be saved in the case of a power interruption. In that case, you can turn off barriers on that disk. This results in a significant boost in write performance. You can disable barriers on the database volume by adding nobarrier to the volume mount options in /etc/fstab. /dev/sda1 /pgsql ext4 noatime,nobarrier,errors = remount-ro 0 1","title":"Write Barrier Tuning"},{"location":"linux/postgresql/postgresql_file_system_tuning/#read-caching-and-swapping-tuning","text":"You can check the current value on your system by looking at /proc/sys/vm/swappiness and the easiest way to make a permanent change is to add a line to the /etc/sysctl.conf as shown below. A value of 0 perfers shrinking the filesystem cache rather then using then swap, which is recommended behavior for getting predictable database performance. cat /proc/sys/vm/swappiness vm.swappiness = 0 Linux's tendency to let processes allocate more RAM than the system has, in hopes not all of it will actually be used. This overcommit behavior should be disabled by making the change below to the sysctl configuration. vm.overcommit_memory = 2","title":"Read Caching and Swapping Tuning"},{"location":"linux/puppet/R10K/","text":"R10K What is R10K ? Description R10k provides a general purpose toolset for deploying Puppet environments and modules. It implements the Puppetfile format and provides a native implementation of Puppet dynamic environments . Synopsis Fundamentally, R10k installs a set of modules into a speicifed module directory and verifies their freshnes So why is it called R10k ? In true fasion the Puppet guys are terible at names. While Adrien aka finch was attempting to come up with a clever name, he recalled that Randall Munroe wrote a bot for controlling IRC chatter, and gave it the most generic name he could think of - Robot 9000. Since Adrien just needed a name, any name,he decided to go with an equally generic name by incrementing the robot index. Installation and Configuration The easiest way to manually install R10k is via ruby gems gem install r10k Configure r10k by editing /etc/r10k.yaml and ensuring it has the following contents: :cachedir: /var/cache/r10k :sources: puppet: basedir: /etc/puppet/environments prefix: false remote: https://github.com/repo.git :purgedirs: - /etc/puppet/environments Another common solution for installation is via the R10K Puppet module . This will install the R10k Ruby gem as well as configure the /etc/r10k.yaml file Example Installation: class { r10k : version = 1.4.1 , sources = { puppet = { remote = PATH_TO_REPO , basedir = ${::settings::confdir}/environments , prefix = false, }, }, purgedirs = [ ${::settings::confdir}/environments ], manage_modulepath = false, } Puppetfile Puppetfiles are a simple Ruby based DSL that specifies a list of modules to install, what version to install, and where to fetch them from. R10k uses a Puppetfile to install a set of Puppet modules for local development, or environment deployments to install additional modules into a given environment. Unlike librarian-puppet, the r10k implementation of Puppetfiles does not include dependency resolution, but it is on the roadmap. Global Settings The forge setting specifies which server that Forge based modules are fetched from. forge https://forge.puppetlabs.com The moduledir setting specifies where modules from the Puppetfile will be installed. This defaults to the modules directory relative to the Puppetfile. If the path is absolute then the modules will be installed to that absolute path, otherwise it's assumed that the moduledir setting should be relative and the modules will be installed in that directory relative to the Puppetfile. Common Patterns GIT mod puppetlabs-apache , :git = https://github.com/puppetlabs/puppetlabs-apache.git SVN mod apache , :svn = https://github.com/puppetlabs/puppetlabs-apache/trunk FORGE mod puppetlabs/apache Common Commands Install or update all modules in a given Puppetfile into ./modules) r10k puppetfile install Verify the Puppetfile syntax r10k puppetfile check Remove any modules in the 'modules' directory that are not specified in the Puppetfile: r10k puppetfile purge Dynamic Environments One of the most important functions of r10k is its ability to dynamically manage your Puppet environments. The core idea of dynamic environments is that you should be able to manage your Puppet modules in the same manner that you would manage any other code base. This builds on top of the GIT topic/feature branch model Whenever changes need to be made that need to be reviewed or tested before going live, they should be done in a different, short lived branch called a topic/feature branch. Work can be freely done on a topic branch in isolation and when the work is completed it is merged into a \"master\" or \"production\" branch. How it works R10k works by tracking the state of your Git repository or repositories. Each repository's branches will be cloned into a directory with a matching name, creating a Puppet environment for the branch. If a repository includes a Puppetfile such as a control repository . Forge modules and Git/SVN repositories described within will be cloned as well into the same directory. Subsequent changes to the branches will be kept in sync on the filesystem by future r10k runs. Finally, if there are directories that do not match existing branches, r10k will assume that the branches for those environments were delete and will remove those environments. r10k will need to be be able to authenticate with each repository. Most Git systems support authentication with SSH keys. GitHub calls them deploy keys. Bitbucket calls them deployment keys. Stash calls them SSH access keys. Workflow Adding New Modules This workflow is used when adding new modules via the forge or a VCS source such as git. These can be either public or internally-developed Create a topic/feature branch in the repository in which your Puppetfile resides. This is usually the control repository git checkout -b feature_branch Update the Puppetfile to include a section declaring the new module mod user-custom_facts , :git = git://github.com/user/custom_facts 1. Commit your changes and push the Puppetfile topic/feature branch upstream git commit -a -m \u2018Add module user/module to branch feature_branch git push origin feature_branch 1. Reference the new module in manifest, module and/or hiera If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip these step(s). 1. Create topic/feature branch for the manifest, module or hiera data that you are modifying. ``` git checkout -b feature_branch ``` Modify the exisiting manifest, module or hiera data and commit your changes and push the topic/feature branch upstream git commit -a -m \u2018Add feature reference to module\u2019 git push origin feature_branch 1. Deploy feature_branch Puppet Environment Can be a hook to automatically deploy environments r10k deploy environment -p 1. Testing New Module Branches If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip this step. There are many methods for testing a cheap/easy for testing is to specify the environment and noop flags puppet agent -t --environment feature_branch --noop Verify the changes are successful, match your expected changes and you are satisfied by the results Merge your changes for the repository in which your Puppetfile resides ( Control Repository ) into the master/production branch. git checkout production git merge feature_branch git push origin production Merge your changes for the manifest, module and/or hiera repositories referencing the new module. If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip this step. git checkout production git merge feature_branch git push origin production Cleanup topic/feature branches You may skip this step for long-lived branches, however most feature branches should be short-lived and can be pruned once testing and merging is complete. git branch -D feature_branch git push origin :feature_branch Editing Existing Modules This workflow is used when updating existing modules. Create topic/feature branch for the manifest, module or hiera data that you are modifying. git checkout -b feature_branch 1. Modify the exisiting manifest, module or hiera data, commit your changes and push the topic/feature branch upstream git commit -a -m \u2018Add feature reference to module\u2019 git push origin feature_branch Update the Puppetfile repository to to reference new branch of the updated module mod user-custom_facts , :git = git://github.com/user/custom_facts :ref = feature_branch 1. Commit your changes and push the Puppetfile topic/feature branch upstream git commit -a -m \u2018updating module user/module to branch feature_branch git push origin feature_branch Deploy feature_branch Puppet Environment r10k deploy environment -p 1. Test New Module Branches There are many methods for testing a cheap/easy for testing is to specify the environment and noop flags puppet agent -t --environment feature_branch --noop Verify the changes are successful, match your expected changes and you are satisfied by the results Merge your changes for the manifest, module and/or hiera repositories for the updated module(s). git checkout production git merge feature_branch git push origin production 1. Merge your changes for the repository in which your Puppetfile resides ( Control Repository ) into the master/production branch. git checkout production git merge feature_branch git push origin production 1. Cleanup topic/feature branches You may skip this step for long-lived branches, however most feature branches should be short-lived and can be pruned once testing and merging is complete. git branch -D feature_branch git push origin :feature_branch This is a vanilla workflow. You may need or desire to customize the workflow to fit the needs of your team, tools and methodolgy used. Post Receive Hooks SVN and GIT post-receive hooks can but used to help streamline your R10k workflow. Providing you with the ability to quickly and painlessly create Puppet environments to verify and test your code changes. The Puppet r10k module by Zack Smith details the integration support for MCollective and R10k in combination with a custom webhook that provides GitHub support. Another popular post-receive hook is reaktor . References Puppetlabs Github R10k Sh*t Gary Says: R10k + Directory Environments Puppetlabs Directory Environments terrarum: Puppet Infrastructure with r10k somthingsinisetal: Rethinking Puppet Deployment R10k Control Repos Reaktor: post-receive hook R10k Puppetfile R10k: Workflow Guide http://webcache.googleusercontent.com/search?q=cache:-FkbO79nhqEJ:www.geoffwilliams.me.uk/r10k_mco_publisher+ cd=6 hl=en ct=clnk gl=us client=safari","title":"R10K"},{"location":"linux/puppet/R10K/#r10k","text":"","title":"R10K"},{"location":"linux/puppet/R10K/#what-is-r10k","text":"","title":"What is R10K ?"},{"location":"linux/puppet/R10K/#description","text":"R10k provides a general purpose toolset for deploying Puppet environments and modules. It implements the Puppetfile format and provides a native implementation of Puppet dynamic environments .","title":"Description"},{"location":"linux/puppet/R10K/#synopsis","text":"Fundamentally, R10k installs a set of modules into a speicifed module directory and verifies their freshnes","title":"Synopsis"},{"location":"linux/puppet/R10K/#so-why-is-it-called-r10k","text":"In true fasion the Puppet guys are terible at names. While Adrien aka finch was attempting to come up with a clever name, he recalled that Randall Munroe wrote a bot for controlling IRC chatter, and gave it the most generic name he could think of - Robot 9000. Since Adrien just needed a name, any name,he decided to go with an equally generic name by incrementing the robot index.","title":"So why is it called R10k ?"},{"location":"linux/puppet/R10K/#installation-and-configuration","text":"The easiest way to manually install R10k is via ruby gems gem install r10k Configure r10k by editing /etc/r10k.yaml and ensuring it has the following contents: :cachedir: /var/cache/r10k :sources: puppet: basedir: /etc/puppet/environments prefix: false remote: https://github.com/repo.git :purgedirs: - /etc/puppet/environments Another common solution for installation is via the R10K Puppet module . This will install the R10k Ruby gem as well as configure the /etc/r10k.yaml file Example Installation: class { r10k : version = 1.4.1 , sources = { puppet = { remote = PATH_TO_REPO , basedir = ${::settings::confdir}/environments , prefix = false, }, }, purgedirs = [ ${::settings::confdir}/environments ], manage_modulepath = false, }","title":"Installation and Configuration"},{"location":"linux/puppet/R10K/#puppetfile","text":"Puppetfiles are a simple Ruby based DSL that specifies a list of modules to install, what version to install, and where to fetch them from. R10k uses a Puppetfile to install a set of Puppet modules for local development, or environment deployments to install additional modules into a given environment. Unlike librarian-puppet, the r10k implementation of Puppetfiles does not include dependency resolution, but it is on the roadmap.","title":"Puppetfile"},{"location":"linux/puppet/R10K/#global-settings","text":"The forge setting specifies which server that Forge based modules are fetched from. forge https://forge.puppetlabs.com The moduledir setting specifies where modules from the Puppetfile will be installed. This defaults to the modules directory relative to the Puppetfile. If the path is absolute then the modules will be installed to that absolute path, otherwise it's assumed that the moduledir setting should be relative and the modules will be installed in that directory relative to the Puppetfile.","title":"Global Settings"},{"location":"linux/puppet/R10K/#common-patterns","text":"GIT mod puppetlabs-apache , :git = https://github.com/puppetlabs/puppetlabs-apache.git SVN mod apache , :svn = https://github.com/puppetlabs/puppetlabs-apache/trunk FORGE mod puppetlabs/apache","title":"Common Patterns"},{"location":"linux/puppet/R10K/#common-commands","text":"Install or update all modules in a given Puppetfile into ./modules) r10k puppetfile install Verify the Puppetfile syntax r10k puppetfile check Remove any modules in the 'modules' directory that are not specified in the Puppetfile: r10k puppetfile purge","title":"Common Commands"},{"location":"linux/puppet/R10K/#dynamic-environments","text":"One of the most important functions of r10k is its ability to dynamically manage your Puppet environments. The core idea of dynamic environments is that you should be able to manage your Puppet modules in the same manner that you would manage any other code base. This builds on top of the GIT topic/feature branch model Whenever changes need to be made that need to be reviewed or tested before going live, they should be done in a different, short lived branch called a topic/feature branch. Work can be freely done on a topic branch in isolation and when the work is completed it is merged into a \"master\" or \"production\" branch.","title":"Dynamic Environments"},{"location":"linux/puppet/R10K/#how-it-works","text":"R10k works by tracking the state of your Git repository or repositories. Each repository's branches will be cloned into a directory with a matching name, creating a Puppet environment for the branch. If a repository includes a Puppetfile such as a control repository . Forge modules and Git/SVN repositories described within will be cloned as well into the same directory. Subsequent changes to the branches will be kept in sync on the filesystem by future r10k runs. Finally, if there are directories that do not match existing branches, r10k will assume that the branches for those environments were delete and will remove those environments. r10k will need to be be able to authenticate with each repository. Most Git systems support authentication with SSH keys. GitHub calls them deploy keys. Bitbucket calls them deployment keys. Stash calls them SSH access keys.","title":"How it works"},{"location":"linux/puppet/R10K/#workflow","text":"","title":"Workflow"},{"location":"linux/puppet/R10K/#adding-new-modules","text":"This workflow is used when adding new modules via the forge or a VCS source such as git. These can be either public or internally-developed Create a topic/feature branch in the repository in which your Puppetfile resides. This is usually the control repository git checkout -b feature_branch Update the Puppetfile to include a section declaring the new module mod user-custom_facts , :git = git://github.com/user/custom_facts 1. Commit your changes and push the Puppetfile topic/feature branch upstream git commit -a -m \u2018Add module user/module to branch feature_branch git push origin feature_branch 1. Reference the new module in manifest, module and/or hiera If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip these step(s). 1. Create topic/feature branch for the manifest, module or hiera data that you are modifying. ``` git checkout -b feature_branch ``` Modify the exisiting manifest, module or hiera data and commit your changes and push the topic/feature branch upstream git commit -a -m \u2018Add feature reference to module\u2019 git push origin feature_branch 1. Deploy feature_branch Puppet Environment Can be a hook to automatically deploy environments r10k deploy environment -p 1. Testing New Module Branches If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip this step. There are many methods for testing a cheap/easy for testing is to specify the environment and noop flags puppet agent -t --environment feature_branch --noop Verify the changes are successful, match your expected changes and you are satisfied by the results Merge your changes for the repository in which your Puppetfile resides ( Control Repository ) into the master/production branch. git checkout production git merge feature_branch git push origin production Merge your changes for the manifest, module and/or hiera repositories referencing the new module. If you are simply adding the module at this time and not referencing it in other modules or manifests, you may skip this step. git checkout production git merge feature_branch git push origin production Cleanup topic/feature branches You may skip this step for long-lived branches, however most feature branches should be short-lived and can be pruned once testing and merging is complete. git branch -D feature_branch git push origin :feature_branch","title":"Adding New Modules"},{"location":"linux/puppet/R10K/#editing-existing-modules","text":"This workflow is used when updating existing modules. Create topic/feature branch for the manifest, module or hiera data that you are modifying. git checkout -b feature_branch 1. Modify the exisiting manifest, module or hiera data, commit your changes and push the topic/feature branch upstream git commit -a -m \u2018Add feature reference to module\u2019 git push origin feature_branch Update the Puppetfile repository to to reference new branch of the updated module mod user-custom_facts , :git = git://github.com/user/custom_facts :ref = feature_branch 1. Commit your changes and push the Puppetfile topic/feature branch upstream git commit -a -m \u2018updating module user/module to branch feature_branch git push origin feature_branch Deploy feature_branch Puppet Environment r10k deploy environment -p 1. Test New Module Branches There are many methods for testing a cheap/easy for testing is to specify the environment and noop flags puppet agent -t --environment feature_branch --noop Verify the changes are successful, match your expected changes and you are satisfied by the results Merge your changes for the manifest, module and/or hiera repositories for the updated module(s). git checkout production git merge feature_branch git push origin production 1. Merge your changes for the repository in which your Puppetfile resides ( Control Repository ) into the master/production branch. git checkout production git merge feature_branch git push origin production 1. Cleanup topic/feature branches You may skip this step for long-lived branches, however most feature branches should be short-lived and can be pruned once testing and merging is complete. git branch -D feature_branch git push origin :feature_branch This is a vanilla workflow. You may need or desire to customize the workflow to fit the needs of your team, tools and methodolgy used.","title":"Editing Existing Modules"},{"location":"linux/puppet/R10K/#post-receive-hooks","text":"SVN and GIT post-receive hooks can but used to help streamline your R10k workflow. Providing you with the ability to quickly and painlessly create Puppet environments to verify and test your code changes. The Puppet r10k module by Zack Smith details the integration support for MCollective and R10k in combination with a custom webhook that provides GitHub support. Another popular post-receive hook is reaktor .","title":"Post Receive Hooks"},{"location":"linux/puppet/R10K/#references","text":"Puppetlabs Github R10k Sh*t Gary Says: R10k + Directory Environments Puppetlabs Directory Environments terrarum: Puppet Infrastructure with r10k somthingsinisetal: Rethinking Puppet Deployment R10k Control Repos Reaktor: post-receive hook R10k Puppetfile R10k: Workflow Guide http://webcache.googleusercontent.com/search?q=cache:-FkbO79nhqEJ:www.geoffwilliams.me.uk/r10k_mco_publisher+ cd=6 hl=en ct=clnk gl=us client=safari","title":"References"},{"location":"linux/puppet/default_template_header/","text":"Default Template Header Overview One posibility for creating a 'default' header to be added to all templates. Custom Facter Fact Here is an example for a custom fact that just sets text for the header #!/usr/bin/ruby require facter text = ###################################################################################### # This file is managed by Pupppet, any manual changes will be OVERWRITTEN in 30min # ###################################################################################### Facter . add ( template_header ) do setcode do text end end Usage To use this you only need to add the following line to the beging of your Puppet .erb templates %= scope [ ::template_header ] -% Pro Tip You can easily add the above ruby code to all files with the following sed sed -i 1s/^/ %= scope[ ::template_header ] -% \\n/ *","title":"Default Template Header"},{"location":"linux/puppet/default_template_header/#default-template-header","text":"","title":"Default Template Header"},{"location":"linux/puppet/default_template_header/#overview","text":"One posibility for creating a 'default' header to be added to all templates.","title":"Overview"},{"location":"linux/puppet/default_template_header/#custom-facter-fact","text":"Here is an example for a custom fact that just sets text for the header #!/usr/bin/ruby require facter text = ###################################################################################### # This file is managed by Pupppet, any manual changes will be OVERWRITTEN in 30min # ###################################################################################### Facter . add ( template_header ) do setcode do text end end","title":"Custom Facter Fact"},{"location":"linux/puppet/default_template_header/#usage","text":"To use this you only need to add the following line to the beging of your Puppet .erb templates %= scope [ ::template_header ] -%","title":"Usage"},{"location":"linux/puppet/default_template_header/#pro-tip","text":"You can easily add the above ruby code to all files with the following sed sed -i 1s/^/ %= scope[ ::template_header ] -% \\n/ *","title":"Pro Tip"},{"location":"linux/puppet/hiera/","text":"Overview What is Hiera ? Hiera is a ordered hierarchical key/value data store/lookup tool for configuration data Hierarchical Data Store : an organizational structure in which items are ranked according to levels of importance Why Hiera ? Hiera makes Puppet better by keeping site-specific data out of your manifests Benefits: Easier to configure your own nodes: default data with multiple levels of overrides is finally easy. Easier to re-use public Puppet modules: don\u2019t edit the code, just put the necessary data in Hiera. Easier to publish your own modules for collaboration: no need to worry about cleaning out your data before showing it around, and no more clashing variable names. What goes into Hiera ? Business-specific data (i.e. internal NTP server, VIP address, per-environment java application versions, etc\u2026) Data that you don\u2019t want to include in your component modules What DOSN'T go into Hiera ? OS-specific data Data common to the component modules everyone needs to know (paths to config files, package names, etc\u2026) Sensitive data ( Password, Keys ) - Unless your using eyaml/eyaml-gpg Hierarchical Structure The hierarchial structure is NOT definded and can be configured to meet your specific needs. Thus hiera does not have any requirements for the hierarchial structure. An example strucuture uses nodes , locations , secruity zones( seczones ), environment tiers( envtiers ) and common as the structure. This sample structure provides the granular ability to set/configure site-specific data at multiple layers Nodes(FQDN) : Node specific data Locations : Data specific to a site or location. Such as different data centers Security Zones(seczones) : Data specific to security zones, such as dmz vs core ( protected ). Environment Teirs : Data specific to environment tiers such as production, stage, dev, qa...etc. Common : \"Global\" data that is common among ALL nodes. This is used as a last match catagory. Creating Hierarchy The hierarchical structure contains two types of elements its data sources : Static data source \u2014 A hierarchy element without any interpolation tokens . A static data source will be the same for every node. Dynamic data source \u2014 A hierarchy element with at least one interpolation tokens . If two nodes have different values for the variables it references, a dynamic data source will use two different data sources for those nodes. Hiera\u2019s variables can come from a variety of sources, depending on how Hiera is invoked. When used in combination with Puppet, Hiera automatically recieves all of Puppet\u2019s current variables This includes facts and built-in variables , as well as local variables from the current scope. Most users will almost exclusively interpolate facts and built-in variables in their Hiera configuration and data. To ultilize facts and built-in variables, remove Puppet\u2019s $ (dollar sign) prefix when using its variables in Hiera. (That is, a variable called $::clientcert in Puppet is called ::clientcert in Hiera.) Best Practices Usually avoid referencing user-set local variables from Hiera. Instead, use facts, built-in variables, top-scope variables, node-scope variables, or variables from an ENC whenever possible. When possible, reference variables by their fully-qualified names (e.g. %{::environment} and %{::clientcert}) to make sure their values are not masked by local scopes. These two guidelines will make Hiera more predictable, and can help protect you from accidentally mingling data and code in your Puppet manifests. Configuration Hiera\u2019s config file is usually referred to as hiera.yaml. Use this file to configure the hierarchy, which backend(s) to use, and settings for each backend. Used in combination with Puppet the default config file location is $confdir/hiera.yaml, which is usually one of the following: /etc/puppet/hiera.yaml in *nix open source Puppet /etc/puppetlabs/puppet/hiera.yaml in *nix Puppet Enterprise COMMON_APPDATA\\PuppetLabs\\puppet\\etc\\hiera.yaml on Windows Format Hiera\u2019s config file must be a YAML hash. The file must be valid YAML, but may contain no data. Each top-level key in the hash must be a Ruby symbol with a colon (:) prefix. Available settings are listed at \u201c Global Settings \u201d and \u201c Backend-Specific Settings \u201d. --- :backends: - yaml :logger: console :hierarchy: - nodes/%{::clientcert} - locations/%{::location} - seczones/%{::seczone} - envtiers/%{::envtier} - common Data Source(s) Within a data source, you can interpolate values into any string, whether it\u2019s a standalone value or part of a hash or array value. This can be useful for values that should be different for every node, but which differ predictably : # /var/lib/hiera/common.yaml --- ntpserver: ntp.%{::domain} As well as basic values data source can contain hashes of data that can be looked up via the hiera_hash function Hiera_Hash Lookup Function $myvhosts = hiera_hash( apache::vhosts , {}) create_resources( apache::vhost , $myvhosts) Data Source Hash apache::vhosts: %{::fqdn} : port: 80 docroot: /var/www/html priority: 25 default_vhost: true directories: - path: /var/www/html allow: - from 127.0.0.1 - from ::1 options: - Indexes - FollowSymLinks - MultiViews %{::fqdn}-ssl : port: 443 docroot: /var/www/html priority: 25 ssl: true ssl_cert: /etc/pki/tls/certs/%{::fqdn}-server.crt ssl_key: /etc/pki/tls/private/%{::fqdn}-server.key ssl_ca: /etc/pki/tls/certs/ca.crt ssl_verify_client: require ssl_verify_depth: 3 ssl_proxyengine: true proxy_pass: - path: / url: https://foreman.%{::domain}/ Lookups Hiera always takes a lookup key and returns a single value (of some simple or complex data type), but it has several methods for extracting/assembling that one value from the hierarchy. We refer to these as \u201clookup methods.\u201d Hiera uses the priority lookup method as its default, which gets a value from the most specific ( a.k.a first match wins ) matching level of the hierarchy. Only one hierarchy level \u2014 the first one to match \u2014 is consulted Priority lookups can retrieve values of any data type (strings, arrays, hashes), but the entire value will come from only one hierarchy level. Consult the Puppetlabs Hiera Documentation for details on additonal lookup methods. When to use hiera, hiera_array and hiera_hash lookup funcitons ? The short answer is to use hiera whenever you expect to need a single lookup from a single part of your hierachy. If you need to merge value across the matching parts of your hierarchy, then you'll need hiera_hash or hiera_array hiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes). hiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results. hiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning. Dynamic Environments On each puppet run the $confdir/hiera.yaml is consulted, thus any configurations must be in this file. Because of this you can NOT have hierarchies per environment. The $datadir is an environment aware namespace that is filled at run time, it can consult a specific environments datadir $confdir/environments/$environment/hieradata/ To configure Hiera for dynamic environments configure the `datadir property :datadir: /etc/puppet/environments/%{::environment}/hieradata Terminology References PuppetLabs Hiera 1 Overview Hiera Lookup Types The hiera.yaml Config File Hiera Data Sources When to use hiera, hiera_array, and hiera_hash?","title":"Hiera"},{"location":"linux/puppet/hiera/#overview","text":"","title":"Overview"},{"location":"linux/puppet/hiera/#what-is-hiera","text":"Hiera is a ordered hierarchical key/value data store/lookup tool for configuration data Hierarchical Data Store : an organizational structure in which items are ranked according to levels of importance","title":"What is Hiera ?"},{"location":"linux/puppet/hiera/#why-hiera","text":"Hiera makes Puppet better by keeping site-specific data out of your manifests Benefits: Easier to configure your own nodes: default data with multiple levels of overrides is finally easy. Easier to re-use public Puppet modules: don\u2019t edit the code, just put the necessary data in Hiera. Easier to publish your own modules for collaboration: no need to worry about cleaning out your data before showing it around, and no more clashing variable names.","title":"Why Hiera ?"},{"location":"linux/puppet/hiera/#what-goes-into-hiera","text":"Business-specific data (i.e. internal NTP server, VIP address, per-environment java application versions, etc\u2026) Data that you don\u2019t want to include in your component modules","title":"What goes into Hiera ?"},{"location":"linux/puppet/hiera/#what-dosnt-go-into-hiera","text":"OS-specific data Data common to the component modules everyone needs to know (paths to config files, package names, etc\u2026) Sensitive data ( Password, Keys ) - Unless your using eyaml/eyaml-gpg","title":"What DOSN'T go into Hiera ?"},{"location":"linux/puppet/hiera/#hierarchical-structure","text":"The hierarchial structure is NOT definded and can be configured to meet your specific needs. Thus hiera does not have any requirements for the hierarchial structure. An example strucuture uses nodes , locations , secruity zones( seczones ), environment tiers( envtiers ) and common as the structure. This sample structure provides the granular ability to set/configure site-specific data at multiple layers Nodes(FQDN) : Node specific data Locations : Data specific to a site or location. Such as different data centers Security Zones(seczones) : Data specific to security zones, such as dmz vs core ( protected ). Environment Teirs : Data specific to environment tiers such as production, stage, dev, qa...etc. Common : \"Global\" data that is common among ALL nodes. This is used as a last match catagory.","title":"Hierarchical Structure"},{"location":"linux/puppet/hiera/#creating-hierarchy","text":"The hierarchical structure contains two types of elements its data sources : Static data source \u2014 A hierarchy element without any interpolation tokens . A static data source will be the same for every node. Dynamic data source \u2014 A hierarchy element with at least one interpolation tokens . If two nodes have different values for the variables it references, a dynamic data source will use two different data sources for those nodes. Hiera\u2019s variables can come from a variety of sources, depending on how Hiera is invoked. When used in combination with Puppet, Hiera automatically recieves all of Puppet\u2019s current variables This includes facts and built-in variables , as well as local variables from the current scope. Most users will almost exclusively interpolate facts and built-in variables in their Hiera configuration and data. To ultilize facts and built-in variables, remove Puppet\u2019s $ (dollar sign) prefix when using its variables in Hiera. (That is, a variable called $::clientcert in Puppet is called ::clientcert in Hiera.)","title":"Creating Hierarchy"},{"location":"linux/puppet/hiera/#best-practices","text":"Usually avoid referencing user-set local variables from Hiera. Instead, use facts, built-in variables, top-scope variables, node-scope variables, or variables from an ENC whenever possible. When possible, reference variables by their fully-qualified names (e.g. %{::environment} and %{::clientcert}) to make sure their values are not masked by local scopes. These two guidelines will make Hiera more predictable, and can help protect you from accidentally mingling data and code in your Puppet manifests.","title":"Best Practices"},{"location":"linux/puppet/hiera/#configuration","text":"Hiera\u2019s config file is usually referred to as hiera.yaml. Use this file to configure the hierarchy, which backend(s) to use, and settings for each backend. Used in combination with Puppet the default config file location is $confdir/hiera.yaml, which is usually one of the following: /etc/puppet/hiera.yaml in *nix open source Puppet /etc/puppetlabs/puppet/hiera.yaml in *nix Puppet Enterprise COMMON_APPDATA\\PuppetLabs\\puppet\\etc\\hiera.yaml on Windows","title":"Configuration"},{"location":"linux/puppet/hiera/#format","text":"Hiera\u2019s config file must be a YAML hash. The file must be valid YAML, but may contain no data. Each top-level key in the hash must be a Ruby symbol with a colon (:) prefix. Available settings are listed at \u201c Global Settings \u201d and \u201c Backend-Specific Settings \u201d. --- :backends: - yaml :logger: console :hierarchy: - nodes/%{::clientcert} - locations/%{::location} - seczones/%{::seczone} - envtiers/%{::envtier} - common","title":"Format"},{"location":"linux/puppet/hiera/#data-sources","text":"Within a data source, you can interpolate values into any string, whether it\u2019s a standalone value or part of a hash or array value. This can be useful for values that should be different for every node, but which differ predictably : # /var/lib/hiera/common.yaml --- ntpserver: ntp.%{::domain} As well as basic values data source can contain hashes of data that can be looked up via the hiera_hash function Hiera_Hash Lookup Function $myvhosts = hiera_hash( apache::vhosts , {}) create_resources( apache::vhost , $myvhosts) Data Source Hash apache::vhosts: %{::fqdn} : port: 80 docroot: /var/www/html priority: 25 default_vhost: true directories: - path: /var/www/html allow: - from 127.0.0.1 - from ::1 options: - Indexes - FollowSymLinks - MultiViews %{::fqdn}-ssl : port: 443 docroot: /var/www/html priority: 25 ssl: true ssl_cert: /etc/pki/tls/certs/%{::fqdn}-server.crt ssl_key: /etc/pki/tls/private/%{::fqdn}-server.key ssl_ca: /etc/pki/tls/certs/ca.crt ssl_verify_client: require ssl_verify_depth: 3 ssl_proxyengine: true proxy_pass: - path: / url: https://foreman.%{::domain}/","title":"Data Source(s)"},{"location":"linux/puppet/hiera/#lookups","text":"Hiera always takes a lookup key and returns a single value (of some simple or complex data type), but it has several methods for extracting/assembling that one value from the hierarchy. We refer to these as \u201clookup methods.\u201d Hiera uses the priority lookup method as its default, which gets a value from the most specific ( a.k.a first match wins ) matching level of the hierarchy. Only one hierarchy level \u2014 the first one to match \u2014 is consulted Priority lookups can retrieve values of any data type (strings, arrays, hashes), but the entire value will come from only one hierarchy level. Consult the Puppetlabs Hiera Documentation for details on additonal lookup methods.","title":"Lookups"},{"location":"linux/puppet/hiera/#when-to-use-hiera-hiera95array-and-hiera95hash-lookup-funcitons","text":"The short answer is to use hiera whenever you expect to need a single lookup from a single part of your hierachy. If you need to merge value across the matching parts of your hierarchy, then you'll need hiera_hash or hiera_array hiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes). hiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results. hiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning.","title":"When to use hiera, hiera_array and hiera_hash lookup funcitons ?"},{"location":"linux/puppet/hiera/#dynamic-environments","text":"On each puppet run the $confdir/hiera.yaml is consulted, thus any configurations must be in this file. Because of this you can NOT have hierarchies per environment. The $datadir is an environment aware namespace that is filled at run time, it can consult a specific environments datadir $confdir/environments/$environment/hieradata/ To configure Hiera for dynamic environments configure the `datadir property :datadir: /etc/puppet/environments/%{::environment}/hieradata","title":"Dynamic Environments"},{"location":"linux/puppet/hiera/#terminology","text":"","title":"Terminology"},{"location":"linux/puppet/hiera/#references","text":"PuppetLabs Hiera 1 Overview Hiera Lookup Types The hiera.yaml Config File Hiera Data Sources When to use hiera, hiera_array, and hiera_hash?","title":"References"},{"location":"linux/puppet/nav/","text":"Puppet Articles Default Template Header Roles and Profiles Pattern (Methodology) Roles and Profiles - Role Guidelines Roles and Profiles - Profile Guidelines Node-Puppet-Foreman Communication Hiera - Hierarchical Structure (Puppet) Environments, (Hiera) Environments, (Application/Network) Environments OH MY! R10k","title":"Puppet"},{"location":"linux/puppet/nav/#puppet-articles","text":"Default Template Header Roles and Profiles Pattern (Methodology) Roles and Profiles - Role Guidelines Roles and Profiles - Profile Guidelines Node-Puppet-Foreman Communication Hiera - Hierarchical Structure (Puppet) Environments, (Hiera) Environments, (Application/Network) Environments OH MY! R10k","title":"Puppet Articles"},{"location":"linux/puppet/node_puppet_foreman_communication/","text":"Overview Details the communcation path between Nodes, Puppet and Foreman for both a Puppet run and Foreman provisioning. The architecutre resides over multiple security zones PFS(External), DMZ and CORE. These security zones are like the layers of an oninion and access inbound from one security zone to another is restricted by a set of firewalls. As a rule inbound traffic can only pass to the adjacent security zone and can not 'hop' security zones. An example is that traffic from the PFS security zone can not communicate directly with the CORE secuirty zone. To allow PuppetServers(Masters) in the PFS and DMZ security zones to request a nodes ENC output as well as submit facts and reports back to the Foreman server located in the CORE security zone a Apache SSL reverse proxy is used. This provides a better security model by limiting the access directly from the PFS sercurity zone and providing a single point of entry from the DMZ security zone into the CORE sercurity zone. Secure Communication To provide a secure communication path from the PuppetServer/Foreman Smart-Proxy back to the Foreman Server, the Apache reverse SSL proxy is configured with both client and server SSL communication. The architechture requires all certificates used for communicaiton by the Foreman Server, Apache reverse SSL proxy and Foreman Smart-Proxies are all signed by the same certificate authority (CA) PuppetServers/Foreman Smart-Proxies that are connecting to the Apache reverse SSL proxy are required to have PKI client certificates that is signed by the same certificate authority (CA) as the Apache reverse proxy PKI server certificate. Foreman Server inturn requires that the Apache reverse SSL proxy provide a client certificate that is signed by the same certificate authority (CA) as the certificate Foreman is configured to communicate with its Foreman Smart-Proxies. Puppet Run Communication Flow Core Security Zone Node send facts to PuppetServer PuppetServer obtains ENC data from Foreman PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts to Foreman DMZ Secruity Zone Node send facts to PuppetServer PuppetServer requests ENC data though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts though Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report information and facts to Foreman in the CORE security zone PFS Security Zone Node send facts to PuppetServer PuppetServer requests ENC data though Apache reverse proxy in the DMZ security zone Apache reverse proxy in the PFS security zone forwards request to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts though the Apache reverse proxy in the PFS security zone Apache reverse proxy in the PFS security zone forwards report information and facts to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report information and facts to Foreman in the CORE security zone Foreman Provisioning Communication Flow Core Security Zone Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file from the Foreman server Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built DMZ Security Zone Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report to Foreman in the CORE security zone PFS Security Zone Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file though the Apache reverse proxy in the PFS security zone Apache reverse proxy in the PFS security zone forwards request to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the PFS security zone forwards report to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report to Foreman in the CORE security zone Multi-Site CORE Security Zone, Puppet Run Communication Flow It is common to have a setup with multiple sites that have both a DMZ and a CORE security zone. In this senario to abide with the security policy of not 'hopping' security zones and only passing traffic to the adjacent security zone. The Puppet server in the 'passive' site CORE security zone needs to utilize the Apache reverse SSL Proxy to foward its report information and facts to the Foreman server in the 'active' site CORE security zone. Local communication between the Puppet server and its local nodes still behaves has described above","title":"Node puppet foreman communication"},{"location":"linux/puppet/node_puppet_foreman_communication/#overview","text":"Details the communcation path between Nodes, Puppet and Foreman for both a Puppet run and Foreman provisioning. The architecutre resides over multiple security zones PFS(External), DMZ and CORE. These security zones are like the layers of an oninion and access inbound from one security zone to another is restricted by a set of firewalls. As a rule inbound traffic can only pass to the adjacent security zone and can not 'hop' security zones. An example is that traffic from the PFS security zone can not communicate directly with the CORE secuirty zone. To allow PuppetServers(Masters) in the PFS and DMZ security zones to request a nodes ENC output as well as submit facts and reports back to the Foreman server located in the CORE security zone a Apache SSL reverse proxy is used. This provides a better security model by limiting the access directly from the PFS sercurity zone and providing a single point of entry from the DMZ security zone into the CORE sercurity zone.","title":"Overview"},{"location":"linux/puppet/node_puppet_foreman_communication/#secure-communication","text":"To provide a secure communication path from the PuppetServer/Foreman Smart-Proxy back to the Foreman Server, the Apache reverse SSL proxy is configured with both client and server SSL communication. The architechture requires all certificates used for communicaiton by the Foreman Server, Apache reverse SSL proxy and Foreman Smart-Proxies are all signed by the same certificate authority (CA) PuppetServers/Foreman Smart-Proxies that are connecting to the Apache reverse SSL proxy are required to have PKI client certificates that is signed by the same certificate authority (CA) as the Apache reverse proxy PKI server certificate. Foreman Server inturn requires that the Apache reverse SSL proxy provide a client certificate that is signed by the same certificate authority (CA) as the certificate Foreman is configured to communicate with its Foreman Smart-Proxies.","title":"Secure Communication"},{"location":"linux/puppet/node_puppet_foreman_communication/#puppet-run-communication-flow","text":"","title":"Puppet Run Communication Flow"},{"location":"linux/puppet/node_puppet_foreman_communication/#core-security-zone","text":"Node send facts to PuppetServer PuppetServer obtains ENC data from Foreman PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts to Foreman","title":"Core Security Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#dmz-secruity-zone","text":"Node send facts to PuppetServer PuppetServer requests ENC data though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts though Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report information and facts to Foreman in the CORE security zone","title":"DMZ Secruity Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#pfs-security-zone","text":"Node send facts to PuppetServer PuppetServer requests ENC data though Apache reverse proxy in the DMZ security zone Apache reverse proxy in the PFS security zone forwards request to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone PuppetServer sends compiled catalog to Node Node sends report information back to PuppetServer PuppetServer sends report information and facts though the Apache reverse proxy in the PFS security zone Apache reverse proxy in the PFS security zone forwards report information and facts to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report information and facts to Foreman in the CORE security zone","title":"PFS Security Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#foreman-provisioning-communication-flow","text":"","title":"Foreman Provisioning Communication Flow"},{"location":"linux/puppet/node_puppet_foreman_communication/#core-security-zone_1","text":"Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file from the Foreman server Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built","title":"Core Security Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#dmz-security-zone","text":"Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report to Foreman in the CORE security zone","title":"DMZ Security Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#pfs-security-zone_1","text":"Node PXE client broadcasts DHCPDISCOVER DHCP server send DHCPOFFER including IP address and PXE file location Node PXE client obtains the boot image from the TFTP server Boot image searches the pxelinux.cfg directory on the TFTP server for boot configuration file Node obtains the provisioning (kickstart) file though the Apache reverse proxy in the PFS security zone Apache reverse proxy in the PFS security zone forwards request to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards request to Foreman in the CORE security zone Kickstart installs from a remote installation tree on the Repository Mirror server Kickstart executes a Puppet run on the Node Node reports to the Foreman server it was built though the Apache reverse proxy in the DMZ security zone Apache reverse proxy in the PFS security zone forwards report to Apache reverse proxy in the DMZ security zone Apache reverse proxy in the DMZ security zone forwards report to Foreman in the CORE security zone","title":"PFS Security Zone"},{"location":"linux/puppet/node_puppet_foreman_communication/#multi-site-core-security-zone-puppet-run-communication-flow","text":"It is common to have a setup with multiple sites that have both a DMZ and a CORE security zone. In this senario to abide with the security policy of not 'hopping' security zones and only passing traffic to the adjacent security zone. The Puppet server in the 'passive' site CORE security zone needs to utilize the Apache reverse SSL Proxy to foward its report information and facts to the Foreman server in the 'active' site CORE security zone. Local communication between the Puppet server and its local nodes still behaves has described above","title":"Multi-Site CORE Security Zone, Puppet Run Communication Flow"},{"location":"linux/puppet/profile_guidelines/","text":"Overview What is a profile ? A profile is a collection (wrapper) of one or more component modules and/or resources along with any required logic or specific parameter values that wrap a application techonolgy stack. Breakdown Technology specific May include resources directly May make calls to Hiera for required data (parameters) These calls may be transparent via automatic parameter lookup Include Component module classes/resources Named according to the technology they manage Do NOT include environments in profile names Guidelines Naming Profile naming needs to adhear to the following conventions: Named according to the technology that they manage Do NOT include a - (hypen), may include an _ (underscore) Do NOT include the puppet environment or environment tier (envtier) (prod, stage, test, dev, si ....) Documentation # == Overview: # # == Requirements: # # == Monitoring: # # == Notes: Inline comments are encouraged where sane, to provide clarity Each 'top level ' profile manifest should be properly documented detailing the following information Overview : one-maybe-two sentence summary of what the profile does/problem it solves Requirements : Anything extra your manifest requires before being applied (bootstraped) Monitoring : Service Checks that are to be configured on the monitoring server Notes : Any additoinal information that provides clarity/suppliments the overview. Each additional 'child/sub ' class or define should include the following information Overview : one-maybe-two sentence summary of what the class does/problem it solves Hierarchy Each profile should attempt to adhere to the following hierarchy model that follows a branching pattern for any 'child/sub ' classes or defines. profile \u251c\u2500\u2500manifests | \u2514\u2500\u2500applicationx.pp | \u2514\u2500\u2500applicationx | \u2514\u2500\u2500sub_class1.pp | \u2514\u2500\u2500sub_class.pp | \u2514\u2500\u2500monitoring | \u2514\u2500\u2500base.pp | \u2514\u2500\u2500define1.pp | \u2514\u2500\u2500define2.pp \u251c\u2500\u2500templates | \u2514\u2500\u2500applicationx | | \u2514\u2500\u2500applicationx_file.ini.erb | \u2514\u2500\u2500icinga2 | \u2514\u2500\u2500checkcommands | | \u2514\u2500\u2500check_applicationx.py.erb | \u2514\u2500\u2500checkplugins | \u2514\u2500\u2500check_applicationx.py.erb \u2514\u2500\u2500files \u2514\u2500\u2500applciationx | \u2514\u2500\u2500applicationx_file.ini \u2514\u2500\u2500icinga2 \u2514\u2500\u2500checkcommands | \u2514\u2500\u2500check_applicationx.py.erb \u2514\u2500\u2500checkplugins \u2514\u2500\u2500check_applicationx.py.erb Manifests 'Top level' manifests will be included in a role class to be applied to a node or nodes Store 'child/sub' classes or defines within a sub-directory with the same name as the 'top level ' manifest. Use 'child/sub' classes or defines with logic for a specific identifier such as site, security zone, etc... This helps to limit the size and complexity of monolithic manifests. Reference these in the 'top level' manifest. ## Include Required Sub-Class if ($::domain == cha.arin.net ) or ($::domain == core.cha.arin.net ) { include profile::applicationx::cha } elsif ($::domain == ash.arin.net ) or ($::domain == core.ash.arin.net ) { include profile::applicationx::ash } else { notify { ApplicationX Sub-Classes Not Loaded : withpath = true, } } Store 'child/sub' classes or defines related to monitoring within a sub-directory named monitoring under the application sub-directory. Define monitoring checksin a class named base profile::applicationx::monitoring::base . Reference puppet defines in the base monitoring class. profile::applicationx::monitoring::define1 = { $applicationx_server_names: } profile::applicationx::monitoring::define2 = { $applicationx_server_names: } Files/Templates Store required files or templates referenced within a profile child/sub' class or define (excluding monitoring) in a sub-directory with the same name as the 'top level' manifest. Store required files or templates referenced within a profile child/sub' class or define in a sub-directory with the same name of the monitoring software such as Icinga2. Store catagory specific (checkcommands, checkplugins) monitoring files or templates in a sub-directory with the catagory name (checkcommands, checkplugins) under the templates/icinga2 directory","title":"Profile guidelines"},{"location":"linux/puppet/profile_guidelines/#overview","text":"What is a profile ? A profile is a collection (wrapper) of one or more component modules and/or resources along with any required logic or specific parameter values that wrap a application techonolgy stack. Breakdown Technology specific May include resources directly May make calls to Hiera for required data (parameters) These calls may be transparent via automatic parameter lookup Include Component module classes/resources Named according to the technology they manage Do NOT include environments in profile names","title":"Overview"},{"location":"linux/puppet/profile_guidelines/#guidelines","text":"","title":"Guidelines"},{"location":"linux/puppet/profile_guidelines/#naming","text":"Profile naming needs to adhear to the following conventions: Named according to the technology that they manage Do NOT include a - (hypen), may include an _ (underscore) Do NOT include the puppet environment or environment tier (envtier) (prod, stage, test, dev, si ....)","title":"Naming"},{"location":"linux/puppet/profile_guidelines/#documentation","text":"# == Overview: # # == Requirements: # # == Monitoring: # # == Notes: Inline comments are encouraged where sane, to provide clarity Each 'top level ' profile manifest should be properly documented detailing the following information Overview : one-maybe-two sentence summary of what the profile does/problem it solves Requirements : Anything extra your manifest requires before being applied (bootstraped) Monitoring : Service Checks that are to be configured on the monitoring server Notes : Any additoinal information that provides clarity/suppliments the overview. Each additional 'child/sub ' class or define should include the following information Overview : one-maybe-two sentence summary of what the class does/problem it solves","title":"Documentation"},{"location":"linux/puppet/profile_guidelines/#hierarchy","text":"Each profile should attempt to adhere to the following hierarchy model that follows a branching pattern for any 'child/sub ' classes or defines. profile \u251c\u2500\u2500manifests | \u2514\u2500\u2500applicationx.pp | \u2514\u2500\u2500applicationx | \u2514\u2500\u2500sub_class1.pp | \u2514\u2500\u2500sub_class.pp | \u2514\u2500\u2500monitoring | \u2514\u2500\u2500base.pp | \u2514\u2500\u2500define1.pp | \u2514\u2500\u2500define2.pp \u251c\u2500\u2500templates | \u2514\u2500\u2500applicationx | | \u2514\u2500\u2500applicationx_file.ini.erb | \u2514\u2500\u2500icinga2 | \u2514\u2500\u2500checkcommands | | \u2514\u2500\u2500check_applicationx.py.erb | \u2514\u2500\u2500checkplugins | \u2514\u2500\u2500check_applicationx.py.erb \u2514\u2500\u2500files \u2514\u2500\u2500applciationx | \u2514\u2500\u2500applicationx_file.ini \u2514\u2500\u2500icinga2 \u2514\u2500\u2500checkcommands | \u2514\u2500\u2500check_applicationx.py.erb \u2514\u2500\u2500checkplugins \u2514\u2500\u2500check_applicationx.py.erb","title":"Hierarchy"},{"location":"linux/puppet/profile_guidelines/#manifests","text":"'Top level' manifests will be included in a role class to be applied to a node or nodes Store 'child/sub' classes or defines within a sub-directory with the same name as the 'top level ' manifest. Use 'child/sub' classes or defines with logic for a specific identifier such as site, security zone, etc... This helps to limit the size and complexity of monolithic manifests. Reference these in the 'top level' manifest. ## Include Required Sub-Class if ($::domain == cha.arin.net ) or ($::domain == core.cha.arin.net ) { include profile::applicationx::cha } elsif ($::domain == ash.arin.net ) or ($::domain == core.ash.arin.net ) { include profile::applicationx::ash } else { notify { ApplicationX Sub-Classes Not Loaded : withpath = true, } } Store 'child/sub' classes or defines related to monitoring within a sub-directory named monitoring under the application sub-directory. Define monitoring checksin a class named base profile::applicationx::monitoring::base . Reference puppet defines in the base monitoring class. profile::applicationx::monitoring::define1 = { $applicationx_server_names: } profile::applicationx::monitoring::define2 = { $applicationx_server_names: }","title":"Manifests"},{"location":"linux/puppet/profile_guidelines/#filestemplates","text":"Store required files or templates referenced within a profile child/sub' class or define (excluding monitoring) in a sub-directory with the same name as the 'top level' manifest. Store required files or templates referenced within a profile child/sub' class or define in a sub-directory with the same name of the monitoring software such as Icinga2. Store catagory specific (checkcommands, checkplugins) monitoring files or templates in a sub-directory with the catagory name (checkcommands, checkplugins) under the templates/icinga2 directory","title":"Files/Templates"},{"location":"linux/puppet/puppet_hiera_application_network_environments/","text":"(Puppet) Environments, (Hiera) Environments, (Application/Network) Environments OH MY! Overview Environments is an easily and extremely overloaded term often used when explaining/with Puppet, Hiera, Deployment/Development \"Application\" (Internal) and Networks. Below is a clarification reclassification of the terms used to help provided a better understanding of what an \"environment\" really is within each definition. Puppet Environments Puppet environments come in one of two flavors, directory environments or config file environments Directory environments are easier to use and will eventually replace config file environments completely. What are they ? An isolated group of Puppet agent nodes, each environment is served with a completley different main manifests and modulepaths What are they used for ? Frequently short-term assignments used for testing changes to your Puppet DSL code ( modules ) when your refactoring or adding new functionailty, but don't want to directly push the changes into production. (because your not crazy, right?). Instead pushing the changes to a subset of nodes to test before merging the changes in. Hiera Environments What are they ? An isolated set hierarchial data sources. Puppet environments and Hiera environments are symbiotic \u2013 both tools use the same environment concept and so environment names MUST match for the data to be shared (i.e. if you create an environment in Puppet called \u2018yellow\u2019, you will need a Hiera environment called \u2018yellow\u2019 for that data). What are they used for ? Testing changes when adding or modifiying hierarchical Hiera specific data, like Puppet environments you don't want to directly push these changes into production, but test on a subset of nodes before merging in the change set. Deployment/Development \"Appliation\" Environments What are they ? Resources designated to a specific tier/stage in the release process, IE: Development, Integration, Staging, Production. In genernal each tier consists of groupings of nodes designated to a specific stage in the release process What are they used for ? Most commonly they are application or deployment gateways, used for the purpose of phased deployments. To provide isolated environments for development, integration testing, and staging of applications/software to ensure the ability to release/install an application/software in a production environment. Network Environmnets What are they ? A logical telecommunications network segment, to seperate devices. Segments often connected to provide connection and communication between various devices. This type of network topology allows for organizational segmentation and often coincides with a VLAN. What are they used for ? While the extent of a segment depends on the nature of the network and the device or devices used. This is most commonly used to address issues of scalability, security and network management. This also may or may not link with an internal \"environment\" or tier Putting It All Together In order to help limit the confusion when referencing the term environment within the Puppet infrastructure, we'll designate the the following terms to replace \" environment \" Puppet and Hiera have a symbiotic relationship and are main focus of the infrastructure thus they shall remain using the term environment Network Environments shall be refered to as network segments Deployment/Development \"Appliation\" Environments shall be refered to as tiers Terminology References Puppet - About Environments Traditional Development/Integration/Staging/Production Practice for Software Development","title":"(Puppet) Environments, (Hiera) Environments, (Application/Network) Environments OH MY!"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#puppet-environments-hiera-environments-applicationnetwork-environments-oh-my","text":"","title":"(Puppet) Environments, (Hiera) Environments, (Application/Network) Environments OH MY!"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#overview","text":"Environments is an easily and extremely overloaded term often used when explaining/with Puppet, Hiera, Deployment/Development \"Application\" (Internal) and Networks. Below is a clarification reclassification of the terms used to help provided a better understanding of what an \"environment\" really is within each definition.","title":"Overview"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#puppet-environments","text":"Puppet environments come in one of two flavors, directory environments or config file environments Directory environments are easier to use and will eventually replace config file environments completely.","title":"Puppet Environments"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they","text":"An isolated group of Puppet agent nodes, each environment is served with a completley different main manifests and modulepaths","title":"What are they ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they-used-for","text":"Frequently short-term assignments used for testing changes to your Puppet DSL code ( modules ) when your refactoring or adding new functionailty, but don't want to directly push the changes into production. (because your not crazy, right?). Instead pushing the changes to a subset of nodes to test before merging the changes in.","title":"What are they used for ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#hiera-environments","text":"","title":"Hiera Environments"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they_1","text":"An isolated set hierarchial data sources. Puppet environments and Hiera environments are symbiotic \u2013 both tools use the same environment concept and so environment names MUST match for the data to be shared (i.e. if you create an environment in Puppet called \u2018yellow\u2019, you will need a Hiera environment called \u2018yellow\u2019 for that data).","title":"What are they ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they-used-for_1","text":"Testing changes when adding or modifiying hierarchical Hiera specific data, like Puppet environments you don't want to directly push these changes into production, but test on a subset of nodes before merging in the change set.","title":"What are they used for ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#deploymentdevelopment-appliation-environments","text":"","title":"Deployment/Development \"Appliation\" Environments"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they_2","text":"Resources designated to a specific tier/stage in the release process, IE: Development, Integration, Staging, Production. In genernal each tier consists of groupings of nodes designated to a specific stage in the release process","title":"What are they ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they-used-for_2","text":"Most commonly they are application or deployment gateways, used for the purpose of phased deployments. To provide isolated environments for development, integration testing, and staging of applications/software to ensure the ability to release/install an application/software in a production environment.","title":"What are they used for ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#network-environmnets","text":"","title":"Network Environmnets"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they_3","text":"A logical telecommunications network segment, to seperate devices. Segments often connected to provide connection and communication between various devices. This type of network topology allows for organizational segmentation and often coincides with a VLAN.","title":"What are they ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#what-are-they-used-for_3","text":"While the extent of a segment depends on the nature of the network and the device or devices used. This is most commonly used to address issues of scalability, security and network management. This also may or may not link with an internal \"environment\" or tier","title":"What are they used for ?"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#putting-it-all-together","text":"In order to help limit the confusion when referencing the term environment within the Puppet infrastructure, we'll designate the the following terms to replace \" environment \" Puppet and Hiera have a symbiotic relationship and are main focus of the infrastructure thus they shall remain using the term environment Network Environments shall be refered to as network segments Deployment/Development \"Appliation\" Environments shall be refered to as tiers","title":"Putting It All Together"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#terminology","text":"","title":"Terminology"},{"location":"linux/puppet/puppet_hiera_application_network_environments/#references","text":"Puppet - About Environments Traditional Development/Integration/Staging/Production Practice for Software Development","title":"References"},{"location":"linux/puppet/puppet_roles_profiles_pattern/","text":"Roles and Profiles Pattern (Methodology) Overview Provides a methodology for abstraction of Puppet code. It's all about abstraction : Data (parameters) gets abstracted by Hiera Providers/Resources abstract the underlying OS implementation Providers get abstracted by Types Resources get abstracted by Classes Classes get abstracted by Modules ( Component Modules ) Modules get abstracted by Profiles Profiles get abstracted by Roles Classification/Abstraction Flow: A node is assigned a Role A Role includes one or more Profiles Profiles include Component modules, Resources combined with logic. Make calls out to Hiera for hierarchal specific data (parameters) Hiera data is passed to Component modules Component modules call Puppet Resources Puppet Resources use Types / Providers to configure setting on a node Profiles Combines modules andresources to define a logical technology stack(single). For more detail consult the documented set of guidelines Profiles follow the following rules: Technology specific May include resources directly May make calls to Hiera for required data (parameters) These calls may be transparent via automatic parameter lookup Include Component module classes/resources Named according to the technology they manage Do NOT include environments in profile nawmes Being technology specific does not limit the profile to managing a single application. It should include all applications and logic need to fully manage an application stack. Example: class profile::wordpress { ## Hiera lookups $site_name = hiera( profiles::wordpress::site_name ) $wordpress_user_password = hiera( profiles::wordpress::wordpress_user_password ) $mysql_root_password = hiera( profiles::wordpress::mysql_root_password ) $wordpress_db_host = hiera( profiles::wordpress::wordpress_db_host ) $wordpress_db_name = hiera( profiles::wordpress::wordpress_db_name ) $wordpress_db_password = hiera( profiles::wordpress::wordpress_db_password ) $wordpress_user = hiera( profiles::wordpress::wordpress_user ) $wordpress_group = hiera( profiles::wordpress::wordpress_group ) $wordpress_docroot = hiera( profiles::wordpress::wordpress_docroot ) $wordpress_port = hiera( profiles::wordpress::wordpress_port ) ## Create user group { wordpress : ensure = present, name = $wordpress_group, } user { wordpress : ensure = present, gid = $wordpress_group, password = $wordpress_user_password, name = $wordpress_group, home = $wordpress_docroot, } ## Configure mysql class { mysql::server : root_password = $mysql_root_password, } class { mysql::bindings : php_enable = true, } ## Configure apache include apache include apache::mod::php apache::vhost { $::fqdn: port = $wordpress_port, docroot = $wordpress_docroot, } ## Configure wordpress class { ::wordpress : install_dir = $wordpress_docroot, db_name = $wordpress_db_name, db_host = $wordpress_db_host, db_password = $wordpress_db_password, } } Roles A unquie collection (wrapper) of one or more profiles (technology stacks) to define a node For more detail consult the documented set of guidelines Roles follow the following rules: Maps technology to a node Nodes get classified with only a SINGLE role A role similar, yet different, from another role is: a NEW role . Do NOT include logic ONLY use includes for abstracting profiles Named according to the nodes purpose (business logic) Example: Includes the profile wordpress to install/configure wordpress and includes the base profile to configure globally common settings such as NTP and default user accounts. class role::myblog { include profile::wordpress include profile::base } References Puppet Camp 2013 Presentation Puppet Roles and Profiles Workflow (SH*% Gary Says)","title":"Puppet roles profiles pattern"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#roles-and-profiles-pattern-methodology","text":"","title":"Roles and Profiles Pattern (Methodology)"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#overview","text":"Provides a methodology for abstraction of Puppet code. It's all about abstraction : Data (parameters) gets abstracted by Hiera Providers/Resources abstract the underlying OS implementation Providers get abstracted by Types Resources get abstracted by Classes Classes get abstracted by Modules ( Component Modules ) Modules get abstracted by Profiles Profiles get abstracted by Roles","title":"Overview"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#classificationabstraction-flow","text":"A node is assigned a Role A Role includes one or more Profiles Profiles include Component modules, Resources combined with logic. Make calls out to Hiera for hierarchal specific data (parameters) Hiera data is passed to Component modules Component modules call Puppet Resources Puppet Resources use Types / Providers to configure setting on a node","title":"Classification/Abstraction Flow:"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#profiles","text":"Combines modules andresources to define a logical technology stack(single). For more detail consult the documented set of guidelines Profiles follow the following rules: Technology specific May include resources directly May make calls to Hiera for required data (parameters) These calls may be transparent via automatic parameter lookup Include Component module classes/resources Named according to the technology they manage Do NOT include environments in profile nawmes Being technology specific does not limit the profile to managing a single application. It should include all applications and logic need to fully manage an application stack. Example: class profile::wordpress { ## Hiera lookups $site_name = hiera( profiles::wordpress::site_name ) $wordpress_user_password = hiera( profiles::wordpress::wordpress_user_password ) $mysql_root_password = hiera( profiles::wordpress::mysql_root_password ) $wordpress_db_host = hiera( profiles::wordpress::wordpress_db_host ) $wordpress_db_name = hiera( profiles::wordpress::wordpress_db_name ) $wordpress_db_password = hiera( profiles::wordpress::wordpress_db_password ) $wordpress_user = hiera( profiles::wordpress::wordpress_user ) $wordpress_group = hiera( profiles::wordpress::wordpress_group ) $wordpress_docroot = hiera( profiles::wordpress::wordpress_docroot ) $wordpress_port = hiera( profiles::wordpress::wordpress_port ) ## Create user group { wordpress : ensure = present, name = $wordpress_group, } user { wordpress : ensure = present, gid = $wordpress_group, password = $wordpress_user_password, name = $wordpress_group, home = $wordpress_docroot, } ## Configure mysql class { mysql::server : root_password = $mysql_root_password, } class { mysql::bindings : php_enable = true, } ## Configure apache include apache include apache::mod::php apache::vhost { $::fqdn: port = $wordpress_port, docroot = $wordpress_docroot, } ## Configure wordpress class { ::wordpress : install_dir = $wordpress_docroot, db_name = $wordpress_db_name, db_host = $wordpress_db_host, db_password = $wordpress_db_password, } }","title":"Profiles"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#roles","text":"A unquie collection (wrapper) of one or more profiles (technology stacks) to define a node For more detail consult the documented set of guidelines Roles follow the following rules: Maps technology to a node Nodes get classified with only a SINGLE role A role similar, yet different, from another role is: a NEW role . Do NOT include logic ONLY use includes for abstracting profiles Named according to the nodes purpose (business logic) Example: Includes the profile wordpress to install/configure wordpress and includes the base profile to configure globally common settings such as NTP and default user accounts. class role::myblog { include profile::wordpress include profile::base }","title":"Roles"},{"location":"linux/puppet/puppet_roles_profiles_pattern/#references","text":"Puppet Camp 2013 Presentation Puppet Roles and Profiles Workflow (SH*% Gary Says)","title":"References"},{"location":"linux/puppet/role_guidelines/","text":"Overview What is a role ? A role is a unquie collection (wrapper) of one or more included profiles (technology stacks). Breakdown Maps technology to a node Nodes get classified with only a SINGLE role A role similar, yet different, from another role is: a NEW role . Do NOT include logic ONLY use includes for abstracting profiles Named according to the nodes purpose (business logic) Guidelines Naming Role naming needs to adhear to the following conventions: Named according to the nodes purpose (business logic) Do NOT include a - (hypen), may include an _ (underscore) Do NOT include the puppet environment or environment tier (envtier) (prod, stage, test, dev, si ....) Documentation # == Overview: # # == Monitoring: # # == Notes: Each role manifest should be properly documented detailing the following information Overview : one-maybe-two sentence summary of what the role manages Monitoring : Roll-up summary of the monitoring provided by included profile classes Notes : Any additoinal information that provides clarity/suppliments the overview. Manifests Roles classes should be a self contained entity and not include or depend upon another role . This provides greater visibility seeing exactly what\u2019s being declared (i.e. all the profiles). A role similar, yet different, from another role is: a NEW role . Roles should ONLY use the include function and should ONLY include profiles . No Component Classes , resources or logic should be used. Example: class role::myblog { include profile::wordpress include profile::base }","title":"Role guidelines"},{"location":"linux/puppet/role_guidelines/#overview","text":"What is a role ? A role is a unquie collection (wrapper) of one or more included profiles (technology stacks). Breakdown Maps technology to a node Nodes get classified with only a SINGLE role A role similar, yet different, from another role is: a NEW role . Do NOT include logic ONLY use includes for abstracting profiles Named according to the nodes purpose (business logic)","title":"Overview"},{"location":"linux/puppet/role_guidelines/#guidelines","text":"","title":"Guidelines"},{"location":"linux/puppet/role_guidelines/#naming","text":"Role naming needs to adhear to the following conventions: Named according to the nodes purpose (business logic) Do NOT include a - (hypen), may include an _ (underscore) Do NOT include the puppet environment or environment tier (envtier) (prod, stage, test, dev, si ....)","title":"Naming"},{"location":"linux/puppet/role_guidelines/#documentation","text":"# == Overview: # # == Monitoring: # # == Notes: Each role manifest should be properly documented detailing the following information Overview : one-maybe-two sentence summary of what the role manages Monitoring : Roll-up summary of the monitoring provided by included profile classes Notes : Any additoinal information that provides clarity/suppliments the overview.","title":"Documentation"},{"location":"linux/puppet/role_guidelines/#manifests","text":"Roles classes should be a self contained entity and not include or depend upon another role . This provides greater visibility seeing exactly what\u2019s being declared (i.e. all the profiles). A role similar, yet different, from another role is: a NEW role . Roles should ONLY use the include function and should ONLY include profiles . No Component Classes , resources or logic should be used. Example: class role::myblog { include profile::wordpress include profile::base }","title":"Manifests"},{"location":"tips_and_tricks/useful_commands/","text":"Useful Commands Bash Display the access rights in octal of all files with in a directory stat -c %a %n * Diffing a local and remote file. The diff utility can take one operand in the form of stdin (represented by \"-\") ssh node1.example.com cat /full/file/path/text.txt | diff -y --suppress-common-lines /full/file/path/text.txt - Diff two remote files The diff utility can take one operand in the form of stdin (represented by \"-\") diff -y --suppress-common-lines ( ssh node1.example.com cat /full/file/path/text.txt ) ( ssh node2.example.com cat /full/file/path/text.txt ) Sort files by human readable size du -sk * | sort -rn | awk {print $2} | xargs -ia du -hs a du -sk * | sort -rn | awk { split( KB MB GB , v ); s=1; while( $1 1024 ){ $1/=1024; s++ } print int($1) v[s], $2 } sudo du -sk . [ !. ] * * | sort -rn | awk { split( KB MB GB , v ); s=1; while( $1 1024 ){ $1/=1024; s++ } print int($1) v[s], $2 } Test GPG Passphrase echo 1234 | gpg --no-use-agent -o /dev/null --local-user keyID -as - echo The correct passphrase was entered for this key Awk Sed","title":"Useful Commands"},{"location":"tips_and_tricks/useful_commands/#useful-commands","text":"","title":"Useful Commands"},{"location":"tips_and_tricks/useful_commands/#bash","text":"Display the access rights in octal of all files with in a directory stat -c %a %n * Diffing a local and remote file. The diff utility can take one operand in the form of stdin (represented by \"-\") ssh node1.example.com cat /full/file/path/text.txt | diff -y --suppress-common-lines /full/file/path/text.txt - Diff two remote files The diff utility can take one operand in the form of stdin (represented by \"-\") diff -y --suppress-common-lines ( ssh node1.example.com cat /full/file/path/text.txt ) ( ssh node2.example.com cat /full/file/path/text.txt ) Sort files by human readable size du -sk * | sort -rn | awk {print $2} | xargs -ia du -hs a du -sk * | sort -rn | awk { split( KB MB GB , v ); s=1; while( $1 1024 ){ $1/=1024; s++ } print int($1) v[s], $2 } sudo du -sk . [ !. ] * * | sort -rn | awk { split( KB MB GB , v ); s=1; while( $1 1024 ){ $1/=1024; s++ } print int($1) v[s], $2 } Test GPG Passphrase echo 1234 | gpg --no-use-agent -o /dev/null --local-user keyID -as - echo The correct passphrase was entered for this key","title":"Bash"},{"location":"tips_and_tricks/useful_commands/#awk","text":"","title":"Awk"},{"location":"tips_and_tricks/useful_commands/#sed","text":"","title":"Sed"}]}